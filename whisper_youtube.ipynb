{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Youtube Videos Transcription with OpenAI's Whisper**\n",
        "\n",
        "[![blog post shield](https://img.shields.io/static/v1?label=&message=Blog%20post&color=blue&style=for-the-badge&logo=openai&link=https://openai.com/blog/whisper)](https://openai.com/blog/whisper)\n",
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/openai/whisper)\n",
        "[![paper shield](https://img.shields.io/static/v1?label=&message=Paper&color=blue&style=for-the-badge&link=https://cdn.openai.com/papers/whisper.pdf)](https://cdn.openai.com/papers/whisper.pdf)\n",
        "[![model card shield](https://img.shields.io/static/v1?label=&message=Model%20card&color=blue&style=for-the-badge&link=https://github.com/openai/whisper/blob/main/model-card.md)](https://github.com/openai/whisper/blob/main/model-card.md)\n",
        "\n",
        "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
        "\n",
        "This Notebook will guide you through the transcription of a Youtube video using Whisper. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript and video audio in your Google Drive."
      ],
      "metadata": {
        "id": "96kvih9mXkNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Check GPU type** 🕵️\n",
        "\n",
        "#@markdown The type of GPU you get assigned in your Colab session defined the speed at which the video will be transcribed.\n",
        "#@markdown The higher the number of floating point operations per second (FLOPS), the faster the transcription.\n",
        "#@markdown But even the least powerful GPU available in Colab is able to run any Whisper model.\n",
        "#@markdown Make sure you've selected `GPU` as hardware accelerator for the Notebook (Runtime &rarr; Change runtime type &rarr; Hardware accelerator).\n",
        "\n",
        "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
        "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
        "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
        "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
        "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Factory reset your Notebook's runtime if you want to get assigned a new GPU.**\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "QshUbLqpX7L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IfG0E_WbRFI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cddb233f-1332-4add-d3e9-135bbff75a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-17bvtbu_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-17bvtbu_\n",
            "  Resolved https://github.com/openai/whisper.git to commit 90db0de1896c23cbfaf0c58bc2d30665f709f170\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper==20240930)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803583 sha256=1e31579dcfbd44f04af76e50e0a1cd9847f928d54815ea7a7b0dfaabc65c8c17\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ew1ld8mv/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.12.23-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2024.12.23-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2024.12.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Install libraries** 🏗️\n",
        "#@markdown This cell will take a little while to download several libraries, including Whisper.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install yt-dlp\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1zwGAsr4sIgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daab185d-9a06-4a73-a6f5-842cfd8a4ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Optional:** Save data in Google Drive 💾\n",
        "#@markdown Enter a Google Drive path and run this cell if you want to store the results inside Google Drive.\n",
        "\n",
        "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
        "from google.colab import drive\n",
        "drive_mount_path = Path(\"/\") / \"content\" / \"drive\"\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"My Drive\"\n",
        "#@markdown ---\n",
        "drive_path = \"Colab Notebooks/Whisper Youtube\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change your Google Drive path.**\n",
        "\n",
        "drive_whisper_path = drive_mount_path / Path(drive_path.lstrip(\"/\"))\n",
        "drive_whisper_path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Model selection** 🧠\n",
        "\n",
        "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
        "\n",
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "\n",
        "#@markdown ---\n",
        "Model = 'medium' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if Model in whisper.available_models():\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is selected.**\"\n",
        "    ))\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is no longer available.**<br /> Please select one of the following:<br /> - {'<br /> - '.join(whisper.available_models())}\"\n",
        "    ))"
      ],
      "metadata": {
        "id": "TMhrSq_GZ6kA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "255c9192-8d28-484e-9e76-33743600e793"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:34<00:00, 44.4MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**medium model is selected.**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Video selection** 📺\n",
        "\n",
        "#@markdown Enter the URL of the Youtube video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n",
        "\n",
        "Type = \"Youtube video or playlist\" #@param ['Youtube video or playlist', 'Google Drive']\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video or playlist**\n",
        "URL = \"https://youtu.be/BOpyP_B3_wI?si=vTypeKWHClAHhT6Z\" #@param {type:\"string\"}\n",
        "# store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive video, audio (mp4, wav), or folder containing video and/or audio files**\n",
        "video_path = \"Colab Notebooks/transcription/my_video.mp4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**\n",
        "\n",
        "video_path_local_list = []\n",
        "\n",
        "if Type == \"Youtube video or playlist\":\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'm4a/bestaudio/best',\n",
        "        'outtmpl': '%(id)s.%(ext)s',\n",
        "        # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "        'postprocessors': [{  # Extract audio using ffmpeg\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'wav',\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        error_code = ydl.download([URL])\n",
        "        list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "    for video_info in list_video_info:\n",
        "        video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))\n",
        "\n",
        "elif Type == \"Google Drive\":\n",
        "    # video_path_drive = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    video_path = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    if video_path.is_dir():\n",
        "        for video_path_drive in video_path.glob(\"**/*\"):\n",
        "            if video_path_drive.is_file():\n",
        "                display(Markdown(f\"**{str(video_path_drive)} selected for transcription.**\"))\n",
        "            elif video_path_drive.is_dir():\n",
        "                display(Markdown(f\"**Subfolders not supported.**\"))\n",
        "            else:\n",
        "                display(Markdown(f\"**{str(video_path_drive)} does not exist, skipping.**\"))\n",
        "            video_path_local = Path(\".\").resolve() / (video_path_drive.name)\n",
        "            shutil.copy(video_path_drive, video_path_local)\n",
        "            video_path_local_list.append(video_path_local)\n",
        "    elif video_path.is_file():\n",
        "        video_path_local = Path(\".\").resolve() / (video_path.name)\n",
        "        shutil.copy(video_path, video_path_local)\n",
        "        video_path_local_list.append(video_path_local)\n",
        "        display(Markdown(f\"**{str(video_path)} selected for transcription.**\"))\n",
        "    else:\n",
        "        display(Markdown(f\"**{str(video_path)} does not exist.**\"))\n",
        "\n",
        "else:\n",
        "    raise(TypeError(\"Please select supported input type.\"))\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    if video_path_local.suffix == \".mp4\":\n",
        "        video_path_local = video_path_local.with_suffix(\".wav\")\n",
        "        result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n"
      ],
      "metadata": {
        "id": "xYLPZQX9S7tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9372db45-405a-4af5-b512-24e7d7e31b44"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/BOpyP_B3_wI?si=vTypeKWHClAHhT6Z\n",
            "[youtube] BOpyP_B3_wI: Downloading webpage\n",
            "[youtube] BOpyP_B3_wI: Downloading ios player API JSON\n",
            "[youtube] BOpyP_B3_wI: Downloading mweb player API JSON\n",
            "[youtube] BOpyP_B3_wI: Downloading m3u8 information\n",
            "[info] BOpyP_B3_wI: Downloading 1 format(s): 140\n",
            "[download] Destination: BOpyP_B3_wI.m4a\n",
            "[download] 100% of   78.51MiB in 00:00:12 at 6.08MiB/s   \n",
            "[FixupM4a] Correcting container of \"BOpyP_B3_wI.m4a\"\n",
            "[ExtractAudio] Destination: BOpyP_B3_wI.wav\n",
            "Deleting original file BOpyP_B3_wI.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://youtu.be/BOpyP_B3_wI?si=vTypeKWHClAHhT6Z\n",
            "[youtube] BOpyP_B3_wI: Downloading webpage\n",
            "[youtube] BOpyP_B3_wI: Downloading ios player API JSON\n",
            "[youtube] BOpyP_B3_wI: Downloading mweb player API JSON\n",
            "[youtube] BOpyP_B3_wI: Downloading m3u8 information\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-X0qB9JAzMLY",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2906d65f-d823-4bc8-c6c9-be292bf29056"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### BOpyP_B3_wI.wav"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:14.600]  Я надеюсь, народ еще подтянется, но чтобы сильные семинары не затягивать, давайте начнем,\n",
            "[00:14.600 --> 00:23.080]  а то у нас обычно на 5 минут, на 6, на 7 не очень хорошо получается. Итак, всем добрый вечер,\n",
            "[00:23.080 --> 00:31.200]  у нас сегодня третья лекция, это последняя лекция перед мира сетями, и она у нас будет посвящена\n",
            "[00:31.200 --> 00:39.600]  достаточно любопытному топику, который называется сжатие изображений и анализ изображений с помощью\n",
            "[00:39.600 --> 00:47.940]  преобразования фурье. Соответственно, темы лекции, про которые мы сегодня поговорим.\n",
            "[00:47.940 --> 00:57.180]  В-первых, мы поговорим про сжатие изображений без потерь, это формат png. Дальше поговорим про\n",
            "[00:57.180 --> 01:05.280]  сжатие с потерями с помощью jpeg. Затем поговорим про преобразование фурье, как с его помощью можно\n",
            "[01:05.280 --> 01:12.020]  анализировать изображения. И наконец поговорим про любопытное применение анализа фурье,\n",
            "[01:12.020 --> 01:19.960]  которое позволяет объяснить метод склейки с помощью Лапласовской пирамиды. В целом у нас будет не очень\n",
            "[01:19.960 --> 01:27.520]  нагруженная математическая лекция, основная ее цель дать вам какое-то интуитивное представление о том,\n",
            "[01:27.520 --> 01:36.500]  как устроены базовые методы. Сейчас, конечно, способов сжатия изображений достаточно много,\n",
            "[01:36.500 --> 01:44.000]  посмотрим самые популярные, которые до факта стандарт и до сих пор еще не вытеснены. Соответственно,\n",
            "[01:44.000 --> 01:51.480]  после того, как вы познакомитесь, вы будете понимать какие-то основные принципы, как сжимаются\n",
            "[01:51.480 --> 01:58.400]  изображения, как могут происходить потери. Дальше, если вы эти изображения разжимаете и засовываете\n",
            "[01:58.400 --> 02:03.860]  в нейросети, тоже можете понимать с какими артефактами может быть вашей нейросети предстоит\n",
            "[02:03.860 --> 02:12.800]  работать. Соответственно, поговорим для начала про png, сжатие без потерь. В целом, скажите мне,\n",
            "[02:12.800 --> 02:19.640]  пожалуйста, попробуйте дать какое-то определение, что такое сжатие изображений, чего мы пытаемся добиться.\n",
            "[02:19.640 --> 02:29.520]  Ну, сжатие изображений — это уменьшение веса, размера изображений. То есть, например, вес 1 гигабайт,\n",
            "[02:29.520 --> 02:36.500]  ну не гигабайт, а 50 мегабайт, 100 килобайтов уменьшаем. При этом не хотим, чтобы сильно качество\n",
            "[02:36.500 --> 02:44.140]  как-то ухудшилось. Хорошо. Ну да, то есть мы хотим у нашего изображения получить какое-то\n",
            "[02:44.140 --> 02:49.260]  специальное представление, то есть как-то закодировать в каком-то специальном виде,\n",
            "[02:49.260 --> 02:58.580]  так чтобы весело это все поменьше, чем изображение SIS. Под изображением SIS я имею в виду,\n",
            "[02:58.580 --> 03:05.200]  что если бы мы брали эти нампай-массивы, с которыми вы уже работаете в первых домашках,\n",
            "[03:05.200 --> 03:11.680]  и их бы сохранили напрямую в файл, тогда, конечно, мы можем очень легко по разрешению,\n",
            "[03:11.680 --> 03:18.520]  по количеству каналов, по битности посчитать, сколько картинки будут занимать. Вот можете проделать\n",
            "[03:18.520 --> 03:25.080]  на засуги, и вы поймете, что картинки даже самые маленькие в таком сыром виде будут\n",
            "[03:25.080 --> 03:33.180]  занимать достаточно много по современным меркам. Соответственно, что значит, что мы хотим,\n",
            "[03:33.180 --> 03:40.980]  чтобы у нас качество не ухудшилось сильно? То есть когда мы сжимаем изображение и когда разжимаем,\n",
            "[03:40.980 --> 03:48.500]  мы можем сравнить, что мы получаем на выходе после рожатия и с тем, что у нас было на входе.\n",
            "[03:48.500 --> 03:54.380]  Соответственно, мы можем вводить какие-то формальные метрики, например, считать\n",
            "[03:54.380 --> 04:00.300]  какую-нибудь среднюю квадратичную ошибку или PSNR, с которым вы познакомились в первых домашках.\n",
            "[04:00.300 --> 04:08.100]  И так можно делать. И в целом, когда методы сжатия, изображения, видео тестируются,\n",
            "[04:08.100 --> 04:15.060]  они могут оцениваться какими-то метриками. Но метрики в целом не идеальные. И финальная,\n",
            "[04:15.900 --> 04:23.220]  неформальная метрика качества – это восприятие человеком. Мы говорили в первой лекции, что человеческая\n",
            "[04:23.220 --> 04:31.500]  визуальная система, зрительная кора, достаточно сложна в процессе изображения. Вот что такое не\n",
            "[04:31.500 --> 04:40.340]  очень сильно отличается изображение одно от другого, это сложно формализовать, потому что для этого нам\n",
            "[04:40.340 --> 04:47.540]  нужно залезть в мозг и точно знать, как визуальный процессинг в нашем мозгу работает. Нам это\n",
            "[04:47.540 --> 04:53.780]  недоступно, поэтому у нас есть некоторые ивристики, некоторые правила, согласно которым мы можем как-то\n",
            "[04:53.780 --> 05:00.580]  попытаться сжимать изображение. Соответственно, начнем с ПНГ. ПНГ – распространенный формат,\n",
            "[05:00.580 --> 05:10.140]  когда у нас то, что на выходе после рожатия, идентично тому, что на входе. Как ПНГ вообще появился?\n",
            "[05:10.140 --> 05:20.180]  И как появляются форматы жатия изображений? Есть специальные комитеты по стандартам,\n",
            "[05:20.180 --> 05:29.380]  которые садятся и представляют какие-то документы, их как-то обсуждают и затем принимают их в качестве\n",
            "[05:29.380 --> 05:37.420]  стандарта. Вот откуда эти комитеты берутся? В 95-м году, например, когда люди работали над ПНГ или\n",
            "[05:37.500 --> 05:45.660]  над JPEG, собирались люди из индустрии и какие-то академические исследователи, то есть это какие-то\n",
            "[05:45.660 --> 05:54.220]  волонтеры, которые обладают достаточной экспертизой, собирались и обсуждали, как можно какой-то стандарт\n",
            "[05:54.220 --> 06:05.100]  принять. После того, как они обсудили и показали документ заинтересованному сообществу, какой-то\n",
            "[06:05.100 --> 06:13.300]  черновик, сообщество эти идеи критикует и дальше это все фиксируется и предполагается, что все\n",
            "[06:13.300 --> 06:20.300]  заинтересованные компании будут этот стандарт добровольно принимать и добровольно реализовывать\n",
            "[06:20.300 --> 06:27.500]  в своих программах. Соответственно, ПНГ – аббревиатура рекурсивная, как в то время любили,\n",
            "[06:27.500 --> 06:36.940]  в самом деле произносится ПНГ как PING по-английски и расшифровывается как PING из нот GIF.\n",
            "[06:36.940 --> 06:45.740]  То есть было еще в 80-х годах, до сих пор на самом деле существует формат GIF, который был\n",
            "[06:45.740 --> 06:53.100]  проприетарным и ПНГ – это такая свободная альтернатива GIF, которая именуется через\n",
            "[06:53.100 --> 07:04.700]  ссылку на GIF. Соответственно, это формат без потерь был предложен в 95-м году, поддерживает\n",
            "[07:04.700 --> 07:12.220]  полупрозрачность до 16 бит на канал и поддерживает палитры. Палитра – это такая удобная вещь,\n",
            "[07:12.220 --> 07:20.860]  функциональность в стандарте, которая позволяет вам, если вы используете не все цвета, которые вам\n",
            "[07:20.860 --> 07:28.060]  доступны в вашем диапазоне, вы можете создать отдельную табличку, небольшую палитру. По сути,\n",
            "[07:28.060 --> 07:34.380]  это табличка с индексами. И дальше вместо того, чтобы кодировать цвет, вы можете кодировать индекс\n",
            "[07:34.380 --> 07:45.460]  из этой палитры. Формат очень простой, я расскажу сейчас его основную идею, такую содержательную.\n",
            "[07:45.460 --> 07:53.820]  И зная ключевой идеи, вы можете, немножко вникнув в какие-то низкоуровневые детали,\n",
            "[07:53.820 --> 08:03.820]  реализовать простую, реализовать самостоятельно ПНГ. И реализация, те библиотеки, которые мы\n",
            "[08:03.820 --> 08:10.380]  обычно пользуемся, например, они достаточно эффективны и кроссплатформными. Соответственно,\n",
            "[08:10.380 --> 08:16.140]  как работает метод кодирования. У нас есть картинка. Как мы говорили в прошлой лекции,\n",
            "[08:16.140 --> 08:23.660]  картинки очень избыточные пространственно, то есть соседний пиксель, как правило, сильно зависит от\n",
            "[08:23.660 --> 08:30.980]  предыдущих пикселей. В данном случае, предположим, что мы идем как-то по картинке,\n",
            "[08:30.980 --> 08:38.300]  слева направо, сверху вниз, и дальше натыкаемся в какой-то момент на пиксель X, который нам нужно\n",
            "[08:38.300 --> 08:45.500]  закодировать. И в ПНГ рассматривается небольшая окрестность этого пикселя X, 4 пикселя A, B, C, D,\n",
            "[08:45.500 --> 08:53.660]  которые мы до этого прошли. По сути, мы работаем с половинкой окрестности 3 на 3. И у нас есть\n",
            "[08:53.660 --> 09:02.900]  способы закодировать очередной пиксель X. Способов 5 штук. Они выбираются построчно,\n",
            "[09:02.900 --> 09:09.460]  то есть мы начинаем, например, переходим на эту строчку, начинаем бежать по всем пикселям этой строке.\n",
            "[09:09.460 --> 09:15.780]  И мы можем заранее пробежаться по всем пикселям и оценить, какой метод кодирования будет наиболее\n",
            "[09:15.780 --> 09:22.180]  эффективным. Соответственно, методы кодирования мы можем X передавать как есть, если у нас какие-то\n",
            "[09:22.180 --> 09:29.620]  очень сильные изменения, которые не зависят от окрестности. Дальше три способа кодирования простых\n",
            "[09:29.620 --> 09:36.220]  закономерностей. То есть у нас пиксель X может быть сильно похож на пиксель A, либо на пиксель B,\n",
            "[09:36.220 --> 09:46.300]  либо на их среднее, либо более хитрый метод кодирования. Мы считаем выражение A плюс B минус C и\n",
            "[09:46.300 --> 09:55.380]  кодируем через тот пиксель, который наиболее близок к этому выражению. Что тут происходит?\n",
            "[09:55.380 --> 10:04.780]  Мы обходим все наши пиксели. У нас есть для каждой строки специально управляющий символ,\n",
            "[10:04.780 --> 10:11.940]  это метод кодирования. И дальше, в зависимости от метода кодирования, мы числа, которые будем\n",
            "[10:11.940 --> 10:19.740]  кодировать, считаем определенным способом. То есть был X, стал X минус A. Получается такая\n",
            "[10:19.740 --> 10:25.060]  последовательность чисел. Дальше, после того, как мы это все вытянули в одну последовательность,\n",
            "[10:25.060 --> 10:33.740]  нам нужно это сжать. Как правило, в методах сжатия картинок есть два этапа. Первый этап – это\n",
            "[10:33.740 --> 10:39.180]  картинка-специфичный метод, то есть как раз использование вот этой двухмерной информации.\n",
            "[10:39.180 --> 10:46.860]  В PNG попроще, в JPEG немножко посложнее будет. И после того, как мы вытянули картинку каким-то\n",
            "[10:46.860 --> 10:53.500]  способом в последовательность каких-то чисел или символов, мы еще применяем общий метод сжатия.\n",
            "[10:53.500 --> 10:59.220]  В данном случае используется метод deflate. Вам он, наверное, больше известен методом,\n",
            "[10:59.220 --> 11:04.900]  который используется в формате zip. Это общий метод кодирования, который можно применять к данным\n",
            "[11:04.900 --> 11:09.980]  произвольной природы. Можно применять к тексту, но текст обычно кодируется или уникодом.\n",
            "[11:09.980 --> 11:19.060]  Соответственно, этот метод универсальный, он хорошо достаточно жмет последовательности общего вида,\n",
            "[11:19.060 --> 11:23.980]  но для картинок его недостаточно. То есть мы можем использовать некоторые prior,\n",
            "[11:23.980 --> 11:28.820]  некоторую специальную информацию про картинки, которые нам позволят картинке жимать эффективнее.\n",
            "[11:28.820 --> 11:37.940]  Следующая интересная вещь – это обход изображения. PNG опционально позволяет нам достаточно\n",
            "[11:37.940 --> 11:44.220]  эффективно получать уменьшенные копии изображения. То есть представьте, что у вас медленный интернет,\n",
            "[11:44.220 --> 11:49.500]  вот раньше были модемы, сейчас, наверное, какой-нибудь мобильный интернет и не в мегаполисе,\n",
            "[11:49.500 --> 11:58.860]  который медленно грузится. Соответственно, по мере того, как вы загружаете к себе на телефон файл PNG,\n",
            "[11:58.860 --> 12:06.580]  вы можете прочитать его начальный кусочек и отрендерить либо уменьшенную копию изображения,\n",
            "[12:06.580 --> 12:13.500]  либо не очень качественную версию картинки. Как это делается? Мы берем наше изображение,\n",
            "[12:13.500 --> 12:22.420]  делим его на блоки размером 8 на 8. И дальше вместо того, чтобы напрямую проходить по всем пикселям,\n",
            "[12:22.420 --> 12:30.540]  мы начинаем обходить пиксели в блоках по определенным правилам. В первом проходе мы берем по одному\n",
            "[12:30.540 --> 12:38.140]  пикселю из каждого блока, просто в левый верхний угол. Соответственно, кодируем их в картинке в первую очередь.\n",
            "[12:38.140 --> 12:48.340]  Если мы эту картинку получили, то мы можем взять начало файла, получить как раз вот эти изображения,\n",
            "[12:48.340 --> 12:56.220]  и мы получим картинку в 8 раз меньше. Дальше следующий второй обход мы добавляем еще по одному пикселю\n",
            "[12:56.220 --> 13:02.420]  из блока, потом у нас на третьем проходе еще парочка пикселей из блока добавляется, то есть у нас уже\n",
            "[13:02.420 --> 13:12.140]  здесь картинка уменьшенная в 4 раза. И так далее, по этим правилам мы делаем 7 проходов, и кодирование в этих\n",
            "[13:12.140 --> 13:20.860]  проходах происходит независимо. То есть мы считаем, что вот это под выборкой пикселей у нас это определенная\n",
            "[13:20.860 --> 13:28.300]  строка, и вот по правилам, как я выше рассказал, происходит кодирование. Соответственно, это то,\n",
            "[13:28.300 --> 13:34.660]  что касается png. Есть ли какие-то вопросы? Да, есть небольшой вопрос. Вот вы когда сказали,\n",
            "[13:34.660 --> 13:41.380]  как png кодируется, вы написали там последовательность 0, 1, 2, 3, 4, что такое? Это различные способы или это все\n",
            "[13:41.380 --> 13:46.820]  эти разности тоже записываются в последовательность? Вот здесь вот. Все пять разностей записываются или\n",
            "[13:46.820 --> 13:55.300]  только одна из? Нет, нет, нет. Мы перешли на строчку и пробежались по всем пикселям и поняли, что вот,\n",
            "[13:55.300 --> 14:01.060]  например, у нас есть какая-то вертикальная зависимость, и x очень сильно похож на b. То есть мы\n",
            "[14:01.060 --> 14:09.140]  пытаемся получить числа как можно меньше, чтобы было у нас как можно меньше избыточности. Соответственно,\n",
            "[14:09.140 --> 14:15.220]  мы решаем, что у нас метод кодирования для этой строчки будет 2. Записываем сначала специально\n",
            "[14:15.220 --> 14:22.220]  управляющий символ, который будет вот как раз этот второй способ кодирования обозначать, и дальше\n",
            "[14:22.220 --> 14:31.180]  бежим уже по всем пикселям и просто считаем x-b для каждого пикселя в этой строке. То есть для каждой\n",
            "[14:31.180 --> 14:37.180]  строчки как бы свой метод условно? Да. Но видимо первую строчку никак, ну нельзя закодировать нормально,\n",
            "[14:37.180 --> 14:46.700]  ее просто по-честному переписывают, самую верхнюю. Нет. А, да, это правильный вопрос. Хороший вопрос.\n",
            "[14:46.700 --> 14:53.900]  Наверное, да, на самом деле. Вот эти граничные вещи я на самом деле не уточнял. Или что будет, если у нас...\n",
            "[14:53.900 --> 15:00.380]  Я знаю. Там отдельно говорится, первая строчка тоже может выбирать тип, но если она пытается\n",
            "[15:00.380 --> 15:07.580]  смотреть наверх или влево, то там нули. О, как? То есть... Ну, у нас один проблем.\n",
            "[15:07.580 --> 15:15.140]  То есть мы предпочитаем задекодить. Но я подозреваю, что если у нас x-a, и мы вот на этот пиксель смотрим,\n",
            "[15:15.140 --> 15:24.020]  то у нас тоже, наверное, он смотрит на ноль. Ну да. Окей, спасибо, не знал. Так, давайте теперь к\n",
            "[15:24.020 --> 15:31.460]  JPEG перейдем. JPEG – метод сжатия с потерями. Соответственно, как я сказал, бывают метрики\n",
            "[15:31.460 --> 15:38.340]  формальные, бывают метрики неформальные. Финальная метрика, с которой мы хотим работать – это\n",
            "[15:38.340 --> 15:43.540]  человеческое восприятие. Формализовать очень сложно. Мы немножко вот сейчас поговорим про особенности\n",
            "[15:43.540 --> 15:50.220]  человеческого восприятия. Соответственно, мы эту тему уже касались первой лекции, и там я говорил,\n",
            "[15:50.220 --> 15:56.580]  что вот когда проводят всякие цветовые эксперименты или там эксперименты с яркостью, еще с чем-то, с\n",
            "[15:56.580 --> 16:03.860]  какими-то паттернами, то исследователи, обычно это в Штатах происходило или в UK, брали каких-то\n",
            "[16:03.860 --> 16:12.020]  наблюдателей. Вот я рассказывал про то, что пространство РГБ получено, по-моему, семью\n",
            "[16:12.020 --> 16:17.980]  наблюдателями. Там это называется стандартный наблюдатель – это все мужчины, и по-моему, из UK.\n",
            "[16:17.980 --> 16:24.980]  Соответственно, тут примерно та же самая история. Нам, чтобы понять, на что человек обращает в\n",
            "[16:24.980 --> 16:29.500]  картинке внимание, на что не обращает, нам нужно собирать каких-то наблюдателей и проводить\n",
            "[16:29.500 --> 16:37.300]  эксперименты. Эти все эксперименты были в 60-х, 70-х годах. То есть мир тогда был совершенно другим,\n",
            "[16:37.300 --> 16:45.540]  но при этом эта специфика исследований с конкретными наблюдателями, она прямо запечатлена в формате.\n",
            "[16:45.540 --> 16:53.820]  Как она запечатлена и что они делали? Есть известное исследование, в котором наблюдателям показывали\n",
            "[16:53.820 --> 17:02.740]  синусоиды. У нас есть какая-то установка, есть какой-то фиксированный экран, какое-то\n",
            "[17:02.740 --> 17:09.380]  фиксированное расстояние. Дальше у людей измеряется острота зрения и, может быть, как-то компенсируется,\n",
            "[17:09.380 --> 17:17.060]  чтобы она тоже была приведенной к общему знаменателю. У синусоиды есть направление\n",
            "[17:17.060 --> 17:23.060]  с одной стороны, с другой стороны есть амплитуда, то есть минимальное и максимальное значение.\n",
            "[17:23.060 --> 17:32.540]  Дальше мы можем фиксировать какое-то направление и начинать менять амплитуду. Будем ее уменьшить\n",
            "[17:32.540 --> 17:40.540]  и видим, что в какой-то момент, сильно зависит на самом деле, насколько вы близко находитесь к экрану,\n",
            "[17:40.540 --> 17:48.940]  но если вы достаточно далеко от него отойдете, то тогда у вас картинка на слайде станет\n",
            "[17:48.940 --> 17:55.420]  неразличимой серой. Соответственно исследователи меряли, в какой момент вот эти изменения,\n",
            "[17:55.420 --> 18:02.100]  которые они моделировали синусоидами, становятся неразличимыми. Если это все свести в один график,\n",
            "[18:02.100 --> 18:10.820]  то получится вот такая интересная картинка. Здесь по x частота, то есть частота возрастает,\n",
            "[18:10.820 --> 18:25.420]  а по y убывание амплитуды. Соответственно, в зависимости от того, насколько близко вы находитесь,\n",
            "[18:25.420 --> 18:30.740]  вы можете посмотреть на этот рисунок. Видите, он здесь комбинированный, то есть здесь разные амплитуды,\n",
            "[18:30.740 --> 18:36.860]  разные частоты. Я сейчас нарисую, я сейчас в очках сижу, у меня достаточно хороший экран у ноутбука,\n",
            "[18:36.860 --> 18:44.180]  я нарисую, пройду линию, после которой мне эти синусоиды становятся неразличимыми.\n",
            "[18:44.180 --> 18:55.020]  Вот примерно так она будет выглядеть. У вас она будет выглядеть по-другому. Если\n",
            "[18:55.020 --> 19:05.460]  провести такой эксперимент с несколькими людьми, усреднить, да еще в добавок проверять не только\n",
            "[19:05.460 --> 19:11.660]  чувствительность к яркости, как здесь нарисовано, но и чувствительность к цвету, то мы можем получить\n",
            "[19:11.660 --> 19:18.580]  три следующих графика. Здесь уже поменялся масштаб, он алгорифмический, но суть осталась та же\n",
            "[19:18.580 --> 19:27.460]  самая. По горизонтали у нас частота, по вертикали у нас это названо чувствительностью контрасту,\n",
            "[19:27.460 --> 19:35.540]  но по сути это убывание контраста. Соответственно у нас здесь три канала, это luminance, будем назвать\n",
            "[19:35.540 --> 19:46.980]  Y, red-green компонента, то есть это компонента красного цвета, будем ее называть CR, color red,\n",
            "[19:46.980 --> 19:55.060]  и компонента синежелтого, тоже вторая цветовая компонента, color blue. Соответственно видно,\n",
            "[19:55.060 --> 20:05.100]  что мы воспринимаем яркость и цвет совершенно по-разному. К яркости мы намного сильнее чувствитель,\n",
            "[20:05.740 --> 20:12.820]  и если мы хотим делать какой-то хороший метод ржатия изображений, то мы должны этот факт учитывать,\n",
            "[20:12.820 --> 20:18.900]  потому что цель у нас обмануть человека, то есть человек не должен заметить, что у нас после\n",
            "[20:18.900 --> 20:26.940]  рожатия что-то поменялось. Это вот если говорить научно, но есть простой эксперимент, который\n",
            "[20:26.940 --> 20:32.900]  позволит вам убедиться, как мы по-разному чувствительны к яркости и к цвету. У нас есть\n",
            "[20:32.900 --> 20:40.980]  изображение в пространстве RGB, мы его разложим на яркость и на цвет. Дальше возьмем гауссовский\n",
            "[20:40.980 --> 20:49.660]  фильтр и будем размывать либо яркость, либо цвет. Соответственно что у нас получится? Вот если мы\n",
            "[20:49.660 --> 20:55.500]  цвет размоем, то у нас получится достаточно хорошо различимое изображение, оно такое немножко вентажное\n",
            "[20:55.500 --> 21:04.740]  с блеклыми цветами, но в целом все понятно. Мальчик мороженое, у него салфеточка видимо какая-то и\n",
            "[21:04.740 --> 21:13.180]  так далее. А вот если мы размываем яркость, то изображение, при этом тут размытие одной и той же\n",
            "[21:13.180 --> 21:20.700]  гауссианой. Видно, что изображение у нас очень неразличимо, плохо различимым стало, какие-то общие\n",
            "[21:20.700 --> 21:27.300]  детали мы видим, но намного меньше, чем если мы размывали цвет. Вот такой простой эксперимент,\n",
            "[21:27.300 --> 21:35.580]  который нам показывает, что действительно для нас яркость намного сильнее важна. Следующая история это\n",
            "[21:35.580 --> 21:47.180]  как нам посчитать яркость. В целом я рассказывал про палочки, про колбочки, про то, что яркость\n",
            "[21:47.180 --> 21:59.220]  это восприятие палочками и колбочками цвета, вот именно в смысле яркости, то есть такая общая\n",
            "[21:59.220 --> 22:05.740]  характеристика. И рассказывал, что есть способ эту яркость считать. Соответственно, тут давайте\n",
            "[22:05.740 --> 22:13.820]  проведем некоторый эксперимент. У нас есть листик, и я здесь привел два варианта, как можно посчитать\n",
            "[22:13.820 --> 22:22.100]  яркость этого листика. Давайте проголосуем, кому какой вариант больше нравится, то есть какая яркость\n",
            "[22:22.100 --> 22:27.700]  интуитивно вам кажется, что больше соответствует вот этому цветному изображению. Чтобы проголосовать,\n",
            "[22:27.700 --> 22:35.140]  напишите в чат либо L, либо R, соответственно, какой вариант вам больше нравится.\n",
            "[22:43.820 --> 22:58.500]  Так, окей, по-моему, все написали R. Интересно, потому что я вот этот опрос, а вот L еще немножко есть. Я этот\n",
            "[22:58.500 --> 23:05.660]  опрос провожу каждый год, вот в таком формате, и в прошлом году подавляющее большинство голосов\n",
            "[23:05.660 --> 23:14.340]  было за L. Это очень интересно. Смотрите, как считалась яркость. У нас есть канал R, G, B. Слева мы берем\n",
            "[23:14.340 --> 23:22.780]  просто три канала и их усредняем, таким образом получаем яркость. А справа у нас будет 0,3 умноженное\n",
            "[23:22.780 --> 23:30.140]  на R, я примерно пишу, там дальше будет точная формула, плюс 0,6 умноженное на G, плюс 0,1 умноженное\n",
            "[23:30.140 --> 23:39.260]  на B. Правый вариант, он правильный, то есть это больше соответствует тому, как мы яркость воспринимаем.\n",
            "[23:39.260 --> 23:47.780]  Зеленый цвет, он очень интенсивный, мы его воспринимаем очень хорошо. За мерами восприятия\n",
            "[23:47.780 --> 23:56.100]  яркости занимается наука фотометрия, и они проводят как раз ученые, проводят эксперименты,\n",
            "[23:56.100 --> 24:04.060]  насколько мы чувствительны к разным длину волн. Это все обобщается в функции. В данном случае\n",
            "[24:04.060 --> 24:10.980]  нарисована фотопика luminosity function, то есть функция чувствительности к яркости, да еще фотопик.\n",
            "[24:10.980 --> 24:17.900]  Фотопик это переводится как чувствительность при нормальном освещении, дневном освещении. Бывает\n",
            "[24:17.900 --> 24:23.860]  чувствительность при дневном освещении, бывает еще чувствительность в темноте, она там немножко по\n",
            "[24:23.860 --> 24:31.620]  другому выглядит, немножко сдвинута. Соответственно, тут нормализованный график в зависимости от длины волны,\n",
            "[24:31.620 --> 24:40.780]  некоторое число от 0 до 1, насколько мы чувствительны к данной длине волны. Соответственно,\n",
            "[24:40.780 --> 24:49.900]  если мы примерно скажем, что здесь у нас длинный волн, который соответствует сине компоненте,\n",
            "[24:49.900 --> 24:59.580]  примерно здесь длинный волн, который соответствует зеленой компоненте, помните, что там отклики,\n",
            "[24:59.580 --> 25:07.700]  все подобные вещи, поэтому это все условно. Видно, что к зеленой компоненте мы очень чувствительны,\n",
            "[25:07.700 --> 25:15.420]  дальше к красной и затем поменьше к сеней. Соответственно, это такое интуитивное описание,\n",
            "[25:15.420 --> 25:25.780]  как мы чувствительны к этим компонентам RGB. И это все нам понадобится для JPEG. Соответственно,\n",
            "[25:25.780 --> 25:35.420]  давайте перейдем к JPEG. Тут у нас уже пошло описание метода. Первый шаг метода сжатия изображений\n",
            "[25:35.420 --> 25:43.540]  JPEG с потерями – это переход из пространства RGB. JPEG обычно с RGB работает, но там есть еще вариант,\n",
            "[25:43.540 --> 25:49.860]  с MIG можно работать, но они там достаточно просто переводятся друг из друга. Из RGB мы переводим\n",
            "[25:49.860 --> 25:56.700]  в то самое пространство YCBCR, про которое мы как раз рисовали графики чувствительности. Соответственно,\n",
            "[25:56.700 --> 26:08.340]  Y у нас будет почти 0.3 умноженное на R, плюс 0.587 на G и плюс 0.114 на B. Примерно те же самые веса,\n",
            "[26:08.340 --> 26:14.180]  про которые я и говорил. Дальше есть правило, по которому мы будем считать цветовые компоненты.\n",
            "[26:14.180 --> 26:20.060]  Яркость достаточно легко визуализировать. Цветовые компоненты немножко сложнее,\n",
            "[26:20.060 --> 26:27.020]  но вот чтобы примерно понимать, как они устроены, это вот так. При этом мы говорили,\n",
            "[26:27.020 --> 26:33.660]  что к яркости и к цвету мы чувствительны по-разному. Первый шаг сжатия в JPEG – это\n",
            "[26:33.660 --> 26:39.880]  мы можем яркость количества информации, которую мы выделяем на цветовые компоненты,\n",
            "[26:39.880 --> 26:48.940]  немножко уменьшить. Мы сделаем это путем уменьшения разрешения, например, в два раза по каждой стране.\n",
            "[26:48.940 --> 26:55.580]  Таким образом, вы можете посчитать, мы уже в два раза уменьшили наши изображения,\n",
            "[26:55.580 --> 27:03.540]  но в целом этого еще недостаточно. При этом, когда у нас обратно будет рожатие из таких\n",
            "[27:03.540 --> 27:10.900]  уменьшенных версий картинок, мы будем просто разжимать напрямую, увеличивать разрешение и вот так\n",
            "[27:10.900 --> 27:16.620]  растягивать. То есть при этом проблем у нас никакой не будет. Это один из вариантов,\n",
            "[27:16.620 --> 27:24.420]  как мы можем выкинуть информацию из цветовых каналов. Обычно это делается путем сглаживания\n",
            "[27:24.420 --> 27:30.660]  гусяной, а потом, например, можно брать каждый второй пиксель по каждой стране.\n",
            "[27:30.660 --> 27:43.260]  Перейдем к следующему более интересному шагу. Для того, чтобы нам дальше сжимать изображение,\n",
            "[27:43.260 --> 27:49.620]  мы возьмем эти три канала и каждый канал поделим на блоке размером 8 на 8 пикселей,\n",
            "[27:49.620 --> 27:57.260]  как в png. 8 на 8 – достаточно стандартный блок. И дальше мы будем работать с каждым блоком\n",
            "[27:57.260 --> 28:03.740]  независимо. И мы сделаем такой zoom in, перейдем к значениям, которые содержатся в конкретном блоке.\n",
            "[28:03.740 --> 28:14.380]  Собственно мы сделали zoom in, вот у нас значение. Блок 8 на 8, значение от 0 до 255. В jpeg обычно\n",
            "[28:14.380 --> 28:21.060]  используются байты, потому что если мы работаем с байтами, то у нас будут более эффективные операции\n",
            "[28:21.060 --> 28:29.580]  на CPU. С целыми числами CPU работает шустрее. Первое преобразование, которое мы делаем с блоком,\n",
            "[28:29.580 --> 28:36.060]  это мы вычитаем 128. Зачем это нужно, я поясню дальше. По сути, у нас значение становится\n",
            "[28:36.060 --> 28:43.300]  центрированное относительно нуля. То есть у нас значение становится от минус 128 до 127.\n",
            "[28:43.300 --> 28:54.700]  Работаем теперь с этим блоком. Следующая интересная вещь, ключевая вещь в jpeg – это\n",
            "[28:54.700 --> 29:02.860]  дискретное косиностное преобразование. У нас есть блок, у него есть какие-то значения,\n",
            "[29:02.860 --> 29:13.140]  есть какая-то индексация x и y. То есть здесь у нас какие-то координаты пикселей от 0 до 1.\n",
            "[29:13.140 --> 29:26.140]  От 0 до 7. И теперь нам нужно перейти в другой базис. Здесь у нас пиксели друг от друга не\n",
            "[29:26.140 --> 29:34.700]  зависят. При этом мы знаем, что пиксели, вообще говоря, в изображении друг на друга достаточно\n",
            "[29:34.700 --> 29:40.940]  сильно влияют. Поэтому нам нужно перейти в какой-то другой базис, который влияние пикселей будет\n",
            "[29:40.940 --> 29:48.980]  более четко учитывать. Соответственно, базис выглядит следующим образом. Здесь он нарисован такой\n",
            "[29:48.980 --> 29:57.260]  дискретный базис. Точную формулу вы посмотрите в семинарии в домашке. По сути, это функция на сетке\n",
            "[29:57.260 --> 30:03.820]  размером 8 на 8 пикселей. Здесь я для удобства нарисовал их в виде картинок. Видно, что это\n",
            "[30:03.820 --> 30:16.060]  синусоида. То есть это cos k1 на x умножено на cos k2 на y. Соответственно, в зависимости от значения\n",
            "[30:16.060 --> 30:24.060]  k1 на k2, мы будем получать разные комбинации синусоид по двум осям. Видно, что здесь у нас идут\n",
            "[30:24.060 --> 30:32.140]  синусоида. У них увеличивается частота и при этом меняются они только в одном направлении.\n",
            "[30:32.140 --> 30:41.180]  То же самое вот так. Соответственно, чем дальше в правый нижний угол, тем более\n",
            "[30:41.180 --> 30:49.340]  высокочастотные синусоида у нас будут располагаться. Дальше нам нужно этот блок разложить по базису.\n",
            "[30:49.340 --> 30:55.660]  В простейшем случае, в наивном случае, мы можем сделать это за квадратичное время.\n",
            "[30:55.660 --> 31:01.020]  Следующим образом, у нас есть какая-то картинка. Давайте нарисуем букву А.\n",
            "[31:01.020 --> 31:09.180]  Например, она как-то изображается. Мы начинаем идти с левого верхнего угла. У нас будет какой-то\n",
            "[31:09.180 --> 31:18.020]  коэффициент А0, умноженный вот на этот базис. Дальше пойдем как в мотоне вправо и там будет\n",
            "[31:18.020 --> 31:26.900]  диагональный обход. Плюс А1, умноженное вот на эту синусоиду. Дальше идем вот так по диагонали.\n",
            "[31:26.900 --> 31:39.500]  Плюс А2, умноженное на эту синусоиду и так далее. Плюс А63, умноженное на вот этот базис.\n",
            "[31:39.500 --> 31:50.860]  Мы считаем скалярные произведения нашего блока с базисом и получаем коэффициент разложений.\n",
            "[31:50.860 --> 32:00.260]  Можно делать это более эффективно и дискретное косинусное преобразование позволяет это делать не за квадрат, а шустрее.\n",
            "[32:00.260 --> 32:08.980]  Обычное дискретное косинусное преобразование есть, быстрое. После того, как мы сделали это разложение,\n",
            "[32:08.980 --> 32:18.420]  эти коэффициенты А0 и так далее А63 мы запишем в матрицу тоже размера 8 на 8. То есть вот это у нас А0,\n",
            "[32:18.420 --> 32:28.140]  вот это у нас А1 и так далее А63. То есть мы сделали разложение перехода к другой базис.\n",
            "[32:28.140 --> 32:36.780]  При этом вот этот блок, который здесь используется для примера, это блок из изображения реального.\n",
            "[32:36.780 --> 32:43.980]  То есть JPEG работает с фотографиями, портретами, ландшафтами и так далее. То есть все,\n",
            "[32:43.980 --> 32:56.900]  что люди обычно фотографируют и передают в интернете. При этом можно увидеть, что здесь амплитуды больше по модулю,\n",
            "[32:56.900 --> 33:04.140]  здесь амплитуды меньше. То есть при повышении частоты у нас амплитуды становятся все меньше и меньше.\n",
            "[33:04.140 --> 33:13.100]  Это предположение, которое есть об изображении в JPEG. В целом так оно и происходит в натуральных изображениях.\n",
            "[33:13.100 --> 33:23.420]  Основная идея JPEG состоит в том, что мы можем вот эти амплитуды занулить аккуратненько,\n",
            "[33:23.420 --> 33:33.140]  и таким образом ими пренебрячить, человек их не заметит. Особенность теста на восприятие\n",
            "[33:33.140 --> 33:38.900]  человека показывает, что человек к низким частотам более чувствителен, чем к высоким.\n",
            "[33:38.900 --> 33:44.740]  Это как раз вопрос о графиках, которые мы разбирали. Давайте теперь формализуем\n",
            "[33:44.740 --> 33:53.900]  вот эту потерю информации. У нас есть амплитуды, и дальше у нас появится такая вещь как матрица квантования.\n",
            "[33:53.900 --> 34:02.700]  То есть мы возьмем амплитуды и поэлементно поделим на матрицу квантования. Например 235,6,\n",
            "[34:02.700 --> 34:13.420]  мы будем делить на соответствующее число 16. После того как поделили, мы округляем и получаем 15,\n",
            "[34:13.420 --> 34:19.740]  записываем в матрицу квантованных амплитуд. То есть потеря информации происходит при округлении.\n",
            "[34:19.740 --> 34:27.420]  Если мы работаем во флотах, то нам в целом точности хватит, но вот здесь мы намеренно округляем,\n",
            "[34:27.420 --> 34:34.700]  чтобы произошла потеря информации. Здесь приведен пример матрицы квантования. Обратите внимание,\n",
            "[34:34.700 --> 34:42.980]  что здесь значения небольшие, здесь значения больше. Соответственно, матрица квантования,\n",
            "[34:42.980 --> 34:50.540]  она определяет, насколько сильно, какой частотой мы будем пренебрегать. Чем значение больше,\n",
            "[34:50.540 --> 34:56.020]  тем сильнее будем пренебрегать. В результате мы получаем квантованные амплитуды, у которых,\n",
            "[34:56.020 --> 35:02.860]  как правило, в правом нижнем углу сплошные нули. Если у вас в правом нижнем углу не ноль,\n",
            "[35:02.860 --> 35:09.780]  это значит, что у вас попалась какая-то высокая частота, и это изображение может быть не совсем\n",
            "[35:09.780 --> 35:16.780]  подходит для сжатия с помощью jpeg. Например, если у вас изображение из текста, какой-нибудь\n",
            "[35:16.780 --> 35:25.300]  скриншот, если вы делаете скриншоты на компьютере и жмете их jpeg, то потом при рожатии вы сразу это\n",
            "[35:25.300 --> 35:33.180]  увидите, потому что изображение текста жмёт с jpeg очень плохо. Теперь посмотрим на матрицу квантования\n",
            "[35:33.180 --> 35:41.740]  для яркости и для цвета. Для цвета для обоих каналов одна и та же матрица квантования. Соответственно,\n",
            "[35:41.740 --> 35:54.740]  я тут для нагрядности обведу значение больше 99. Вот для яркости и вот тут для цвета. То есть\n",
            "[35:54.740 --> 36:01.940]  видно, что цвет мы квантуем сильнее, и это соответствует тем графикам, которые мы ранее обсуждали.\n",
            "[36:01.940 --> 36:12.500]  Это стандартные матрицы квантования, которые прописаны в стандарте jpeg. Но в целом вам никто\n",
            "[36:12.500 --> 36:22.100]  не мешает придумывать свои матрицы квантования, и формат jpeg позволяет в вот этот файл.jpeg записать\n",
            "[36:22.100 --> 36:27.500]  матрицу квантования какую-то собственную. Этим пользуется Photoshop, и дальше даже до сих пор\n",
            "[36:27.500 --> 36:33.340]  исследователи придумывают свои матрицы квантования, которые более эффективно подходят к разным доменам.\n",
            "[36:33.340 --> 36:42.220]  Матрица квантования помимо того, что вот она такая задана в стандарте, она может ещё зависеть от\n",
            "[36:42.220 --> 36:50.540]  специального числа Q. Это число, которое задаёт качество, то есть силу жатия или качество\n",
            "[36:50.540 --> 36:57.260]  картинки. Это тот ползунок, который вы можете обычно двигать в графическом редакторе вроде\n",
            "[36:57.260 --> 37:06.700]  Photoshop и регулировать качество. В зависимости от этого числа Q происходит вычисление новой\n",
            "[37:06.700 --> 37:12.820]  матрицы квантования, и она используется для жатия. В домашней части вы более подробно с этим\n",
            "[37:12.820 --> 37:20.420]  познакомитесь и увидите, как там это всё формируется. Матрицы квантования вообще, откуда они взялись\n",
            "[37:20.420 --> 37:29.540]  в стандарте? Тут может быть два соображения. Мы говорили про то, что мы хотим работать с\n",
            "[37:29.540 --> 37:37.780]  человеческим зрением, соответственно проводили эксперименты, каким частотам человек чувствителен.\n",
            "[37:37.780 --> 37:44.860]  В зависимости от этого, в зависимости от направления выбирали разные коэффициенты. И другой способ – это\n",
            "[37:44.860 --> 37:51.220]  подсчёт статистики реальных изображений, которые мы хотим сжимать. При подготовке стандарта смотрели\n",
            "[37:51.220 --> 37:59.020]  на какие-то примеры изображений, которые могли быть интересны. Например, если вы посмотрите на\n",
            "[37:59.020 --> 38:10.540]  это число для квантования определенной синусоиды и такое число, то есть эти два коэффициента\n",
            "[38:10.540 --> 38:15.340]  соответствуют синусоидам в разных направлениях, там горизонтально или вертикально, но с одинаковой\n",
            "[38:15.340 --> 38:23.740]  частотой. И при этом видно, что мы их по-разному будем квантовать. Число 18 соответствует вот такому\n",
            "[38:23.740 --> 38:30.060]  базисному вектору, а число 24 соответствует вот такому базисному вектору. То есть авторы предполагали,\n",
            "[38:30.060 --> 38:37.620]  что к таким изменениям яркости мы чувствительно сильнее, чем к таким. И, например, если вы посмотрите\n",
            "[38:37.620 --> 38:44.780]  на какой-нибудь ландшафт, то там действительно вот эти горизонтальные, точнее вертикальные переходы,\n",
            "[38:44.780 --> 38:49.740]  они будут встречаться чаще, потому что у нас изображение как раз горизонтально ориентировано\n",
            "[38:49.740 --> 38:56.420]  слева направо, если мы говорим про европейский взгляд на изображение. Так, соответственно,\n",
            "[38:56.420 --> 39:04.180]  давайте дальше. После того, как мы квантовали наши амплитуды, мы делаем обход такой же,\n",
            "[39:04.180 --> 39:11.620]  диагональный, и выписываем подряд числа, которые у нас получаются. У нас получается какая-то\n",
            "[39:11.620 --> 39:17.380]  последовательность. В начале какие-то не нулевые числа, в конце, как правило, сплошные нули.\n",
            "[39:17.380 --> 39:25.380]  Последовательность таких чисел, как и в png, мы будем сжимать общими методами кодирования. Первый\n",
            "[39:25.380 --> 39:32.220]  метод – это RLE для нулевых последовательностей, который состоит в следующем. Если у нас есть,\n",
            "[39:32.220 --> 39:38.700]  например, парочка подряд идущих нулей, то мы их записываем как 0.2. Или здесь у нас n нулей,\n",
            "[39:38.700 --> 39:45.620]  мы их записываем как 0.n. И это будет специальный символ, который мы будем дальше в последовательности\n",
            "[39:45.620 --> 39:52.100]  кодировать тоже. После того, как мы сжали эту последовательность именно от нулей, например,\n",
            "[39:52.100 --> 39:59.300]  минус 1 они не будут заменяться на минус 1.3. Такого происходить не будет, поэтому называется\n",
            "[39:59.300 --> 40:11.420]  0.1. И дальше мы натравливаем на эту последовательность общий метод жатия. В университетских курсах\n",
            "[40:11.420 --> 40:17.740]  часто проходит метод жатия Хадами Хаффмана. Есть более эффективный метод – арифметическое\n",
            "[40:17.740 --> 40:24.780]  кодирование. Но это общие методы, которые мы сейчас подробно не будем рассматривать. Итого.\n",
            "[40:24.780 --> 40:30.940]  Метод жатия изображений JPEG работает следующим образом. Во-первых, мы переходим в специальное\n",
            "[40:30.940 --> 40:37.260]  пространство YCBCR с выделенной яркостной компонентой и двумя цветовыми компонентами,\n",
            "[40:37.260 --> 40:44.700]  потому что с яркостью цветом нам нужно отдельно работать. Дальше мы делаем downsampling света либо в\n",
            "[40:44.700 --> 40:49.340]  два раза по каждой стороне, либо там есть другие варианты, можем по одной стороне, например,\n",
            "[40:49.340 --> 40:56.540]  два раза уменьшить. Дальше делим изображение, то есть все каналы независимы, делим на блоке\n",
            "[40:56.540 --> 41:04.060]  размером 8 на 8 пикселей, вычитаем 128. Зачем мы вычитали 128? Поскольку мы используем дальше\n",
            "[41:04.060 --> 41:11.180]  дискретное косинусное преобразование, а косинус относительно нуля симметричен, нам удобнее,\n",
            "[41:11.180 --> 41:19.060]  чтобы значения, с которыми мы работаем, тоже были центрированы относительно нуля. Соответственно,\n",
            "[41:19.060 --> 41:24.540]  дальше в каждом блоке мы применяем дискретное косинусное преобразование, то есть переходим из\n",
            "[41:24.540 --> 41:32.420]  пространственного базиса в частотный базис, далее делаем квантование получившихся амплитуд и сжимаем\n",
            "[41:32.420 --> 41:40.060]  общим методом сжатия квантованные коэффициенты. При этом нужно понимать, что этот алгоритм был\n",
            "[41:40.060 --> 41:47.060]  придуман для того, чтобы сжимать изображение, которое показывается на экране. И тогда в те\n",
            "[41:47.060 --> 41:54.060]  годы, в 90-е качество экранов было несравнимо с печатью. Сейчас, конечно, экраны стали более\n",
            "[41:54.060 --> 42:01.860]  качественными, но печать тоже все равно, но печать, как правило, все равно более качественная. Соответственно,\n",
            "[42:01.860 --> 42:07.260]  одно дело, если вы JPEG показываете на экране, а другое дело, если вы печатаете в печати больше\n",
            "[42:07.260 --> 42:13.300]  разрешения, все ваши артефакты сразу вылезут. Поэтому для того, чтобы, ну вообще, как правило,\n",
            "[42:13.300 --> 42:21.020]  JPEG не печатают, но если уж очень надо, то тогда нужно либо ручку качества выкручивать на максимум,\n",
            "[42:21.020 --> 42:29.700]  чтобы это почти у вас было, либо, может быть, там свою какую-то придумывать специальную матрицу\n",
            "[42:29.700 --> 42:36.260]  квантования. Обычно ручки достаточно, потому что она тоже матрицу квантования регулирует. И при\n",
            "[42:36.260 --> 42:42.020]  этом изображения сжимаются за счет того, что мы теряем высокочастотную информацию. Вот, как я\n",
            "[42:42.020 --> 42:47.380]  сказал, для каких-то приложений это может быть недопустимо. Могут быть какие-то скриншоты,\n",
            "[42:47.380 --> 42:54.140]  либо чертежи и подобные вещи. Они будут плохо сжиматься. Соответственно, про JPEG мы поговорили.\n",
            "[42:54.140 --> 43:06.540]  Есть ли какие-то вопросы? У меня есть вопросик. А можно? Давайте. На слайде, там, где мальчики\n",
            "[43:06.540 --> 43:12.900]  ел мороженое, я не очень понял, что такое blur цвета. Это мы в каком пространстве применили фильтр?\n",
            "[43:12.900 --> 43:21.300]  Это мы сходили в пространство YCBCR, взяли, например, канал CB, его размазали гусяной,\n",
            "[43:21.300 --> 43:26.820]  а потом вернулись обратно. У нас же есть правило конвертации RGB в YCBCR и обратно.\n",
            "[43:26.820 --> 43:32.500]  Все, все, все. Отлично. И тогда второй вопросик настолько же, по всей видимости, короткий. Там,\n",
            "[43:32.500 --> 43:38.100]  где у нас были картиночки с изображением ландшафта с домиком, где мы downsample-или цветовые\n",
            "[43:38.100 --> 43:45.300]  компоненты, я не очень понял, после того, как мы их с downsample-или, мы их в какой-то момент обратно\n",
            "[43:45.300 --> 43:52.100]  upsample-им? Нет. Но мы их будем upsample-ить при декодировании. А когда мы их сейчас, если мы\n",
            "[43:52.100 --> 44:00.060]  их сейчас сжимаем, то мы их точно так же делим на блоки 8х8. Так. И точно так же с блоком каждым\n",
            "[44:00.060 --> 44:04.540]  работаем. У нас тут только могут возникать граничные эффекты, то есть там пэддинг, скорее\n",
            "[44:04.540 --> 44:09.660]  всего, какой-то нужно делать будет. Но это как бы за рамками остается техническая деталь. А\n",
            "[44:09.660 --> 44:20.140]  тогда получается, что в какой момент размеры всех компонент в этом цветовом пространстве у нас\n",
            "[44:20.140 --> 44:25.980]  станут одинаковыми. Мы здесь с downsample-или разбили на блоки, потом мы к этому тоже применили\n",
            "[44:25.980 --> 44:34.260]  последующие косиностное преобразование и квантизацию. И к недownsample-ином компонентам\n",
            "[44:34.260 --> 44:41.700]  применили тоже самое. Вот в какой момент они станут одинаковыми по размеру? Когда мы это все\n",
            "[44:41.700 --> 44:49.020]  сохраняем в точке JPEG, у нас не происходит никакого upsampling-а. То есть каждый блок превращается в\n",
            "[44:49.020 --> 44:55.140]  свою сжатую последовательность, которая независимо просто как бы конкатинируется и записывается\n",
            "[44:55.140 --> 45:03.780]  подряд в файл JPEG. А потом, когда мы открываем файл JPEG, читаем его, у нас происходит как бы вот\n",
            "[45:03.780 --> 45:11.700]  весь этот метод сжатия, он идет в обратную сторону. У нас есть сжатые коэффициенты. Мы делаем\n",
            "[45:11.700 --> 45:18.660]  декодирование Huffman. Соответственно, у нас там есть словарь Huffman какой-то записанный в этом файле.\n",
            "[45:18.660 --> 45:23.700]  Либо если у нас арифметическое кодирование, там тоже другие специальные как бы сужебные вещи\n",
            "[45:23.700 --> 45:28.660]  нужные. И дальше делаем все это кодирование в обратную сторону. Это все происходит в памяти\n",
            "[45:28.660 --> 45:38.020]  компьютера, понятное дело. И потом уже возвращаем изображение в RGB. При этом тут у нас если был\n",
            "[45:38.020 --> 45:45.260]  downsample, значит здесь нам надо сделать upsample. При декодировании у нас это все в обратную сторону\n",
            "[45:45.260 --> 45:51.940]  происходит. Ага, понятно. И всякий раз, когда вьюер открывает JPEG, он выполняет последовательность\n",
            "[45:51.940 --> 46:00.700]  вот этих достаточно непростых алгоритмов. На самом деле это простые вещи, потому что вот вы посмотрите\n",
            "[46:00.700 --> 46:06.900]  на формулу, они очень простые. При этом быстрое косинусное преобразование, да еще на целых числах.\n",
            "[46:06.900 --> 46:16.700]  Это все достаточно шустро работает. Это сделано как раз чтобы декодирование было быстрым. Я понял.\n",
            "[46:16.700 --> 46:21.900]  Спасибо большое. Все, у меня кончились вопросы. Так, еще вопросы есть? Уточнить можно, пожалуйста.\n",
            "[46:21.900 --> 46:27.620]  То есть мы в результате косинусного преобразования просто получаем вектор, который именно коэффициенты\n",
            "[46:27.620 --> 46:32.940]  обозначает. Он не обозначает какие-то уже там преобразованные вещи, это просто коэффициенты,\n",
            "[46:32.940 --> 46:38.540]  которые потом можно по ним восстановить исходное изображение. Да, да, да. В данном случае у нас\n",
            "[46:38.540 --> 46:43.500]  эти амплитуды записаны, вот эти коэффициенты записаны в виде матрицы, потому что у нас здесь есть\n",
            "[46:43.500 --> 46:50.380]  все равно еще смысл именно частотный. А дальше, после того, как мы уже сделали квантование и их\n",
            "[46:50.380 --> 46:59.220]  обошли, мы уже можем работать с последовательностью. Хорошо, спасибо. Так, еще вопросы?\n",
            "[47:03.220 --> 47:09.820]  Окей, если вопросов нет, лирическое отступление, а что сейчас вообще происходит? То есть здесь мы,\n",
            "[47:09.820 --> 47:21.300]  если мы перейдем к JPEG, я вот сказал, что можно что-то делать с матрицей квантования, вот здесь,\n",
            "[47:21.300 --> 47:28.100]  то есть улучшение возможно здесь. И вот до недавних пор, по-моему, году в 2020 я еще читал,\n",
            "[47:28.100 --> 47:37.140]  что подбирали вот эти коэффициенты квантования, в том числе с помощью нейросетей. Дальше у нас\n",
            "[47:37.140 --> 47:43.220]  возникает вопрос еще базиса. Есть косинусное преобразование, бывают другие базисы, например,\n",
            "[47:43.220 --> 47:50.300]  есть вивелетный базис, который более эффективно работает. Если говорить про вивелет базис,\n",
            "[47:50.300 --> 47:58.220]  это базис ХАР, обычно называется, то есть такой метод, называется JPEG 2000, он как раз там в районе\n",
            "[47:58.220 --> 48:06.060]  2000-х годов был принят, дальше его пытались засунуть в всякие разные программы, но он более\n",
            "[48:06.060 --> 48:14.180]  эффективный, чем JPEG, но при этом он не принят де-факто, то есть мало программ его поддерживает.\n",
            "[48:14.180 --> 48:20.180]  Какой-нибудь фотошоп или какие-то продвинутые просмотрщики, не знаю там, и CDC, например,\n",
            "[48:20.180 --> 48:25.900]  на Windows, не знаю, жив ли он до сих пор, но много всяких есть продвинутых просмотрщиков,\n",
            "[48:25.900 --> 48:31.740]  они это все открывают. А если мы говорим про какие-нибудь там браузеры, которые основной,\n",
            "[48:31.740 --> 48:39.260]  основное потребление контента сейчас через браузеры, в браузерах JPEG 2000 не во всех есть,\n",
            "[48:39.260 --> 48:44.740]  можете посмотреть на досуге, где он вообще есть. И там возникает проблема, что у нас есть как бы\n",
            "[48:44.740 --> 48:53.700]  N браузеров, у нас есть N просмотрщиков, и вот если у нас есть, например, какой-нибудь Safari и Chrome,\n",
            "[48:53.700 --> 49:00.060]  которые являются там де-факто стандартами либо в операционной системе, либо вот просто\n",
            "[49:00.060 --> 49:06.340]  распространенный браузер Chrome, если компания Apple или там компания Google решает, что мы вот такой\n",
            "[49:06.340 --> 49:15.900]  хороший, какой-то новый формат не хотим поддерживать, то этот формат на самом деле обречен. Такое было\n",
            "[49:15.900 --> 49:23.820]  много-много раз, то есть связано это с тем, что компании в целом придумывают свои собственные\n",
            "[49:23.820 --> 49:30.620]  форматы и пытаются вот такими рыночными или полурыночными способами как-то на это влиять.\n",
            "[49:30.620 --> 49:38.820]  То есть возникает какая-то монополия, не монополия, но вот это связанные вещи. Соответственно,\n",
            "[49:38.820 --> 49:50.100]  множество форматов, которые были счастливо приняты сообществом, из последних интересных,\n",
            "[49:50.100 --> 49:57.380]  это есть еще JPEG XL тоже был принят. Вот JPEG XL был счастливым недавно выпилен в Chrome,\n",
            "[49:57.380 --> 50:02.500]  и как-то вот Google его очень старательно игнорирует. Это на самом деле больная тема,\n",
            "[50:02.500 --> 50:08.940]  можно много чего хорошего придумать, но вот как-то потом засунуть в программы это сложно. Что\n",
            "[50:08.940 --> 50:16.900]  сейчас как бы самого прорывного? Самого прорывного сейчас JPEG AI. JPEG AI это сжатие с помощью\n",
            "[50:16.900 --> 50:24.820]  нейросетей и в том числе нейросети, которые мы будем дальше рассматривать, они работают над сырыми\n",
            "[50:24.820 --> 50:34.580]  пикселями. То есть вы взяли в JPEG файл с помощью какого-нибудь OpenCV либо JPEG, внутри все равно\n",
            "[50:34.580 --> 50:39.820]  будет находиться, с помощью JPEG прочитали, получили, раскодировали значение пикселей,\n",
            "[50:39.820 --> 50:44.460]  дальше это все подаёте в нейросетку. В целом можно было бы сказать, что а зачем нам работать с\n",
            "[50:44.460 --> 50:50.060]  пикселями, у нас же есть сжатое представление, давайте работать напрямую над вот этими квантованными\n",
            "[50:50.060 --> 50:59.100]  коэффициентами. Такие работы есть, их не так много, но в целом есть. Вот JPEG AI это развитие этой\n",
            "[50:59.100 --> 51:05.580]  идеи, что мы будем с помощью нейросетей представлять какие-то, получать какие-то коэффициенты,\n",
            "[51:05.580 --> 51:10.460]  то есть мы будем не дискретное костюмсовое преобразование делать, какое-то фиксированное,\n",
            "[51:10.460 --> 51:16.420]  будем делать какое-то обучаемое преобразование. И дальше с помощью этого преобразования\n",
            "[51:16.420 --> 51:22.300]  получать какие-то коэффициенты, дальше с ними работать. Посмотрим, насколько это будет принято.\n",
            "[51:22.300 --> 51:32.620]  JPEG AI достаточно сильное комьюнити развивает, и методы, которые в основе JPEG AI находятся,\n",
            "[51:32.620 --> 51:38.700]  мы будем разбирать в лекциях про генеративки. Там у нас будут вариационные автокодировщики,\n",
            "[51:38.700 --> 51:44.380]  их модификации. И вот по сути это как раз JPEG AI. Это будет у нас ближе к концу курса.\n",
            "[51:44.380 --> 51:50.380]  Соответственно, на всем лирическом отступлении, в котором я вот вам постарался показать,\n",
            "[51:50.380 --> 51:58.340]  насколько дремучие стандарты связаны с современностью очень сильно, мы перейдем\n",
            "[51:58.340 --> 52:03.660]  к следующей части. Это анализ с помощью преобразования фурье. Преобразование фурье\n",
            "[52:03.660 --> 52:11.660]  достаточно активно изучается в матане. И в классических университетских курсах оно счастливо\n",
            "[52:11.660 --> 52:16.980]  изучается и потом также счастливо забывается. Соответственно, мы сейчас сначала посмотрим на\n",
            "[52:16.980 --> 52:23.940]  формулы, а дальше я вам все-таки постараюсь дать интуицию, зачем оно нужно. Соответственно,\n",
            "[52:23.940 --> 52:30.060]  если мы говорим о матане, то в матане преобразование фурье — это способ перехода из одного базиса\n",
            "[52:30.060 --> 52:35.660]  в другой. То есть у нас есть какая-то функция в данном случае g от x. У нас есть какой-то\n",
            "[52:35.660 --> 52:41.660]  комплексный базис. Мы считаем скалярные произведения. Этот интеграл с нормировкой — это\n",
            "[52:41.660 --> 52:48.540]  как раз скалярное произведение. И получаем... Вот эта у нас обычно функция называется сигнал.\n",
            "[52:48.540 --> 52:58.180]  Это как раз базис. И в результате получаем те же самые амплитуды, которые зависит от омеги какой-то\n",
            "[52:58.180 --> 53:03.740]  частоты. То есть у нас те же самые синусоида, про которые мы раньше говорили, но математически это\n",
            "[53:03.740 --> 53:08.540]  немножко более сложный объект. То есть это вообще говоря комплексный базис. Спектр у нас тоже вообще\n",
            "[53:08.540 --> 53:15.500]  говоря комплексный. Идея примерно та же самая. То есть мы раскладываем функцию по базису. Мы ее разложили\n",
            "[53:15.500 --> 53:21.420]  в одну сторону, получили непрерывную функцию. Можем с помощью обратного преобразования фурье тоже\n",
            "[53:21.420 --> 53:31.420]  вернуть эту функцию, как бы восстановить эту функцию. Дальше нам нужно перейти к дискретному случаю.\n",
            "[53:31.420 --> 53:37.900]  Это картинки. Дискретные, да еще и периодические. Делать мы это будем следующим образом. Сейчас я\n",
            "[53:37.900 --> 53:46.340]  буду это показывать на пальцах. Придется поверить. Если у нас сигнал произвольный, то спектр у него\n",
            "[53:46.340 --> 53:54.140]  тоже будет какой-то произвольной формы. Если у нас непрерывный сигнал и периодический, то тогда\n",
            "[53:54.140 --> 54:03.060]  при выполнении некоторых условий на непрерывность мы можем разложить эту функцию в ряд фурье. То есть\n",
            "[54:03.060 --> 54:10.180]  спектр у нас будет дискретный. Это то, что обычно делается в матане. Все, я думаю, помнят семинары,\n",
            "[54:10.180 --> 54:20.340]  где мы берем какую-то функцию, начинаем аналитически считать коэффициенты при конкретном слагаемом в\n",
            "[54:20.340 --> 54:31.700]  ряду фурье. Следующая история. Если у нас сигнал дискретный, то тогда у него будет периодический\n",
            "[54:31.700 --> 54:41.060]  спектр. Не помню, доказывалось ли это в матане, но вот такой факт. Если мы совместим вот эти два\n",
            "[54:41.060 --> 54:49.180]  варианта, то есть если у нас сигнал периодический и сигнал дискретный, тогда у этого сигнала будет\n",
            "[54:49.180 --> 55:00.660]  периодический спектр и дискретный спектр одновременно. Что нам это дает? Это как раз история про сигналы,\n",
            "[55:00.660 --> 55:05.740]  с которыми мы обычно работаем. Представьте, что вот этот сигнал периодически это одномерное\n",
            "[55:05.740 --> 55:13.780]  изображение. У него есть m отчетов, то есть m каких-то клеточек и дальше в зависимости от значений у нас\n",
            "[55:13.780 --> 55:20.980]  вот какая-то картинка возникает. И точно так же мы переходим к спектру. У нас будет m чисел,\n",
            "[55:20.980 --> 55:30.940]  который мы можем хранить в памяти компьютера. Если мы это обобщаем на двумерный случай, то примерно\n",
            "[55:30.940 --> 55:37.460]  такая же история происходит. То есть у нас там будет m на n отчетов и будет m на n спектр. Какой\n",
            "[55:37.460 --> 55:44.460]  вывод из этого все можно сделать? Что картинки, которые мы храним в памяти компьютера, мы можем\n",
            "[55:44.460 --> 55:50.900]  считать, что это дискретный периодический сигнал и в памяти у нас все поместится. Нам нужно\n",
            "[55:50.900 --> 55:55.420]  конечное число отчетов. Точно так же у нас спектр тоже поместится в памяти компьютера.\n",
            "[55:55.420 --> 56:02.300]  Математически это будет выглядеть следующим образом. Здесь написан одномерный случай. У нас был\n",
            "[56:02.300 --> 56:08.660]  до этого интеграл, сейчас у нас сигналы дискретные, интеграл меняется на сумму. Как раз у нас есть\n",
            "[56:08.660 --> 56:20.780]  та же самая функция g сигнал. У нас базис комплексный, опять же, только дискретный. Суммируем m отчетов,\n",
            "[56:20.780 --> 56:31.340]  получаем спектр. Посчитать это можно в наивном случае за квадрат, либо если у нас быстрая\n",
            "[56:31.340 --> 56:36.820]  реализация, это обычно быстрое преобразование Fourier в продвинутых курсах алгоритмов,\n",
            "[56:36.820 --> 56:43.060]  его рассказывают, то мы можем это делать за m логи. Соответственно теперь перейдем к картинке,\n",
            "[56:43.060 --> 56:51.140]  будем визуализировать базисы для картинок. Комплексная базис, вообще говоря, достаточно\n",
            "[56:51.140 --> 57:03.460]  сложная изобразить. Здесь я нарисовал базисы амплитуды этих базисов. Изображение очень\n",
            "[57:03.460 --> 57:11.420]  примерное, но очень похоже на JPEG и должно нам дать интуицию. Соответственно здесь у нас не будет\n",
            "[57:11.420 --> 57:17.220]  разделения на блоке 8 на 8, как это было в JPEG. Здесь мы будем работать с изображениями целиком,\n",
            "[57:17.220 --> 57:21.860]  в зависимости от размера изображения у нас будет соответствующего размера базис и их будет\n",
            "[57:21.860 --> 57:30.220]  достаточно много. Посмотрим на картинку, у нас тут тигр, предположим, что он размером 600 на 400\n",
            "[57:30.220 --> 57:40.540]  пикселей. Соответственно мы начинаем этого тигра раскладывать, какое-то изображение, начинаем\n",
            "[57:40.540 --> 57:50.900]  раскладывать базис. А0, умноженное на низкочастотный базис и так далее. А600 умножить на 400 и тут\n",
            "[57:50.900 --> 57:57.540]  какой-то прям высокочастотный базис. Соответственно мы можем это разложить как в дискретном космосном\n",
            "[57:57.540 --> 58:04.340]  преобразовании, просто у нас немножко другой базис. И справа визуализированы коэффициенты.\n",
            "[58:04.340 --> 58:11.100]  При этом, поскольку у нас коэффициенты комплексные, мы показываем здесь только модуль этих комплексных\n",
            "[58:11.100 --> 58:18.220]  чисел, при этом еще с алгорифом, потому что иначе у них еще большая разница может быть. Вы подробно\n",
            "[58:18.220 --> 58:24.260]  обсудите на семинаре, но вот сейчас кратко я скажу, что для удобства визуализации обычно\n",
            "[58:24.260 --> 58:32.700]  низкочастотные компоненты находятся в центре, высокочастотные компоненты находятся ближе к углам.\n",
            "[58:32.700 --> 58:40.580]  То есть здесь видно, что у нас низкочастотных компонент больше, чем высокочастотных. Тут у нас\n",
            "[58:40.580 --> 58:47.980]  посветление, это если на пальцах. Соответственно как это выглядит в коде. Мы берем преобразование\n",
            "[58:47.980 --> 58:53.140]  фурье от изображения, дальше есть специальная функция frt-shift, про нее на семинаре поговорите,\n",
            "[58:53.140 --> 59:00.700]  которая немножко перетасовывает эти компоненты так, чтобы низкочастотные находились в центре.\n",
            "[59:00.700 --> 59:05.620]  Дальше берем модуль и засовываем это все под алгорифом с единичкой, чтобы у нас все было\n",
            "[59:05.620 --> 59:12.700]  корректно. И вот так я нарисовал эту картинку. Эта картинка естественная и видно, что все частоты\n",
            "[59:12.700 --> 59:20.420]  более-менее заполнены. Дальше перейдем к картинке рукотворного объекта. Здесь у нас есть например\n",
            "[59:20.420 --> 59:29.740]  прямые линии, этим прямым линиям будут соответствовать базисы, которые вот такие такого вида.\n",
            "[59:30.700 --> 59:37.820]  Сейчас я неправильно нарисовал, давайте вот так нарисую и вот так. Какие-то такие синусоиды\n",
            "[59:37.820 --> 59:46.360]  с разными частотами, им будут соответствовать вот эти компоненты. То есть мы видим, что на вот этом\n",
            "[59:46.360 --> 59:54.780]  рисунке амплитуд компоненты условно выстраиваются в прямую линию. Смотря на преобразование фурье,\n",
            "[59:54.780 --> 01:00:00.420]  мы можем заподозрить, что это какое-то изображение рукотворного объекта. Здесь у нас есть еще вертикальные\n",
            "[01:00:00.420 --> 01:00:06.660]  горизонтальные линии, но про них мы дальше поговорим. То есть в целом пристальным взглядом вы можете\n",
            "[01:00:06.660 --> 01:00:13.580]  смотреть на эти компоненты и как-то анализировать. Может быть пытаться например замаскировать\n",
            "[01:00:13.580 --> 01:00:20.020]  какие-то вот эти компоненты, домножить их на какое-то значение и потом вернуть изображение обратно.\n",
            "[01:00:20.020 --> 01:00:27.620]  Тогда у вас получится какая-то обработка изображений. Соответственно здесь картинки цветные,\n",
            "[01:00:27.620 --> 01:00:33.700]  это для удобства восприятия. На самом деле их переводил в ЧБ и преобразование фурье считал\n",
            "[01:00:33.700 --> 01:00:40.500]  на ЧБ. Но если вам нужно посчитать преобразование фурье над РГБ, то там делается по-канально. Теперь\n",
            "[01:00:40.500 --> 01:00:47.060]  перейдем к граничным эффектам. Мы говорили про то, что сигнал у нас дискретный и периодический. На самом\n",
            "[01:00:47.060 --> 01:00:55.060]  деле сигнал у нас как бы картинка бесконечная и у нас возникают переходы, когда этот сигнал как бы\n",
            "[01:00:55.060 --> 01:01:03.440]  происходит перескок с низа картинки наверх. При этом у нас если низ и верх картинки или там право\n",
            "[01:01:03.440 --> 01:01:11.220]  и лево сильно отличаются, то у нас могут возникать как раз вот очень высокие частоты и здесь они\n",
            "[01:01:11.220 --> 01:01:18.260]  показаны. То есть мы здесь видим вертикальная линия и горизонтальная линия. Они как раз соответствуют\n",
            "[01:01:18.260 --> 01:01:32.260]  переходам между этими соседними отчетами, когда мы повторяем картинку. Чтобы от этих, от этого\n",
            "[01:01:32.260 --> 01:01:38.540]  явления избавиться, обычно делают виндитирование, то есть накладывают какое-то окошко, берут центр\n",
            "[01:01:38.540 --> 01:01:44.420]  картинки и начинают потихонечку затемнять, чтобы здесь у нас был плавный темненький переход. И\n",
            "[01:01:44.420 --> 01:01:53.780]  тогда вот эти паразитные истории с амплитудами они пропадут. Наконец, последняя важная вещь про\n",
            "[01:01:53.780 --> 01:02:00.940]  фурье это теорема о свертке. Теорема о свертке говорит о том, что мы можем, используя преобразование\n",
            "[01:02:00.940 --> 01:02:08.780]  фурье, произвести свертку. Как это делается? У нас есть картинка тигра, у нас есть ядро к,\n",
            "[01:02:08.780 --> 01:02:14.380]  с которым мы хотим свернуть. В данном случае гауссовское ядро. Вот если мы применим операцию\n",
            "[01:02:14.380 --> 01:02:19.660]  свертки, как мы ее изучали на предыдущей лекции, тогда мы получим размытые изображения тигра.\n",
            "[01:02:19.660 --> 01:02:25.580]  При этом мы помним, что у свертки есть некоторая сложность. То есть в наивном случае,\n",
            "[01:02:25.580 --> 01:02:35.460]  если у нас там n пикси, давайте здесь n на n, здесь k на k. Соответственно у нас n квадрат на\n",
            "[01:02:35.460 --> 01:02:41.300]  k квадрат будет синтетическая сложность. Мы можем сделать это через преобразование фурье.\n",
            "[01:02:41.300 --> 01:02:47.420]  Возьмем наше ядро, дополним его нулями до подходящего размера. То есть нам уже понадобится\n",
            "[01:02:47.420 --> 01:02:54.740]  побольше памяти. Преобразование фурье в плане памяти требует больше, чем свертка. Затем мы\n",
            "[01:02:54.740 --> 01:03:02.740]  применяем преобразование фурье к изображению и к ядру. Далее они у нас получаются образы фурье,\n",
            "[01:03:03.140 --> 01:03:07.860]  вот эти матрицы с амплитудами одинакового размера. Мы их поэлементно перемножаем,\n",
            "[01:03:07.860 --> 01:03:15.300]  получаем образ фурье, который мы раскодируем с помощью обратного преобразования фурье.\n",
            "[01:03:15.300 --> 01:03:20.100]  Если вы все это аккуратно сделаете, то у вас получится тоже самое размытые изображения тигра.\n",
            "[01:03:20.100 --> 01:03:30.180]  При этом преобразование фурье позволяет избавиться от зависимости от ядра. Вместо того,\n",
            "[01:03:30.180 --> 01:03:36.660]  что у нас тут n² на k², преобразование фурье будет работать и прямой, и обратно,\n",
            "[01:03:36.660 --> 01:03:42.980]  у него одинаковая сложность. А симпатически это будет n² на log n. Соответственно, можно сравнить\n",
            "[01:03:42.980 --> 01:03:51.020]  k² с log n. Если ядро достаточно большое, то преобразование фурье будет значительно\n",
            "[01:03:51.020 --> 01:03:58.740]  экономить скорость, значительно экономить время. Это одна история. Следующая история\n",
            "[01:03:58.740 --> 01:04:07.060]  про теорема о свертке такая, что теорема о свертке позволяет нам анализировать,\n",
            "[01:04:07.060 --> 01:04:13.540]  как себя поведет фильтр. Видите, что здесь у нас гаусяна. Образ фурье на самом деле у\n",
            "[01:04:13.540 --> 01:04:22.500]  нее тоже гаусяна, это можно показать. Там только сигман меняется. Гаусяна является фильтром низких\n",
            "[01:04:22.500 --> 01:04:28.620]  частот, то есть низкие частоты остаются. Это происходит за счет того, что у нас в образе фурье у\n",
            "[01:04:28.620 --> 01:04:35.220]  гаусяны сплошные низкие частоты, высоких частот мало. Когда это все будет домножаться по элементам\n",
            "[01:04:35.220 --> 01:04:40.460]  на изображение, у нас будут оставаться только низкие частоты. Соответственно, я говорил на прошлой\n",
            "[01:04:40.460 --> 01:04:49.500]  лекции, что бокс-фильтр, у которого все элементы единички, например фильтр 3х3, является не очень\n",
            "[01:04:49.500 --> 01:04:55.420]  хорошим фильтром низких частот. Плохо размывает изображение, там возникают поразительные горизонтальные\n",
            "[01:04:55.420 --> 01:05:00.780]  вертикальные линии. Когда мы смотрим на образ фурье бокс-фильтра, мы как раз можем понять,\n",
            "[01:05:00.780 --> 01:05:06.220]  почему так происходит. У нас есть некоторые низкочастотные компоненты, похожие на гаусяну,\n",
            "[01:05:06.220 --> 01:05:13.940]  и у нас есть вот эти поразительные высокочастотные компоненты, которые как раз при свертке оставляют\n",
            "[01:05:13.940 --> 01:05:22.220]  в изображении вот эти вертикальные и горизонтальные компоненты. Для того, чтобы нам гаусяну приблизить,\n",
            "[01:05:22.220 --> 01:05:28.460]  нам нужно бокс-фильтр применять несколько раз, тогда получится достаточно хорошая\n",
            "[01:05:28.460 --> 01:05:35.300]  опроксимация, которая на самом деле в образе фурье описывается следующим образом. У нас вот эта\n",
            "[01:05:35.300 --> 01:05:41.780]  центральная часть только останется. Это как раз такое более-менее математическое обоснование,\n",
            "[01:05:41.780 --> 01:05:49.220]  почему бокс-фильтр не очень удачный фильтр низких частот. Соответственно, на анализе с помощью фурье\n",
            "[01:05:49.220 --> 01:05:58.860]  мы закончили такое описание. Есть ли какие-то вопросы по этой части? Есть пару вопросов.\n",
            "[01:05:58.860 --> 01:06:03.500]  Объясните, пожалуйста, зачем мы логарифмировали, когда вы там формулу показали. Я не очень понял,\n",
            "[01:06:03.500 --> 01:06:13.220]  зачем логарифм нужен. После логарифма 1 плюс АПС ФТ. Зачем логарифм? Просто у нас тут диапазон\n",
            "[01:06:13.220 --> 01:06:19.220]  этих амплитуд очень высокий. И для того, чтобы нам это хоть как-то удобно визуализировать,\n",
            "[01:06:19.220 --> 01:06:25.940]  чтобы у нас контрасты могли хоть что-то понять качественно, мы как раз это засовываем под\n",
            "[01:06:25.940 --> 01:06:33.660]  логарифм. То есть у вас там может быть где-то 10 по модулю числа, а где-то миллион. И вот чтобы\n",
            "[01:06:33.660 --> 01:06:40.060]  это как-то скомпенсировать, засовываем под логарифм. Это первое. Второе, бывает ли такое,\n",
            "[01:06:40.060 --> 01:06:45.740]  что у нас изображение требует меньше коэффициентов, чем вот А6 на 400 и как-нибудь\n",
            "[01:06:45.740 --> 01:06:50.220]  это соптимизировать можно? Вдруг у нас линия на зависимость вообще какая-то появляется от\n",
            "[01:06:50.220 --> 01:07:00.540]  какого-то подмножества этих? Смотрите, в общем случае, конечно, как бы нам все нужно. Но тут та же\n",
            "[01:07:00.540 --> 01:07:06.180]  история, что с JPEG. То есть какие-то частоты могут не сильно влиять. И какими-то частотами, да,\n",
            "[01:07:06.180 --> 01:07:12.500]  мы можем пренебречь. Но заранее посчитать в преобразовании фурьи, что-то откинуть,\n",
            "[01:07:12.500 --> 01:07:21.620]  вот. Но так не получится. То есть в целом это очень сильно похоже на сжатие. Но для теоремы\n",
            "[01:07:21.620 --> 01:07:28.980]  освертки и для там какого-то анализа изображения вряд ли нужно будет что-то откидывать. А не возникли\n",
            "[01:07:28.980 --> 01:07:34.500]  еще проблемы с точностью при FFT? Потому что я помню, что у нас по крайней мере говорили,\n",
            "[01:07:34.500 --> 01:07:41.020]  что там прям очень плохо начинается с коэффициентами какими-то прям. Смерть происходит, нет? Да нет,\n",
            "[01:07:41.020 --> 01:07:46.660]  но преобразование фурьи это вещь, достаточно хорошо устоявшийся. Не, если быстрое преобразование\n",
            "[01:07:46.660 --> 01:07:53.380]  фурьи делать? Ну, все равно, если грамотно написанный метод, то там все должно нормально\n",
            "[01:07:53.380 --> 01:07:59.700]  работать. Преобразование фурьи оно хорошо устоялось, у него там хорошие реализации,\n",
            "[01:07:59.700 --> 01:08:06.100]  типа лип FFT, какой-нибудь QFFT, если вы хотите на видеокарте считать. Поэтому тут проблем не\n",
            "[01:08:06.100 --> 01:08:14.620]  должно возникать с точностью. Спасибо. Так, еще вопрос. А можно уточнить, вот еще раз, смотрите,\n",
            "[01:08:14.620 --> 01:08:20.180]  у нас есть, допустим, какой-то конкретный пиксель тигра, и вот как он преобразовывается после\n",
            "[01:08:20.180 --> 01:08:25.540]  преобразования фурьи? Вот конкретно один какой-то пиксель. Вот еще раз, можно, пожалуйста. Один пиксель,\n",
            "[01:08:25.540 --> 01:08:31.820]  ну, один конкретный пиксель никак не преобразовывается, вы смотрите на изображения в целом, и вы рассматриваете\n",
            "[01:08:31.820 --> 01:08:40.140]  не пиксели, а если как-то на пальцах, мы смотрим на какие-то переходы, более плавные, менее плавные,\n",
            "[01:08:40.140 --> 01:08:45.820]  соответственно, мы переходим в другой базис, этот базис в целом влияет на все изображения,\n",
            "[01:08:45.820 --> 01:08:51.500]  поэтому про конкретный пиксель мы ничего тут сказать не можем. Если мы говорим просто про конкретный\n",
            "[01:08:51.500 --> 01:08:53.500]  тогда это уже не переход в другую базис.\n",
            "[01:08:58.500 --> 01:09:01.500]  Надеюсь, ответил на вопрос. Ещё вопрос есть?\n",
            "[01:09:03.500 --> 01:09:07.500]  Если нет вопросов, у нас осталась последняя небольшая часть.\n",
            "[01:09:07.500 --> 01:09:12.500]  Это достаточно любопытное приложение.\n",
            "[01:09:12.500 --> 01:09:15.500]  Называется с клика панорам.\n",
            "[01:09:15.500 --> 01:09:17.500]  Реализовывается очень просто.\n",
            "[01:09:17.500 --> 01:09:19.500]  Мы сейчас разберём реализацию,\n",
            "[01:09:19.500 --> 01:09:25.500]  но при этом интересная вещь, которую можно понять с помощью преобразования Fourier.\n",
            "[01:09:25.500 --> 01:09:31.500]  Мы обсуждали, что Boxfilter его можно лучше понять с помощью преобразования Fourier.\n",
            "[01:09:31.500 --> 01:09:36.500]  При этом нам для свёртки Boxfilter совершенно необязательно использовать преобразование Fourier.\n",
            "[01:09:36.500 --> 01:09:39.500]  Это может быть инструмент анализа.\n",
            "[01:09:39.500 --> 01:09:43.500]  Соответственно, склейка. Для чего нужна?\n",
            "[01:09:43.500 --> 01:09:47.500]  Вот картинка из оригинальной работы достаточно старая.\n",
            "[01:09:47.500 --> 01:09:49.500]  Это 80-е годы прошлого века.\n",
            "[01:09:49.500 --> 01:09:51.500]  У нас есть две каких-то картинки.\n",
            "[01:09:51.500 --> 01:09:54.500]  Условно яблоко и апельсин.\n",
            "[01:09:54.500 --> 01:09:56.500]  И мы хотим склеить.\n",
            "[01:09:56.500 --> 01:09:58.500]  У нас есть какая-то маска.\n",
            "[01:09:58.500 --> 01:10:01.500]  Это пользовательские предпочтения.\n",
            "[01:10:01.500 --> 01:10:07.500]  Откуда нам желательно взять пиксели с одного изображения и с другого изображения.\n",
            "[01:10:07.500 --> 01:10:12.500]  И дальше мы хотим склеить так, чтобы это было не очень заметно.\n",
            "[01:10:12.500 --> 01:10:19.500]  Если мы напрямую используем эту маску бинарно, то у нас получается...\n",
            "[01:10:19.500 --> 01:10:20.500]  Склейка очень заметная.\n",
            "[01:10:20.500 --> 01:10:26.500]  Если мы будем использовать лопластовскую пирамиду, про которую я расскажу, то склейка будет аккуратной.\n",
            "[01:10:26.500 --> 01:10:30.500]  При этом тут изображение немножко разблёривалось, почему-то у авторов.\n",
            "[01:10:30.500 --> 01:10:32.500]  Но на это можно не обращать внимания.\n",
            "[01:10:32.500 --> 01:10:38.500]  На практике вот эти детали снаружи, они все сохраняются и получается очень аккуратный шовчик.\n",
            "[01:10:38.500 --> 01:10:41.500]  Даже не шовчик, это очень аккуратный переход.\n",
            "[01:10:41.500 --> 01:10:43.500]  Зачем это нужно?\n",
            "[01:10:43.500 --> 01:10:46.500]  Например, это нужно для того, чтобы склеивать панорамы.\n",
            "[01:10:46.500 --> 01:10:50.500]  Можете открыть какие-нибудь Яндекс карты.\n",
            "[01:10:50.500 --> 01:10:57.500]  И вот там, когда у нас фотоаппарат, там едет машина,\n",
            "[01:10:57.500 --> 01:11:04.500]  на машине несколько фотоаппаратов, которые снимают кадры, которые пересекаются друг с другом.\n",
            "[01:11:04.500 --> 01:11:11.500]  И вот нам нужно в этих местах пересечения как раз произвести склейку.\n",
            "[01:11:11.500 --> 01:11:18.500]  При этом кадры сначала как-то выравниваются, но как они выравниваются, мы подробно не будем сейчас обсуждать.\n",
            "[01:11:18.500 --> 01:11:24.500]  Считаем, что кадры уже выровнены, у нас задана только вот эта маска, и нам нужно просто провести склейку.\n",
            "[01:11:24.500 --> 01:11:31.500]  Помимо выравнивания, там ещё возникают всякие нюансы, связанные с тем, что машина движется,\n",
            "[01:11:31.500 --> 01:11:35.500]  камеры могут быть не очень хорошо синхронизированы.\n",
            "[01:11:35.500 --> 01:11:41.500]  Одновременно снять всеми фотоаппаратами – это не самая тривейная задача на самом деле.\n",
            "[01:11:41.500 --> 01:11:46.500]  У нас может быть небольшой временной лаг между кадрами двух соседних фотоаппаратов.\n",
            "[01:11:46.500 --> 01:11:49.500]  Такое тоже бывает.\n",
            "[01:11:49.500 --> 01:11:52.500]  Эти вещи мы сейчас не будем рассматривать.\n",
            "[01:11:52.500 --> 01:11:54.500]  Давайте я теперь порисую.\n",
            "[01:11:54.500 --> 01:12:01.500]  У нас есть изображение А, и давайте из него построим сначала Гауссовскую пирамиду.\n",
            "[01:12:01.500 --> 01:12:06.500]  Для этого нам понадобится операция свертки с Гаусяной.\n",
            "[01:12:06.500 --> 01:12:16.500]  Будем строить тут А1, дальше опять с Гаусяной сворачиваем, у нас будет А2 и так далее.\n",
            "[01:12:16.500 --> 01:12:19.500]  У нас будет АН.\n",
            "[01:12:19.500 --> 01:12:24.500]  Это Гауссовская пирамида, стандартная история в представлении изображений.\n",
            "[01:12:24.500 --> 01:12:29.500]  Дальше мы из этого всё построим Лапласовскую пирамиду.\n",
            "[01:12:29.500 --> 01:12:37.500]  Из одного слоя Гауссовской пирамиды будем поэлементно вычитать размытые изображения.\n",
            "[01:12:37.500 --> 01:12:48.500]  Тут у нас будет L1, тут будет L2 и так далее.\n",
            "[01:12:48.500 --> 01:12:54.500]  Здесь у нас будет LAN.\n",
            "[01:12:54.500 --> 01:12:57.500]  Здесь у нас очень простая операция.\n",
            "[01:12:57.500 --> 01:13:01.500]  Мы используем только свертку с Гаусяной и только вычитание.\n",
            "[01:13:01.500 --> 01:13:03.500]  Никакого преобразования фурье.\n",
            "[01:13:03.500 --> 01:13:08.500]  Но преобразование фурье нам понадобится для того, чтобы понять, что здесь произошло.\n",
            "[01:13:08.500 --> 01:13:13.500]  Давайте посмотрим на преобразование фурье от А.\n",
            "[01:13:13.500 --> 01:13:16.500]  Нарисуем частотный домен, то есть амплитуды.\n",
            "[01:13:16.500 --> 01:13:19.500]  Все значения как-то используются.\n",
            "[01:13:19.500 --> 01:13:25.500]  После того, как мы свернули с Гаусяной фильтр низких частот,\n",
            "[01:13:25.500 --> 01:13:30.500]  у нас возникнет такой блок в частотном диапазоне.\n",
            "[01:13:30.500 --> 01:13:37.500]  Он будет уменьшаться по мере того, как мы будем сворачивать.\n",
            "[01:13:37.500 --> 01:13:41.500]  И в конце будут совсем низкие частоты.\n",
            "[01:13:41.500 --> 01:13:43.500]  Сильно размытые изображения.\n",
            "[01:13:43.500 --> 01:13:44.500]  Дальше.\n",
            "[01:13:44.500 --> 01:13:49.500]  Преобразование фурье – это переход в базис.\n",
            "[01:13:49.500 --> 01:13:55.500]  Относительно другого базиса у нас линейные операции сохраняются.\n",
            "[01:13:55.500 --> 01:14:00.500]  Мы можем точно так же здесь делать вычитание по элементной, как оно здесь произошло.\n",
            "[01:14:00.500 --> 01:14:04.500]  Тогда получится, что у нас происходит какая-то такая картина.\n",
            "[01:14:04.500 --> 01:14:07.500]  Мы получаем тут только высокие частоты.\n",
            "[01:14:07.500 --> 01:14:13.500]  Дальше, когда мы вычитаем здесь, мы получаем какое-то колечко.\n",
            "[01:14:13.500 --> 01:14:21.500]  Это обычно называется словом «бенд» по-английски или по-русски «полоса частот».\n",
            "[01:14:21.500 --> 01:14:23.500]  И так далее.\n",
            "[01:14:23.500 --> 01:14:31.500]  Здесь у нас будет тоже какое-то кольцо низкочастотное.\n",
            "[01:14:31.500 --> 01:14:33.500]  Что мы сделали?\n",
            "[01:14:33.500 --> 01:14:38.500]  Вот это изображение от а-н и так далее до l-а-1.\n",
            "[01:14:38.500 --> 01:14:45.500]  Мы получили другое представление изображения в несколько слоев.\n",
            "[01:14:45.500 --> 01:14:52.500]  И эти слои, каждый соответствует своей частоте, своей полосе частот.\n",
            "[01:14:52.500 --> 01:14:57.500]  Такой же вещь мы можем сделать с изображением b.\n",
            "[01:14:57.500 --> 01:15:01.500]  Давайте я это быстренько проделаю, только покороче.\n",
            "[01:15:01.500 --> 01:15:09.500]  Тут будет lb1, lb2 и так далее.\n",
            "[01:15:09.500 --> 01:15:13.500]  Тут у нас будет lbn и bn.\n",
            "[01:15:13.500 --> 01:15:16.500]  То есть тоже построили Лапласовскую пирамиду.\n",
            "[01:15:16.500 --> 01:15:21.500]  Дальше. Как происходит сколейка с помощью маски?\n",
            "[01:15:21.500 --> 01:15:27.500]  Мы разложили на разные полосы частот. У нас осталась маска.\n",
            "[01:15:27.500 --> 01:15:29.500]  И с маской история такая.\n",
            "[01:15:29.500 --> 01:15:32.500]  Вот у нас есть маска, она сейчас бинарная.\n",
            "[01:15:32.500 --> 01:15:35.500]  У нее тоже много высокочастотной информации.\n",
            "[01:15:35.500 --> 01:15:38.500]  И дальше идея сколейки следующая.\n",
            "[01:15:38.500 --> 01:15:44.500]  Нам нужно смешивать разные частоты с разной маской.\n",
            "[01:15:44.500 --> 01:15:47.500]  Например, если у нас в изображении...\n",
            "[01:15:47.500 --> 01:15:49.500]  Давайте какую-то синусоиду нарисую.\n",
            "[01:15:49.500 --> 01:15:56.500]  У нас в изображении один плавный переход, в изображении b другой плавный переход.\n",
            "[01:15:56.500 --> 01:15:59.500]  Я его нарисую в противофазе.\n",
            "[01:15:59.500 --> 01:16:03.500]  Здесь значения меняются очень медленно.\n",
            "[01:16:03.500 --> 01:16:10.500]  Чтобы нам их смешать, нам нужно смешивать по достаточно широкой полосе.\n",
            "[01:16:10.500 --> 01:16:13.500]  Вот по такой.\n",
            "[01:16:13.500 --> 01:16:17.500]  Здесь у нас будут какие-то промежуточные коэффициенты.\n",
            "[01:16:17.500 --> 01:16:20.500]  Здесь мы берем значение из a, здесь значение из b.\n",
            "[01:16:20.500 --> 01:16:24.500]  А тут какую-то линейную комбинацию мы будем делать.\n",
            "[01:16:24.500 --> 01:16:31.500]  Соответственно, если это формализовать, то мы берем нашу маску и строим из нее гауссовскую пирамиду.\n",
            "[01:16:31.500 --> 01:16:35.500]  Выглядеть это будет следующим образом.\n",
            "[01:16:35.500 --> 01:16:38.500]  Сначала...\n",
            "[01:16:41.500 --> 01:16:46.500]  Тут у нас будут исходные значения.\n",
            "[01:16:46.500 --> 01:16:49.500]  Тут какие-то промежуточные серые условия, какой-то градиент.\n",
            "[01:16:49.500 --> 01:16:53.500]  Дальше мы будем сворачивать все сильнее и сильнее.\n",
            "[01:16:53.500 --> 01:17:03.500]  И в конце у нас получится маска для совсем низких частот с широкой полосой, на которой мы будем смешивать.\n",
            "[01:17:03.500 --> 01:17:07.500]  Дальше мы берем лопласовскую пирамиду.\n",
            "[01:17:07.500 --> 01:17:18.500]  Соответственно, эти высокие частоты, то есть lA1 и lB1, мы будем смешивать с высокочастотной маской.\n",
            "[01:17:18.500 --> 01:17:25.500]  Дальше lA2 и lB2 мы будем смешивать уже с этой маской.\n",
            "[01:17:25.500 --> 01:17:34.500]  И aN и bN мы будем смешивать с последним уровнем этой гауссовской пирамиды для маски.\n",
            "[01:17:34.500 --> 01:17:38.500]  При этом, когда мы строим лопласовскую пирамиду,\n",
            "[01:17:38.500 --> 01:17:52.500]  если мы эти значения aN плюс lA1 просуммируем, то мы получим исходное изображение.\n",
            "[01:17:52.500 --> 01:17:58.500]  Так построена пирамида, потому что у нас выкидывается информация, вычитается,\n",
            "[01:17:58.500 --> 01:18:04.500]  и получается, что мы раскладываем на какие-то непересекающиеся более-менее полосы частот.\n",
            "[01:18:04.500 --> 01:18:09.500]  Соответственно, мы после того, как смешали эти лопласовские пирамиды с помощью маски,\n",
            "[01:18:09.500 --> 01:18:14.500]  мы их суммируем и получаем уже смешанное изображение.\n",
            "[01:18:14.500 --> 01:18:18.500]  Как это будет выглядеть, если мы это подытожим?\n",
            "[01:18:18.500 --> 01:18:23.500]  У нас на вход есть изображение a, b и маска M, по которой нужно смешивать.\n",
            "[01:18:23.500 --> 01:18:26.500]  И склейка будет происходить следующим образом.\n",
            "[01:18:26.500 --> 01:18:32.500]  Нам нужно посчитать лопласовские пирамиды lA и lB для изображений a и b.\n",
            "[01:18:32.500 --> 01:18:35.500]  Для этого нам требуется только две операции.\n",
            "[01:18:35.500 --> 01:18:39.500]  Это гауссовская фильтрация и поэлементное вычитание.\n",
            "[01:18:39.500 --> 01:18:43.500]  Обратите внимание, здесь не используется преобразование фурьева вообще никак.\n",
            "[01:18:43.500 --> 01:18:50.500]  Мы его используем только для того, чтобы понять, как это все внутри устроено.\n",
            "[01:18:50.500 --> 01:18:59.500]  Дальше для маски мы считаем гауссовскую пирамиду и начинаем послойно смешивать lSN,\n",
            "[01:18:59.500 --> 01:19:05.500]  это равно gmn lAn, вот здесь коэффициенты проставлю, чтобы было понятно.\n",
            "[01:19:05.500 --> 01:19:12.500]  Для каждого слоя, для каждого N мы бежим, смешиваем по конкретной маске полученной.\n",
            "[01:19:12.500 --> 01:19:19.500]  И дальше можем уже результатившие изображения из лопласовской пирамиды lS получить.\n",
            "[01:19:19.500 --> 01:19:23.500]  Соответственно, на этом мы закончим.\n",
            "[01:19:23.500 --> 01:19:31.500]  Мы рассмотрели следующие топики, посмотрели на стандарт сжатия изображений без потерь png.\n",
            "[01:19:31.500 --> 01:19:38.500]  Он использует очень простое разностное кодирование, дельтакодирование и общий метод сжатия defade,\n",
            "[01:19:38.500 --> 01:19:46.500]  который позволяет нам это и последовательность, описанная с помощью дельтакодирования,\n",
            "[01:19:46.500 --> 01:19:49.500]  уже более эффективно сжимать.\n",
            "[01:19:49.500 --> 01:19:56.500]  Дальше JPEG, это метод сжатия с потерями, но на самом деле, если выкрутить ручку качества,\n",
            "[01:19:56.500 --> 01:20:00.500]  он может быть и без потерь, правда, не суперэффективно это будет.\n",
            "[01:20:00.500 --> 01:20:05.500]  Он базируется на дискретном костюмственном преобразовании, дальше происходит квантование\n",
            "[01:20:05.500 --> 01:20:09.500]  и сжатие амплитуд, опять же, общим методом кодирования.\n",
            "[01:20:09.500 --> 01:20:18.500]  Затем мы посмотрели на преобразование в прие, это удобная вещь для того, чтобы анализировать изображение\n",
            "[01:20:18.500 --> 01:20:21.500]  и делать быструю фильтрацию.\n",
            "[01:20:21.500 --> 01:20:30.500]  И наконец, мы поговорили про интересное приложение, это склейка с помощью Лапасовской пирамида.\n",
            "[01:20:30.500 --> 01:20:34.500]  Дает прикольный эффект и при этом очень легко реализуется.\n",
            "[01:20:34.500 --> 01:20:38.500]  И объяснить, как это работает, мы можем с помощью анализа в Fourier.\n",
            "[01:20:38.500 --> 01:20:43.500]  Соответственно, на этом всё. На моих часах сейчас 25 минут.\n",
            "[01:20:43.500 --> 01:20:49.500]  Давайте сделаем 10-минутный перерыв, кто хочет, может налить себе чайку к семинару.\n",
            "[01:20:49.500 --> 01:20:53.500]  А кто хочет, может ещё мне позадавать в эти 10 минут какие-нибудь вопросы.\n",
            "[01:20:56.500 --> 01:21:01.500]  Можно, пожалуйста, как слайд вернуться, про вот как Лаплас у нас там был, вот это вот?\n",
            "[01:21:01.500 --> 01:21:02.500]  Ага.\n",
            "[01:21:02.500 --> 01:21:08.500]  Смотрите, вот пару вопросов, я понял идею, что мы получили L, A, L, B.\n",
            "[01:21:08.500 --> 01:21:09.500]  Ага.\n",
            "[01:21:09.500 --> 01:21:13.500]  Потом, вот не очень понятно, что мы с ними дальше делаем. Можете повторить этот момент.\n",
            "[01:21:14.500 --> 01:21:22.500]  Ну то есть у нас есть, например, L, A, 1. Мы его домножаем на соответствующую ему маску.\n",
            "[01:21:22.500 --> 01:21:27.500]  Давайте я её обозначу L. Ну тут гауссовская маска, поэтому G.\n",
            "[01:21:27.500 --> 01:21:36.500]  G, M, 1. Плюс 1 минус G, M, 1 умножить на L, B, 1.\n",
            "[01:21:36.500 --> 01:21:41.500]  То есть здесь у нас по-элементное умножение, здесь у нас по-элементное вычитание.\n",
            "[01:21:41.500 --> 01:21:43.500]  А, ещё вот по-элементное сложение.\n",
            "[01:21:44.500 --> 01:21:50.500]  А вот эта маска, она вообще что обозначает, которая M, M1 и так далее? Это маска, которая что делает?\n",
            "[01:21:51.500 --> 01:22:00.500]  Ну смотрите, исходная маска, это как бы такая жёсткая маска, которую задал нам пользователь, что вот как бы одни пиксели нужно брать.\n",
            "[01:22:00.500 --> 01:22:02.500]  А, ну мы типа размываем, да, эту маску немножечко.\n",
            "[01:22:02.500 --> 01:22:08.500]  И из другого изображения. Да, мы её размываем, чтобы у нас вот здесь как-то более плавно смешение было.\n",
            "[01:22:08.500 --> 01:22:16.500]  Но при этом мы размываем как бы специфичным образом для разных полос, для разных полос частот.\n",
            "[01:22:19.500 --> 01:22:24.500]  А вот дальше, дальше можете алгоритм показать? Мы сделали на первой этой рации, потом мы что с этим будем делать?\n",
            "[01:22:24.500 --> 01:22:42.500]  А дальше мы просто вот эти, вот у нас получилось там Ls1, там у нас аналогично получится и так далее Lsn, и мы Ls1 плюс и так далее Lsn.\n",
            "[01:22:42.500 --> 01:22:52.500]  Просто их суммируем. У нас получается как бы итоговое изображение, это будет склейка на всех полосах частот.\n",
            "[01:22:52.500 --> 01:22:56.500]  А, вот это просто не очевидно, что у нас при сложении очень хорошо получится. Ну ладно.\n",
            "[01:22:56.500 --> 01:22:57.500]  Да, да.\n",
            "[01:22:57.500 --> 01:23:01.500]  Это из-за фурья, да? Вы в фурье это объяснили, что такое сложение так хорошо сработает.\n",
            "[01:23:01.500 --> 01:23:05.500]  Просто непонятно, почему мы не потеряем вообще картинки.\n",
            "[01:23:05.500 --> 01:23:17.500]  Ну тут у нас как бы мы ничего точно не потеряем, потому что у нас как бы есть исходное изображение, мы из него какие-то низкие частоты вычитаем.\n",
            "[01:23:17.500 --> 01:23:25.500]  То есть у нас тут высокие частоты в любом случае останутся, дальше идет разложение там на разные полосы частот.\n",
            "[01:23:25.500 --> 01:23:27.500]  То есть так у нас ничего не потеряется.\n",
            "[01:23:27.500 --> 01:23:34.500]  Это размытие с помощью гауссиана и вот такой способ, что мы вычитаем.\n",
            "[01:23:34.500 --> 01:23:40.500]  То есть условно частоты как бы, ну да, видимо они пересекаются друг с другом, мы поэтому можем спокойно их посылать.\n",
            "[01:23:40.500 --> 01:23:42.500]  Это разложение в другой базе как раз, да.\n",
            "[01:23:42.500 --> 01:23:47.500]  Скажите пожалуйста еще, я не понял, откуда берется LA1 самая первая?\n",
            "[01:23:47.500 --> 01:23:58.500]  Мы берем исходное изображение, берем A1, которое чуть-чуть размытое изображение, и из A вычитаем A1, получаем LA1.\n",
            "[01:23:58.500 --> 01:24:05.500]  А, и все следующие тоже так же получаем?\n",
            "[01:24:05.500 --> 01:24:13.500]  Да, просто по очереди там соседние масштабы вот этой гауссовской пирамиды мы их вычитаем.\n",
            "[01:24:13.500 --> 01:24:15.500]  Спасибо.\n",
            "[01:24:25.500 --> 01:24:29.500]  Так есть ли еще какие-то вопросы?\n",
            "[01:24:36.500 --> 01:24:40.500]  Окей, видимо вопросов нет.\n",
            "[01:24:40.500 --> 01:24:42.500]  Тогда на этом давайте закончим.\n",
            "[01:24:42.500 --> 01:24:44.500]  Спасибо за внимание.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Transcript file created: /content/drive/My Drive/Colab Notebooks/Whisper Youtube/BOpyP_B3_wI.txt**"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown # **Run the model** 🚀\n",
        "\n",
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** ⚙️\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown ---\n",
        "language = \"Russian\" #@param ['Auto detection', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba']\n",
        "#@markdown > Language spoken in the audio, use `Auto detection` to let Whisper detect the language.\n",
        "#@markdown ---\n",
        "verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n",
        "#@markdown > Whether to print out the progress and debug messages.\n",
        "#@markdown ---\n",
        "output_format = 'txt' #@param ['txt', 'vtt', 'srt', 'tsv', 'json', 'all']\n",
        "#@markdown > Type of file to generate to record the transcription.\n",
        "#@markdown ---\n",
        "task = 'transcribe' #@param ['transcribe', 'translate']\n",
        "#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown <br/>\n",
        "\n",
        "#@markdown ### **Optional: Fine tunning**\n",
        "#@markdown ---\n",
        "temperature = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to use for sampling.\n",
        "#@markdown ---\n",
        "temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n",
        "#@markdown ---\n",
        "best_of = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of candidates when sampling with non-zero temperature.\n",
        "#@markdown ---\n",
        "beam_size = 8 #@param {type:\"integer\"}\n",
        "#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n",
        "#@markdown ---\n",
        "patience = 1.0 #@param {type:\"number\"}\n",
        "#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n",
        "#@markdown ---\n",
        "length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n",
        "#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n",
        "#@markdown ---\n",
        "suppress_tokens = \"-1\" #@param {type:\"string\"}\n",
        "#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n",
        "#@markdown ---\n",
        "initial_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Optional text to provide as a prompt for the first window.\n",
        "#@markdown ---\n",
        "condition_on_previous_text = True #@param {type:\"boolean\"}\n",
        "#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n",
        "#@markdown ---\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "#@markdown > whether to perform inference in fp16.\n",
        "#@markdown ---\n",
        "compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n",
        "#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "logprob_threshold = -1.0 #@param {type:\"number\"}\n",
        "#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n",
        "#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n",
        "#@markdown ---\n",
        "\n",
        "verbose_lut = {\n",
        "    'Live transcription': True,\n",
        "    'Progress bar': False,\n",
        "    'None': None\n",
        "}\n",
        "\n",
        "args = dict(\n",
        "    language = (None if language == \"Auto detection\" else language),\n",
        "    verbose = verbose_lut[verbose],\n",
        "    task = task,\n",
        "    temperature = temperature,\n",
        "    temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "    best_of = best_of,\n",
        "    beam_size = beam_size,\n",
        "    patience=patience,\n",
        "    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "    suppress_tokens=suppress_tokens,\n",
        "    initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "    condition_on_previous_text=condition_on_previous_text,\n",
        "    fp16=fp16,\n",
        "    compression_ratio_threshold=compression_ratio_threshold,\n",
        "    logprob_threshold=logprob_threshold,\n",
        "    no_speech_threshold=no_speech_threshold\n",
        ")\n",
        "\n",
        "temperature = args.pop(\"temperature\")\n",
        "temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "if temperature_increment_on_fallback is not None:\n",
        "    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "else:\n",
        "    temperature = [temperature]\n",
        "\n",
        "if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n",
        "    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "    args[\"language\"] = \"en\"\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    display(Markdown(f\"### {video_path_local}\"))\n",
        "\n",
        "    video_transcription = whisper.transcribe(\n",
        "        whisper_model,\n",
        "        str(video_path_local),\n",
        "        temperature=temperature,\n",
        "        **args,\n",
        "    )\n",
        "\n",
        "    # Save output\n",
        "    whisper.utils.get_writer(\n",
        "        output_format=output_format,\n",
        "        output_dir=video_path_local.parent\n",
        "    )(\n",
        "        video_transcription,\n",
        "        str(video_path_local.stem),\n",
        "        options=dict(\n",
        "            highlight_words=False,\n",
        "            max_line_count=None,\n",
        "            max_line_width=None,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    def exportTranscriptFile(ext: str):\n",
        "        local_path = video_path_local.parent / video_path_local.with_suffix(ext).name\n",
        "        export_path = drive_whisper_path / video_path_local.with_suffix(ext).name\n",
        "        shutil.copy(\n",
        "            local_path,\n",
        "            export_path\n",
        "        )\n",
        "        display(Markdown(f\"**Transcript file created: {export_path}**\"))\n",
        "\n",
        "    if output_format==\"all\":\n",
        "        for ext in ('.txt', '.vtt', '.srt', '.tsv', '.json'):\n",
        "            exportTranscriptFile(ext)\n",
        "    else:\n",
        "        exportTranscriptFile(\".\" + output_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ad6n1m4deAHp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}