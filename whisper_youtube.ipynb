{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Youtube Videos Transcription with OpenAI's Whisper**\n",
        "\n",
        "[![blog post shield](https://img.shields.io/static/v1?label=&message=Blog%20post&color=blue&style=for-the-badge&logo=openai&link=https://openai.com/blog/whisper)](https://openai.com/blog/whisper)\n",
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/openai/whisper)\n",
        "[![paper shield](https://img.shields.io/static/v1?label=&message=Paper&color=blue&style=for-the-badge&link=https://cdn.openai.com/papers/whisper.pdf)](https://cdn.openai.com/papers/whisper.pdf)\n",
        "[![model card shield](https://img.shields.io/static/v1?label=&message=Model%20card&color=blue&style=for-the-badge&link=https://github.com/openai/whisper/blob/main/model-card.md)](https://github.com/openai/whisper/blob/main/model-card.md)\n",
        "\n",
        "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
        "\n",
        "This Notebook will guide you through the transcription of a Youtube video using Whisper. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript and video audio in your Google Drive."
      ],
      "metadata": {
        "id": "96kvih9mXkNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Check GPU type** 🕵️\n",
        "\n",
        "#@markdown The type of GPU you get assigned in your Colab session defined the speed at which the video will be transcribed.\n",
        "#@markdown The higher the number of floating point operations per second (FLOPS), the faster the transcription.\n",
        "#@markdown But even the least powerful GPU available in Colab is able to run any Whisper model.\n",
        "#@markdown Make sure you've selected `GPU` as hardware accelerator for the Notebook (Runtime &rarr; Change runtime type &rarr; Hardware accelerator).\n",
        "\n",
        "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
        "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
        "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
        "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
        "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Factory reset your Notebook's runtime if you want to get assigned a new GPU.**\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "QshUbLqpX7L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IfG0E_WbRFI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cddb233f-1332-4add-d3e9-135bbff75a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-17bvtbu_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-17bvtbu_\n",
            "  Resolved https://github.com/openai/whisper.git to commit 90db0de1896c23cbfaf0c58bc2d30665f709f170\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper==20240930)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803583 sha256=1e31579dcfbd44f04af76e50e0a1cd9847f928d54815ea7a7b0dfaabc65c8c17\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ew1ld8mv/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.12.23-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2024.12.23-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2024.12.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Install libraries** 🏗️\n",
        "#@markdown This cell will take a little while to download several libraries, including Whisper.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install yt-dlp\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1zwGAsr4sIgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daab185d-9a06-4a73-a6f5-842cfd8a4ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Optional:** Save data in Google Drive 💾\n",
        "#@markdown Enter a Google Drive path and run this cell if you want to store the results inside Google Drive.\n",
        "\n",
        "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
        "from google.colab import drive\n",
        "drive_mount_path = Path(\"/\") / \"content\" / \"drive\"\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"My Drive\"\n",
        "#@markdown ---\n",
        "drive_path = \"Colab Notebooks/Whisper Youtube\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change your Google Drive path.**\n",
        "\n",
        "drive_whisper_path = drive_mount_path / Path(drive_path.lstrip(\"/\"))\n",
        "drive_whisper_path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Model selection** 🧠\n",
        "\n",
        "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
        "\n",
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "\n",
        "#@markdown ---\n",
        "Model = 'medium' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if Model in whisper.available_models():\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is selected.**\"\n",
        "    ))\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is no longer available.**<br /> Please select one of the following:<br /> - {'<br /> - '.join(whisper.available_models())}\"\n",
        "    ))"
      ],
      "metadata": {
        "id": "TMhrSq_GZ6kA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "255c9192-8d28-484e-9e76-33743600e793"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:34<00:00, 44.4MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**medium model is selected.**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Video selection** 📺\n",
        "\n",
        "#@markdown Enter the URL of the Youtube video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n",
        "\n",
        "Type = \"Youtube video or playlist\" #@param ['Youtube video or playlist', 'Google Drive']\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video or playlist**\n",
        "URL = \"https://youtu.be/IEz-lvAqC_A?si=u6OTibo12voHb_0o\" #@param {type:\"string\"}\n",
        "# store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive video, audio (mp4, wav), or folder containing video and/or audio files**\n",
        "video_path = \"Colab Notebooks/transcription/my_video.mp4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**\n",
        "\n",
        "video_path_local_list = []\n",
        "\n",
        "if Type == \"Youtube video or playlist\":\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'm4a/bestaudio/best',\n",
        "        'outtmpl': '%(id)s.%(ext)s',\n",
        "        # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "        'postprocessors': [{  # Extract audio using ffmpeg\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'wav',\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        error_code = ydl.download([URL])\n",
        "        list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "    for video_info in list_video_info:\n",
        "        video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))\n",
        "\n",
        "elif Type == \"Google Drive\":\n",
        "    # video_path_drive = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    video_path = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    if video_path.is_dir():\n",
        "        for video_path_drive in video_path.glob(\"**/*\"):\n",
        "            if video_path_drive.is_file():\n",
        "                display(Markdown(f\"**{str(video_path_drive)} selected for transcription.**\"))\n",
        "            elif video_path_drive.is_dir():\n",
        "                display(Markdown(f\"**Subfolders not supported.**\"))\n",
        "            else:\n",
        "                display(Markdown(f\"**{str(video_path_drive)} does not exist, skipping.**\"))\n",
        "            video_path_local = Path(\".\").resolve() / (video_path_drive.name)\n",
        "            shutil.copy(video_path_drive, video_path_local)\n",
        "            video_path_local_list.append(video_path_local)\n",
        "    elif video_path.is_file():\n",
        "        video_path_local = Path(\".\").resolve() / (video_path.name)\n",
        "        shutil.copy(video_path, video_path_local)\n",
        "        video_path_local_list.append(video_path_local)\n",
        "        display(Markdown(f\"**{str(video_path)} selected for transcription.**\"))\n",
        "    else:\n",
        "        display(Markdown(f\"**{str(video_path)} does not exist.**\"))\n",
        "\n",
        "else:\n",
        "    raise(TypeError(\"Please select supported input type.\"))\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    if video_path_local.suffix == \".mp4\":\n",
        "        video_path_local = video_path_local.with_suffix(\".wav\")\n",
        "        result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n"
      ],
      "metadata": {
        "id": "xYLPZQX9S7tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5698cc76-de75-419c-b6cb-fbd5f48b9e0f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/IEz-lvAqC_A?si=u6OTibo12voHb_0o\n",
            "[youtube] IEz-lvAqC_A: Downloading webpage\n",
            "[youtube] IEz-lvAqC_A: Downloading ios player API JSON\n",
            "[youtube] IEz-lvAqC_A: Downloading mweb player API JSON\n",
            "[youtube] IEz-lvAqC_A: Downloading m3u8 information\n",
            "[info] IEz-lvAqC_A: Downloading 1 format(s): 140\n",
            "[download] Destination: IEz-lvAqC_A.m4a\n",
            "[download] 100% of   72.29MiB in 00:00:13 at 5.23MiB/s   \n",
            "[FixupM4a] Correcting container of \"IEz-lvAqC_A.m4a\"\n",
            "[ExtractAudio] Destination: IEz-lvAqC_A.wav\n",
            "Deleting original file IEz-lvAqC_A.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://youtu.be/IEz-lvAqC_A?si=u6OTibo12voHb_0o\n",
            "[youtube] IEz-lvAqC_A: Downloading webpage\n",
            "[youtube] IEz-lvAqC_A: Downloading ios player API JSON\n",
            "[youtube] IEz-lvAqC_A: Downloading mweb player API JSON\n",
            "[youtube] IEz-lvAqC_A: Downloading m3u8 information\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-X0qB9JAzMLY",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78aaea14-91c3-4f4c-d973-90fe713f756f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### IEz-lvAqC_A.wav"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:15.480]  Давайте начнем. Всем добрый вечер. У нас сегодня лекция про классификацию изображений и мягкое\n",
            "[00:15.480 --> 00:23.200]  введение в нейронные сети. Соответственно, это у нас первая лекция уже по нейросетям. То есть мы\n",
            "[00:23.200 --> 00:30.160]  заканчиваем введение в обработку изображений, в жатии изображений и постепенно переходим уже\n",
            "[00:30.160 --> 00:38.120]  к разным задачкам компьютерного зрения. И сегодня у нас такая корзадача. Это задача классификации.\n",
            "[00:38.120 --> 00:44.800]  Лекция у нас будет состоять из нескольких частей. В первой части мы поговорим про задачу\n",
            "[00:44.800 --> 00:50.760]  классификации, про датасеты. Приведем некоторые примеры датасетов, поговорим про то,\n",
            "[00:50.800 --> 01:00.680]  как они формируются. Затем перейдем к нейрону, который является базовой единицей в нейросетях,\n",
            "[01:00.680 --> 01:07.440]  поговорим про оперсептроны. Затем перейдем к сверточным нейронным сетям. Поговорим про то,\n",
            "[01:07.440 --> 01:15.760]  как они исторически появились, какие идеи в них заложены и как они были вдохновлены тем,\n",
            "[01:15.760 --> 01:24.160]  как исследовали человеческое зрение. И наконец закончим это все этапным результатом AlexNet,\n",
            "[01:24.160 --> 01:34.000]  от которого можно начинать современную эпоху компьютерного зрения, то, что называется\n",
            "[01:34.000 --> 01:42.940]  обычный deep learning era. Лекция у нас будет достаточно вводная, поскольку у нас аудитории\n",
            "[01:42.940 --> 01:51.340]  очень разные, не только shat, но и FTI, WMK, у всех может быть достаточно разный background,\n",
            "[01:51.340 --> 01:58.380]  кто-то больше слушает ML, кто-то меньше слушает ML, поэтому я буду достаточно подробно разбирать\n",
            "[01:58.380 --> 02:04.220]  базовые вопросы, но в целом даже те, кто слушали какие-то другие курсы deep learning, я думаю,\n",
            "[02:04.220 --> 02:10.740]  что какие-то новые вещи здесь подчеркнут, в том числе исторические, потому что, как правило,\n",
            "[02:10.740 --> 02:19.540]  в курсах по deep learning и в курсах по ML все нюансы архитектуры рассматриваются достаточно\n",
            "[02:19.540 --> 02:27.100]  по-верхам, очень быстро, и не всегда понятно, какая мотивация была у авторов, как это все появлялось,\n",
            "[02:27.100 --> 02:32.620]  какие ключевые идеи были. А вот эти самые ключевые идеи, которые там были 10 лет назад,\n",
            "[02:32.620 --> 02:39.460]  они сейчас часто переиспользуются, и в целом, если смотреть на современную работу по компьютерному\n",
            "[02:39.460 --> 02:46.020]  зрению, то все идет немножко по спирали, то есть с одной стороны кажется, что что-то новое происходит,\n",
            "[02:46.020 --> 02:50.980]  с другой стороны видно, что старые проверенные идеи переиспользуются в каком-то новом качестве.\n",
            "[02:50.980 --> 02:57.700]  Соответственно, давайте уже перейдем к коллекции после такого небольшого лирического вступления.\n",
            "[02:57.700 --> 03:05.380]  Задача классификации. Бывают в разных вариантах. В первую очередь это бинарная классификация,\n",
            "[03:05.380 --> 03:14.580]  у нас есть какая-то картинка, если формально это тензор из веществных чисел с каким-то разрешением,\n",
            "[03:14.580 --> 03:23.180]  с каким-то количеством цветовых каналов, с, как правило, это три цветовых канала. Задача\n",
            "[03:23.180 --> 03:30.300]  бинарной классификации. Это задача построения модели, которая на вход принимает эту картинку,\n",
            "[03:30.300 --> 03:38.220]  как правило, фиксированного размера, а дальше отвечает на вопрос, например, содержат ли данные\n",
            "[03:38.220 --> 03:46.420]  изображения пешехода, да или нет. И в результате модель должна выдать одно число, метку класса,\n",
            "[03:46.420 --> 03:57.420]  0,1, 1 если ответ положительный, 0 если отрицательный. Бывает иногда минус 1,1, но это все для удобства\n",
            "[03:57.420 --> 04:07.260]  обозначения обычно используется. Другой вариант. Мы можем оценивать, выдавать не бинарный\n",
            "[04:07.260 --> 04:15.140]  метку класса, а вероятность правильного ответа, вероятность позитивного ответа. То есть мы выдаем\n",
            "[04:15.140 --> 04:23.020]  какое-то число в промежутке от 0 до 1 вещественное. И дальше мы выбираем какой-то порог, например,\n",
            "[04:23.020 --> 04:35.740]  если у нас PS больше 0,5, тогда мы говорим, что класс у нас единичка, а иначе говорим, что 0.\n",
            "[04:35.740 --> 04:44.980]  Это первый простой вариант классификации. Следующий вариант это многоклассовая классификация.\n",
            "[04:44.980 --> 04:54.580]  Опять же у нас есть картинка и у нас есть зафиксированный заранее набор классов, мета класса,\n",
            "[04:54.580 --> 05:01.100]  которые мы можем присваивать данному изображению. И мы должны из этого набора классов выбрать один\n",
            "[05:01.100 --> 05:08.580]  класс, который будет описывать данное изображение. То есть здесь мы уже говорим, да нет, а какому классу\n",
            "[05:08.580 --> 05:17.060]  принадлежит объект на данном изображении. И в зависимости от формализации мы можем кодировать\n",
            "[05:17.060 --> 05:24.780]  либо целыми числами от 1 до s, и каждому числу соответствует какой-то фиксированный заранее\n",
            "[05:24.780 --> 05:33.940]  выбранный класс, или мы можем выдавать вектор вероятностей длины s, каждая вероятность от 0 до 1,\n",
            "[05:33.940 --> 05:45.020]  и элементы вектора суммируются в единицу. Следующий вариант это распознавание аттрибутов\n",
            "[05:45.020 --> 05:52.420]  объекта. То есть у нас есть какое-то изображение, например, изображение человека. У нас есть некоторые\n",
            "[05:52.420 --> 05:59.940]  зафиксированные свойства объекта, которые обязательно есть на данном изображении. Для человека это может\n",
            "[05:59.940 --> 06:12.780]  быть раса, пол, возраст, цвет волос и так далее. Соответственно нам нужно определить какому\n",
            "[06:12.780 --> 06:22.740]  провести классификацию, для каждого из таких аттрибутов обозначить какому классу принадлежит\n",
            "[06:22.740 --> 06:27.780]  аттрибут. Это может быть либо многоклассовая классификация, либо бинарная классификация.\n",
            "[06:27.780 --> 06:36.060]  Например, в задачах видеоаналитики часто встречаются классификации, если на человеке маска или какая-нибудь\n",
            "[06:36.060 --> 06:46.820]  каска, например. В данном случае написано шляпа. Если мы хотим находить каски на людях, на заводах\n",
            "[06:46.820 --> 06:54.340]  часто возникает такая задачка, то тогда это будет бинарная классификация. То есть по сути мы решаем\n",
            "[06:54.340 --> 06:59.780]  одну или несколько задач классификации. Если пол, то у нас бинарная классификация, раса\n",
            "[06:59.780 --> 07:07.300]  многоклассовая. Если возраст, то тут есть варианты. Мы можем решать задачу либо регрессии, то есть пытаться\n",
            "[07:07.300 --> 07:17.060]  определить какое-то число, условно от 0 до 120. Но как правило, задачу определения возраста сводят к\n",
            "[07:17.060 --> 07:26.500]  многоклассовой классификации, то есть разбивают возможные диапазоны возрастов на какие-то поддиапазоны\n",
            "[07:26.500 --> 07:36.500]  с шагом, например, 5 лет от 0 до 5, от 5 до 10 и так далее. И проводят многоклассовую классификацию, то\n",
            "[07:36.500 --> 07:46.260]  есть пытаются определить в каком диапазоне находится возраст человека. Какие метрики мы можем считать\n",
            "[07:46.260 --> 07:54.020]  для задачи классификации? Первая, наиболее часто используемая метрика, это accuracy, по-русски\n",
            "[07:54.020 --> 08:02.180]  точность. Это процент корректно классифицированных сэмплов, которые у нас есть в тестовой выборке.\n",
            "[08:02.180 --> 08:13.700]  Если у нас бинарная классификация, тогда у нас должна метка класса совпадать с разметкой, где мы\n",
            "[08:13.700 --> 08:21.700]  предсказали 0, там должен быть и на самом деле 0. И точно так же с единичкой. Если у нас многоклассовая\n",
            "[08:21.700 --> 08:31.220]  классификация, тогда у нас тоже должно быть точное совпадение. То есть если у нас 10 классов,\n",
            "[08:31.220 --> 08:36.980]  то и, например, объект пятого класса, то тогда он считается верно классифицирован только,\n",
            "[08:36.980 --> 08:44.060]  если мы предсказали, что он действительно пятого класса. Если у нас классов много,\n",
            "[08:44.060 --> 08:53.500]  тогда можно задачу немножко упростить и считать не такую строгую метрику точности, а считать\n",
            "[08:53.500 --> 09:04.500]  метрику точность топка. Это процент сэмплов, для которых верное предсказание находится в\n",
            "[09:04.500 --> 09:11.900]  К наиболее вероятных предсказанных классов. То есть мы предсказываем нас, например, тысячи классов,\n",
            "[09:11.900 --> 09:19.380]  мы предсказываем вектор вероятности. Из этого вектора вероятности берем 5 классов наиболее\n",
            "[09:19.380 --> 09:28.300]  вероятных и смотрим, попал ли истинный класс данного объекта в тот набор из пяти классов,\n",
            "[09:28.300 --> 09:39.180]  который мы предсказали. Теперь давайте поговорим про данные. И тут, когда мы переходим к данным,\n",
            "[09:39.180 --> 09:48.900]  имеет смысл сделать некоторое лирическое отступление, что, вообще говоря, картинки можно\n",
            "[09:48.900 --> 09:56.900]  описать пространством бещественных чисел высокой размерности h на В наце. При этом подавляющее\n",
            "[09:56.900 --> 10:06.740]  большинство сэмплов, которые находятся в этом пространстве, являются шумом, всевозможными\n",
            "[10:06.740 --> 10:15.740]  рандомными изображениями, которые нас, вообще говоря, не интересуют. И вот выбор изображений,\n",
            "[10:15.740 --> 10:24.900]  с которыми мы вообще можем работать, какие-то естественные изображения или, может быть, те\n",
            "[10:24.980 --> 10:33.500]  изображения, которые мы можем встретить в реальной жизни, намного меньше. А под выбор изображений,\n",
            "[10:33.500 --> 10:41.220]  с которыми мы хотим работать в данные задачи, она может быть еще уже. Какие могут быть задачи?\n",
            "[10:41.220 --> 10:48.140]  Например, задача анализа рентгеновских снимков. Как правило, это черно-белые изображения, они могут\n",
            "[10:48.140 --> 10:56.580]  быть от 0 до 255, или там бывают вариации с диапазонами. И мы можем сказать, что на этом изображении\n",
            "[10:56.580 --> 11:04.060]  должна быть, должна находиться грудная клетка. При этом формализовать, что значит находится\n",
            "[11:04.060 --> 11:11.900]  грудная клетка, мы не можем. Мы можем просто привести набор картинок, которые характеризуют данный\n",
            "[11:11.900 --> 11:19.260]  домен. Точно также можно говорить про какие-нибудь задачи видеоаналитики, когда у нас какой-то\n",
            "[11:19.260 --> 11:25.660]  видеопоток, мы из этого видеопотока вырезаем какие-то картинки. Эти картинки сильно зависит от сцены,\n",
            "[11:25.660 --> 11:30.980]  которую мы наблюдаем, то есть у нас здесь всегда серый асфальт, и сильно зависит от камеры,\n",
            "[11:30.980 --> 11:36.820]  с которой снимается эта сцена. То есть у этой камеры будут конкретные артефакты, которые мы\n",
            "[11:36.820 --> 11:43.140]  регулярно сможем наблюдать. И точно также это может касаться как спутниковых снимков,\n",
            "[11:43.140 --> 11:52.100]  как изображений людей. Вообще изображения людей очень низкоразмерные, их очень эффективно можно\n",
            "[11:52.100 --> 11:59.940]  сжимать. И есть такое понятие «внутренняя размерность». Формально мы сейчас не будем давать его описание,\n",
            "[11:59.940 --> 12:07.740]  но вот можно представить, что изображение людей формирует подпространство, в этом пространстве R,\n",
            "[12:07.740 --> 12:15.740]  H на V на C, намного меньше размерности. Ну и также это касается OCR и других медицинских изображений.\n",
            "[12:15.740 --> 12:25.220]  И в целом алгоритмы компьютерного зрения, как правило, затачиваются под конкретный домен\n",
            "[12:25.220 --> 12:35.500]  данных. И этот домен данных, как я сказал, обычно характеризуется какими-то выборками. При этом\n",
            "[12:35.500 --> 12:42.340]  алгоритм обучается так, чтобы из этой выборки извлекать какие-то закономерности, которые для\n",
            "[12:42.340 --> 12:50.220]  нее свойственны, какие-то инварианты. И таким образом получаются эффективные алгоритмы,\n",
            "[12:50.220 --> 12:58.140]  которые можно достаточно быстро считать и которые будут показывать высокую точность,\n",
            "[12:58.140 --> 13:04.140]  например, если мы говорим про задачу классификации. При этом надо заметить, что сейчас есть тренд\n",
            "[13:04.140 --> 13:13.380]  активный, там уже несколько лет идет, на построение фундаментальных моделей. И фундаментальные модели в\n",
            "[13:13.380 --> 13:22.700]  их определении обычно определяются так, что это модели, которые работают со всеми возможными картинками,\n",
            "[13:22.700 --> 13:29.580]  которые только бывают. Вот эта общая выборка и они более-менее со всеми картинками должны\n",
            "[13:29.580 --> 13:38.660]  достаточно хорошо работать. Но тут надо понимать, что это приходит не бесплатно. Мы либо жертвуем\n",
            "[13:38.660 --> 13:45.860]  точностью, то есть никакая фундаментальная модель не сможет побить какой-то узкоспециализированный\n",
            "[13:45.860 --> 13:51.540]  метод видеоаналитики по точности. При этом в видеоаналитике, поскольку там обработка\n",
            "[13:51.540 --> 13:57.780]  видеопотока, очень много кадров, там точность требуется 99 и несколько девятых после запятой,\n",
            "[13:57.780 --> 14:06.420]  чтобы у нас не было слишком много ошибок в этом видеопотоке. И помимо точности фундаментальной\n",
            "[14:06.420 --> 14:17.180]  модели, как правило, будут на порядке медленнее. То есть с научной точки зрения фундаментальные\n",
            "[14:17.180 --> 14:23.260]  модели это сейчас самое активно растущее направление уже на стыке компьютерного зрения,\n",
            "[14:23.260 --> 14:30.620]  обработки языка и даже других дисциплин. Но если мы говорим про практику, то вот специализированные\n",
            "[14:30.620 --> 14:39.140]  алгоритмы под какие-то конкретные задачи, они будут более эффективны. Поэтому тут надо различать,\n",
            "[14:39.140 --> 14:46.700]  чем вы занимаетесь. Если вы идете в какой-то продакшен, то многие вещи хайповые, про которые\n",
            "[14:46.700 --> 14:54.300]  вы сейчас можете читать, тот же самый GPT Vision, они могут быть либо неприменимы на практике,\n",
            "[14:54.300 --> 15:01.180]  либо слишком дороги. Теперь перейдем, собственно, к данным. Откуда берутся данные? Данные, как правило,\n",
            "[15:01.180 --> 15:10.620]  берутся из интернета. Это такой самый популярный источник. И для того, чтобы нам собрать датасет,\n",
            "[15:10.620 --> 15:16.820]  нам требуются картинки из интернета, и нам требуются эти картинки с учебной классификацией\n",
            "[15:16.820 --> 15:26.220]  размечать на какие-то классы, то есть присваивать им какие-то метки. Один из первых датасетов,\n",
            "[15:26.220 --> 15:34.940]  которые собрали таким образом, называется Tiny Images. Так он называется, потому что собирался он\n",
            "[15:34.940 --> 15:41.940]  в начале 2000-х годов, и поскольку жесткие диски тогда были не такие большие, как сейчас,\n",
            "[15:41.940 --> 15:50.020]  достаточно дорогие, авторы задались целью собрать достаточно большое количество изображений,\n",
            "[15:50.020 --> 15:58.020]  но при этом, чтобы они поместились на диск, они уменьшили их до разрешения 32 на 32. Вообще,\n",
            "[15:58.020 --> 16:03.300]  если почитать оригинальную статью, то там проводились два варианта экспериментов,\n",
            "[16:03.300 --> 16:17.460]  32 на 32 RGB либо 64 на 64 Grayscale. Эксперименты с людьми показали, что эти два разрешения,\n",
            "[16:17.460 --> 16:25.020]  это более-менее минимальные разрешения, до которых человек может достаточно хорошо распознавать,\n",
            "[16:25.020 --> 16:32.580]  что на изображении какой объект находится. И взяли в результате 32 на 32 RGB, потому что на\n",
            "[16:32.580 --> 16:43.820]  четверть меньше места занимают. Дальше этот датасет Tiny Images дал начало двум известным датасетам,\n",
            "[16:43.820 --> 16:56.940]  цифр 10 и цифр 100. Подготовил их магистр канадского университета Торонто, Алекс Крижевский.\n",
            "[16:56.940 --> 17:05.460]  Он сделал под выборки из этих изображений по 60 тысяч изображений и получилось два варианта датасетов,\n",
            "[17:05.460 --> 17:13.380]  10 и 100 классов. Цифр эти датасеты называется по названию канадского исследовательского института,\n",
            "[17:13.380 --> 17:22.220]  которому он принадлежал. Дальше выборки были разделены на тренировочную и тестовую. То есть на\n",
            "[17:22.220 --> 17:30.100]  тренировочную мы обучаем нашу модель, на тестовую оцениваем качество. И вот здесь слева показаны\n",
            "[17:30.100 --> 17:40.900]  примеры картинок из цифр 10. Тут какие-то объекты общего вида. Кошки, собаки, автомобили, птички,\n",
            "[17:40.900 --> 17:51.260]  всевозможные. То есть это и страусы, и курицы, и другие птицы. Следующий датасет, который сыграл\n",
            "[17:51.260 --> 17:58.700]  ключевую роль в развитии компьютерного зрения называется ImageNet. Его начали составлять чуть\n",
            "[17:58.700 --> 18:07.780]  позже Tiny Images. Там уже разрешение побольше. И цель изначально была создать датасет с тысячи\n",
            "[18:07.780 --> 18:19.820]  изображений для каждого из 117 тысяч классов. Для классов использовалась специальная лексическая\n",
            "[18:19.820 --> 18:29.020]  иерархия, которая называется WordNet, и предполагалось собрать такой гигантский датасет. Такой датасет в\n",
            "[18:29.020 --> 18:35.860]  результате не собрали, как это часто бывает. То есть поставили слишком высокую планку, но все равно,\n",
            "[18:35.860 --> 18:42.340]  поскольку планка была достаточно хорошая, собрали очень хороший датасет. В датасете всего\n",
            "[18:42.340 --> 18:52.940]  получилось 14 миллионов изображений и почти 22 тысячи классов. Дальше датасет ImageNet использовался\n",
            "[18:52.940 --> 19:05.380]  для соревнования ImageNet Large Scale Visual Recognition Challenge. Там использовалось порядка одного миллиона\n",
            "[19:05.380 --> 19:17.580]  изображений с тысячи классов. Этот датасет ImageNet 1 миллион считается таким золотым стандартом,\n",
            "[19:17.580 --> 19:27.880]  но при этом часто, обычно он называется ImageNet 1K по количеству классов. Но часто в статьях можно\n",
            "[19:27.880 --> 19:39.260]  встретить и упоминания датасета 21K либо 22K по количеству классов. Это расширенный датасет,\n",
            "[19:39.260 --> 19:45.280]  то есть вот этот датасет с одним миллионом изображений достаточно качественный. Датасет\n",
            "[19:45.280 --> 19:53.480]  с 14 миллионами изображений менее качественный, но сейчас настолько большие модели, что для их\n",
            "[19:53.480 --> 20:02.440]  обучения необходим вот такой датасет. Помимо разметки классов, тысяча изображений еще была\n",
            "[20:02.440 --> 20:14.720]  размечена не только классом, но и ограничивающим прямым уголиком. И если посмотреть на разметку\n",
            "[20:14.720 --> 20:25.280]  ImageNet и результаты одного из методов классификации, то видно, что несмотря на то, что авторы потратили\n",
            "[20:25.280 --> 20:30.160]  много времени на качественную разметку, все равно в датасете присутствуют ошибки.\n",
            "[20:30.160 --> 20:42.280]  То есть здесь у нас, например, правильный ответ – гриль, а метод предсказывает, что это\n",
            "[20:42.280 --> 20:51.440]  кабриолет. И дальше, если у нас здесь, например, изображен долматинец, то в принципе тут правильным\n",
            "[20:51.440 --> 20:59.280]  ответом может быть и вишня. Тут видно как раз предсказан долматинец, вишня тут вообще нету,\n",
            "[20:59.280 --> 21:05.440]  а правильный ответ как раз вишня, тут просто розовенький не подсвечен. То есть в целом видно,\n",
            "[21:05.440 --> 21:14.360]  что в датасете все равно присутствуют ошибки разметки. Это как бы первый важный нюанс.\n",
            "[21:14.360 --> 21:25.320]  Следующий важный нюанс, который начал вскрываться году примерно в 2018, это лицензия картинок и privacy.\n",
            "[21:25.320 --> 21:35.400]  ImageNet собирался путем скрапинга Flickr, если мне не изменяет память. При этом авторы не сильно\n",
            "[21:35.400 --> 21:40.440]  заморачивались с лицензиями. Вообще, когда мы говорим про картинки, есть два важных нюанса.\n",
            "[21:40.440 --> 21:48.360]  Первый – это лицензия. То есть вы делаете фотографию, вы как фотограф обладаете правами на эту картинку,\n",
            "[21:48.360 --> 21:55.000]  и вы можете какие-то действия с этим изображением либо разрешать, либо не разрешать. Обычно это\n",
            "[21:55.000 --> 22:02.720]  регулируется лицензиями, их несчетное количество есть, как на данные, так и на код. Это достаточно\n",
            "[22:02.720 --> 22:12.480]  похожая история. В ImageNet есть картинки, авторы которых не давали разрешение на их использование\n",
            "[22:12.480 --> 22:25.080]  для обучения алгоритмов машины. Второй момент – это privacy. То есть примерно в 18-м году в Европе\n",
            "[22:25.080 --> 22:31.680]  и в Штатах приняли серьезные законы, которые регулируют использование персональных данных.\n",
            "[22:31.680 --> 22:37.160]  Что такое персональные данные? Вот, например, изображение вот этих двух людей – это персональные\n",
            "[22:37.160 --> 22:44.560]  данные. И не достаточно получить разрешение от фотографа, то есть сделать лицензирование,\n",
            "[22:44.560 --> 22:51.400]  но нужно еще по-хорошему получить разрешение от конкретных людей, которые изображены на данные\n",
            "[22:51.400 --> 22:57.720]  фотографии. Но на практике этого никто не делает. То есть до 18-го года в компьютерном зрении был\n",
            "[22:57.720 --> 23:03.640]  вообще дикий запад, когда собирали большие датасеты из интернета, не заморачиваясь с\n",
            "[23:03.640 --> 23:10.600]  лицензиями, их размечали и выкладывали. Не забудьте о никаких юридических нюансах. Сейчас это все\n",
            "[23:10.600 --> 23:16.440]  стало немножко сложнее, но в целом, если вы посмотрите какое-нибудь интервью OpenAI,\n",
            "[23:16.440 --> 23:25.800]  технического директора OpenAI, когда ее спрашивают, откуда картинки, например, на которых вы обучались,\n",
            "[23:25.800 --> 23:31.520]  откуда данные, она уходит от ответа и говорит, что у нас юридически все хорошо. Хотя по факту\n",
            "[23:31.520 --> 23:39.320]  сейчас все крупные компании выкачивают интернет почти целиком и регулярно выкачивают более свежей\n",
            "[23:39.320 --> 23:47.040]  части. И в целом, это такая юридическая лакуна, которая никак сейчас не покрыта. То есть если\n",
            "[23:47.040 --> 23:52.920]  вы будете в какой-то большой компании, которая сильно об этом заботится, то с подобными вещами\n",
            "[23:52.920 --> 24:01.320]  вы можете столкнуться. Когда возникли вот эти вопросы с лицензированием, стали делать другие\n",
            "[24:01.320 --> 24:07.840]  датасеты более свободные. Например, есть датасет OpenImages, он открытый именно в смысле\n",
            "[24:07.840 --> 24:13.880]  лицензирования. Все равно здесь есть люди и есть нюансы, что вот у конкретного человека нужно\n",
            "[24:13.880 --> 24:20.920]  получать разрешение, а это разрешение на использование изображения может быть отзываемым и так далее.\n",
            "[24:20.920 --> 24:26.640]  То есть на практике достаточно сложно все выполнить. В плане лицензирования это открытый датасет,\n",
            "[24:26.640 --> 24:37.600]  который был собран из мест, где явно указывалось лицензия от CCB. И дальше для этого датасета\n",
            "[24:37.600 --> 24:45.360]  делались как классификация, так и даже выделяли с сегментами какие-то объекты и много других\n",
            "[24:45.360 --> 24:55.520]  еще, много других вариантов разметки. Например, есть текстовое описание. Помимо общих задач\n",
            "[24:55.520 --> 25:04.120]  классификации, есть еще задача Fine-grained classification. Это мы берем какой-то класс типа птиц или машин,\n",
            "[25:04.120 --> 25:14.120]  или самолетов, или еды и начинаем выделять в этом классе объектов какие-то более fine-grained\n",
            "[25:14.120 --> 25:22.840]  детали. То есть если птица, то это какие-то виды птиц. Если машины, то марки машин. Если еда,\n",
            "[25:22.840 --> 25:30.680]  то это конкретные виды блюд. Какая-нибудь пицца, суп, какой-то конкретный стейк и так далее. И такая\n",
            "[25:30.680 --> 25:40.200]  задача, она позволяет проверять методы, насколько они хорошо различают не общий вид объектов,\n",
            "[25:40.200 --> 25:48.600]  а детали объектов. Теперь поговорим про то, как данные размечаются. Мы поговорили про то,\n",
            "[25:48.600 --> 25:55.560]  что их можно выкачивать из интернета. Вместе с картинками в интернете достаточно много\n",
            "[25:55.560 --> 26:01.720]  метаинформации, какие-то текстовые описания, может быть теги. Из них в целом можно вытаскивать\n",
            "[26:01.720 --> 26:07.200]  какую-то сырую информацию, и если ее тщательно профильтровать, то можно получать достаточно\n",
            "[26:07.200 --> 26:14.920]  неплохие датасеты. И сейчас, когда изображений собирается много, то обычно так и делают. Но в целом\n",
            "[26:14.920 --> 26:21.120]  все равно даже на больших датасетах сейчас приходится так или иначе прибегать к человеческой\n",
            "[26:21.120 --> 26:27.560]  разметке. И когда мы говорим про человеческую разметку, тут стоит рассказать про любопытную\n",
            "[26:27.560 --> 26:39.080]  историю. Называется механический турк. Это такой механизм, который был создан в 1770 году,\n",
            "[26:39.080 --> 26:49.400]  и он мог играть в шахматы и даже побеждать каких-то достаточно компетентных игроков. В 1820 году\n",
            "[26:49.400 --> 26:58.480]  было раскрыто, что это не просто какой-то сложный механизм, а это механизм с местом для человека,\n",
            "[26:58.480 --> 27:05.960]  то есть там внутри находился игрок, который с помощью этого сложного механизма мог управлять\n",
            "[27:05.960 --> 27:22.400]  этим псевдороботом. И вот этот исторический механизм дал название системе разметки,\n",
            "[27:22.400 --> 27:29.320]  которая дальше использовалась в Амазон. Мы дальше про нее поговорим, но вот прежде чем переходить к этой\n",
            "[27:29.320 --> 27:39.360]  системе Амазоновской, еще поговорим про этапный результат. Это разметка изображений галактик.\n",
            "[27:39.360 --> 27:46.520]  То есть астрономы в какой-то момент, в десятых годах сделали такой волонтерский проект,\n",
            "[27:46.520 --> 27:56.280]  куда они выложили большое количество изображений галактик и попросили волонтеров проклассифицировать\n",
            "[27:56.280 --> 28:01.640]  эти изображения на небольшое количество классов. Поучаствовал в результате 150 тысяч волонтеров,\n",
            "[28:01.640 --> 28:08.520]  которые сделали 60 тысяч меток, но это могло быть по нескольким меткам на изображении. При этом они\n",
            "[28:08.520 --> 28:14.160]  это все сделали бесплатно. Тогда были вот те золотые времена, когда это все делалось просто по фанам.\n",
            "[28:14.160 --> 28:19.360]  Сейчас, конечно, такое не прокатит, потому что есть большое количество сервисов, которые все делают\n",
            "[28:19.360 --> 28:24.360]  на коммерческой основе. Вот если говорить про коммерческую основу, механический турк дал\n",
            "[28:24.360 --> 28:32.360]  название амазоновскому сервису механический турк, в котором предоставляется интерфейс для\n",
            "[28:32.360 --> 28:42.400]  заказчика, интерфейс для разметчика. И там можно решать всевозможные задачки. Задачки\n",
            "[28:42.400 --> 28:49.400]  классификации, детекции и так далее. Можно проверять какие-то сайты и в целом там хорошо\n",
            "[28:49.400 --> 28:58.600]  кастомизируемые интерфейсы. Можно придумывать достаточно замороченные пайплайны. В России раньше\n",
            "[28:58.600 --> 29:05.440]  была ТЛАК, сейчас она разделилась на международную часть и на российскую часть. Российская часть\n",
            "[29:05.440 --> 29:11.040]  называется Яндекс.Задания. В целом, я очень рекомендую попробовать выйти на такой сервис и\n",
            "[29:11.040 --> 29:19.640]  попробовать с ним поработать даже для какой-то простой задачки. И даже есть в ШАДе, например, курс про то,\n",
            "[29:19.640 --> 29:26.880]  как вот эту разметку организовывать. Потому что, поскольку здесь уже данные за разметку данных\n",
            "[29:26.880 --> 29:33.600]  платят деньги, понятное дело, что возникли люди, которые хотят эту систему хакнуть, делают каких-то\n",
            "[29:33.600 --> 29:39.960]  ботов, которые за них начинают кликать. Соответственно, вы не просто выходите с заданий, вы начинаете думать,\n",
            "[29:39.960 --> 29:45.600]  как ваше задание можно хакнуть. Начинаете отсеивать ботов, какое-то подозрительное поведение\n",
            "[29:45.600 --> 29:53.280]  пользователя анализируете. Затем, если у вас сильно нагруженное задание, сложное, то его,\n",
            "[29:53.280 --> 30:01.960]  как правило, нужно разбивать на несколько частей. От простой до сложной части. Затем, не все вещи\n",
            "[30:01.960 --> 30:10.120]  можно проверять автоматически. Бывает, что нужно одних людей просить проверять других людей,\n",
            "[30:10.120 --> 30:16.920]  и при этом тех, кто проверяет, их же тоже нужно как-то проверить, что они не мухлюют. Поэтому для них\n",
            "[30:16.920 --> 30:24.240]  можно делать специальные экзамены и периодически им давать задание с каким-то известным ответом,\n",
            "[30:24.240 --> 30:31.640]  чтобы проводить такую вероятностную проверку, что они не мухлюют. На самом деле, когда у вас\n",
            "[30:31.640 --> 30:39.880]  задача становится более-менее реальной, то процедура разметки уже совсем тривиальная,\n",
            "[30:39.880 --> 30:47.600]  и в больших компаниях этим занимаются целые команды. Тут мы поговорили про данные, про классификацию,\n",
            "[30:47.600 --> 30:55.200]  откуда берется источник данных, про метрики. Есть ли какие-то вопросы?\n",
            "[30:55.200 --> 31:04.440]  Наверное, есть вопрос по поводу мультиклассификации. Вы сказали, что у нее есть два способа. Почему\n",
            "[31:04.440 --> 31:10.000]  не выделили мультиклассификацию, не разбили на два отдельных варианта и не назвали два\n",
            "[31:10.000 --> 31:17.960]  каких-то других способа? Почему именно мультиклассификацию сделали такой объемной?\n",
            "[31:17.960 --> 31:22.560]  Есть много классов классификации, есть атрибуты.\n",
            "[31:22.560 --> 31:28.840]  Вы приводили пример, если я правильно понял, когда у нас была бинарная классификация,\n",
            "[31:28.840 --> 31:36.720]  мы могли вероятностным вектор возвращать, а в мультиклассификации сказали, что можно либо так,\n",
            "[31:36.720 --> 31:40.520]  либо так, либо вероятностный вектор, либо вообще принадлежность к каким-то классам,\n",
            "[31:40.520 --> 31:44.560]  и почему-то не разделили мультиклассификацию на несколько подмножис, почему-то,\n",
            "[31:44.560 --> 31:50.280]  по отличию от бинарной классификации. Тут та же самая история, вы берете вектор вероятностей,\n",
            "[31:50.280 --> 31:58.840]  которые предсказали, и дальше берете максимум, например. И даже это сводите к тем же самым меткам.\n",
            "[31:58.840 --> 32:03.560]  Просто обобщение по факту бинарной классификации на размерности.\n",
            "[32:03.560 --> 32:08.360]  Да, конечно. Так, еще вопрос.\n",
            "[32:10.360 --> 32:18.920]  Окей, если вопроса нет, давайте пойдем дальше. Дальше мы поговорим про нейрогу и про уперсептору.\n",
            "[32:18.920 --> 32:28.800]  Нейросети в самом начале вдохновлялись биологическими нейростиометами в наш мозг,\n",
            "[32:28.800 --> 32:38.640]  и мозг состоит из достаточно сложной системы. Она может показаться и достаточно халатичной,\n",
            "[32:38.640 --> 32:48.840]  в основе которой находятся нейроны. Нейрон – базовая часть мозга и в целом любой нервной\n",
            "[32:48.840 --> 32:58.720]  системы, которая состоит из тела с ядром. На вход нейрон принимает электрические сигналы\n",
            "[32:58.720 --> 33:04.480]  от других нейронов. Он это делает с помощью таких отростков, которые называются дендриты.\n",
            "[33:04.480 --> 33:13.360]  Дальше происходит анализ в ядре поступивших электрических сигналов, и по длинному отростку\n",
            "[33:13.360 --> 33:22.000]  аксону, который может длину достигать до нескольких метров, выходной сигнал электрический\n",
            "[33:22.000 --> 33:30.920]  либо передается дальше, либо не передается в зависимости от того, как внутри были проанализированы\n",
            "[33:30.920 --> 33:38.840]  входные сигналы. И дальше аксон соединяется с другими нейронами, и это все получается\n",
            "[33:38.840 --> 33:47.600]  сеть направленная. Если мы это формализуем, то часто используют модель Maccala Capita, когда у нас\n",
            "[33:47.600 --> 34:01.960]  есть какой-то вход, обозначается x1 и так далее xn. Эти входы суммируются, по сути какие-то\n",
            "[34:01.960 --> 34:10.600]  числа суммируются с какими-то фисами, дальше добавляется некоторый сдвиг b, и затем эта сумма\n",
            "[34:10.600 --> 34:20.360]  подается в функцию активации f, и результат из этой функции уже является выходом нейрона. При этом\n",
            "[34:20.360 --> 34:29.920]  изначально нейроны были бинарные, и тогда функции активации были поруговые. Если мы нарисуем здесь\n",
            "[34:29.920 --> 34:40.160]  x и здесь у, то вот у нас вход, если просуммировался меньше нуля, то оставляем 0, если больше нуля, то\n",
            "[34:40.160 --> 34:47.880]  просто перетавим в единичку. Дальше, когда нейроны стали вещественными, стали использовать\n",
            "[34:47.880 --> 34:57.360]  дифференцируемую аппроксимацию этой пороговой функции, это сегмоиды. При этом у сегмоида далеко\n",
            "[34:57.360 --> 35:06.720]  от нуля градиент, обычно небольшой, поэтому стали использовать другие вариации. Например, функцию\n",
            "[35:06.720 --> 35:16.440]  real, которая везде имеет понятный градиент, либо 0, либо единица, и он не затухает. Нейрон является\n",
            "[35:16.440 --> 35:21.920]  линейным классификатором, то есть предположим, что у нас вот эти точки это какие-то картинки,\n",
            "[35:21.920 --> 35:28.680]  и они описываются какими-то признаками x. В базовом случае эти признаки x это значения\n",
            "[35:28.680 --> 35:37.880]  пикселей. Мы эти значения пикселей можем вытянуть в вектор, домножить на какой-то вектор весов,\n",
            "[35:37.880 --> 35:45.360]  просуммировать и дальше подать в какую-то функцию активации. При этом мы получаем какое-то значение,\n",
            "[35:45.360 --> 35:55.760]  если оно больше нуля, то мы говорим, что у нас класс 1, если оно меньше нуля, то мы говорим,\n",
            "[35:55.760 --> 36:02.080]  что класс другой. То есть, по сути, линейная классификация происходит за счет того,\n",
            "[36:02.080 --> 36:08.720]  что мы строим некоторую гиперплоскость в пространстве признаков, и дальше смотрим в одну и\n",
            "[36:08.720 --> 36:15.120]  другую сторону этой гиперплоскости и осуществляем классификацию. Дальше, если мы переходим к\n",
            "[36:15.120 --> 36:19.520]  многоклассовой классификации, то у нас возникает уже несколько нейронов по количеству классов. У\n",
            "[36:19.520 --> 36:26.440]  нас есть вход, то есть это какие-то признаки, в базовом случае это просто значение пикселей,\n",
            "[36:26.440 --> 36:34.280]  и у нас есть выход, это три нейрона. Каждый нейрон будет отвечать за свой класс. Например,\n",
            "[36:34.280 --> 36:45.360]  здесь три класса, самолет, олень и машина. Соответственно, нейрон отвечает за классификацию 1\n",
            "[36:45.360 --> 36:51.920]  против всех, то есть принадлежит ли данный объект, например, классу машина, либо не принадлежит. И для\n",
            "[36:51.920 --> 36:59.880]  того, чтобы нам осуществить классификацию, мы можем дальше получить вектор вероятности из\n",
            "[36:59.880 --> 37:08.360]  этого выхода, либо сделать выбрать максимум. Вот другой пример, то есть у нас картинка, мы ее вытянули\n",
            "[37:08.360 --> 37:15.320]  в вектор столбец. Дальше, для каждого класса у нас есть определенные веса, например, для кошки у\n",
            "[37:15.320 --> 37:23.000]  нас вот такие веса. Мы домножаем, делаем матричное векторное домножение, складываем с байесом,\n",
            "[37:23.000 --> 37:30.960]  получаем какие-то значения, и наибольшая скорость соответствует классу. В данном случае видно,\n",
            "[37:30.960 --> 37:41.920]  что классификация неправильно получилась. Дальше, поверх выходов нашего персептрона мы можем\n",
            "[37:41.920 --> 37:52.600]  накинуть функцию активации softmax, которая добавляет экспоненцирование и нормализацию. То\n",
            "[37:52.600 --> 37:59.720]  есть у нас наши скоры превращаются в вектор вероятности, который мы можем уже интерпретировать.\n",
            "[37:59.720 --> 38:06.280]  И для того, чтобы обучать, мы используем статартную cross entropy. Обучать мы можем с помощью\n",
            "[38:06.280 --> 38:15.840]  sgd, и как правило выборка достаточно большая, поэтому нам нужно выбирать из этой выборки какие-то\n",
            "[38:15.840 --> 38:23.960]  батчи, на которых мы будем постепенно шаг за шагом проходить и обновлять наши веса. Для тех,\n",
            "[38:23.960 --> 38:29.360]  кто не очень хорошо с этим всем знаком, на семинаре будет тоже некоторые еще дополнительные объяснения.\n",
            "[38:29.360 --> 38:40.720]  Перейдем теперь к многосвоенному персептрону. Мы можем упорядочить нейроны от входа к выходу,\n",
            "[38:40.720 --> 38:49.120]  и в простейшем случае, когда у нас будет возникать вот такой граф без каких-то сложных связей,\n",
            "[38:49.120 --> 38:54.720]  вот эти последовательные нейроны, то есть нейроны, которые находятся на одном расстоянии от входа,\n",
            "[38:54.720 --> 39:08.160]  мы будем объединять в слои. В классическом машинном обучении эти слои входные и выходные слои\n",
            "[39:08.160 --> 39:17.240]  у нас называются наблюдаемые, а промежуточные слои называются ненаблюдаемыми. При этом в любом\n",
            "[39:17.240 --> 39:24.360]  университетном фреймворке вы можете значения с этих нейронов вытащить. При этом скрытые слои\n",
            "[39:24.360 --> 39:32.640]  обычно имеют функции активации нелинейной для того, чтобы у нас все вот эти линейные функции не\n",
            "[39:32.640 --> 39:37.720]  схлопнулись в одну, чтобы действительно нейросеть задавала какую-то сложную функцию.\n",
            "[39:37.720 --> 39:44.480]  Количество выходов зависит от того, какая у нас задача. То есть здесь у нас может быть\n",
            "[39:44.480 --> 39:52.240]  трехклассовая классификация. Дальше у вас будет задачка регрессии ключевых точек лица,\n",
            "[39:52.240 --> 40:02.040]  там 14 точек лица с координатами х1 и у1 и так далее, х14 и у14. Тогда получается,\n",
            "[40:02.040 --> 40:06.680]  что у вас будет 28 выходов, там у вас уже не будет вектороверенностей, там будут\n",
            "[40:06.680 --> 40:14.720]  просто предсказываться конкретные координаты. При этом что такое слой в нейросети? В классическом\n",
            "[40:14.720 --> 40:24.080]  понимании слой это набор активаций конкретных нейронов. То есть вот значения, которые получаются\n",
            "[40:24.080 --> 40:34.520]  здесь на выходе из нелинейности, это как раз активации. Другой вариант, который часто используется в\n",
            "[40:34.520 --> 40:48.760]  фреймворках, это набор связей между нейронами и функции, которые они задают, и веса, которые мы\n",
            "[40:48.760 --> 40:55.640]  при этом выучиваем. То есть веса как раз и задает функции. То есть тут может быть путаница,\n",
            "[40:55.640 --> 41:03.520]  поэтому внимательно смотрите, если вы в какой-то работе или статье смотрите, что авторы имеют в\n",
            "[41:03.520 --> 41:08.640]  виду под слоем. Как правило, это как раз сейчас функциональное какое-то преобразование.\n",
            "[41:08.640 --> 41:19.520]  Как можно определять архитектуру нейронной сети? В целом это открытый вопрос. Есть набор теорем\n",
            "[41:19.520 --> 41:27.000]  об аппроксимационных способностях нейросети, то есть что мы можем приблизить любую функцию с\n",
            "[41:27.000 --> 41:35.880]  достаточной точностью, если у нас будет достаточно много нейронов. Но и там есть другие математические\n",
            "[41:35.880 --> 41:43.400]  условия. Андрей, я думаю, в семинаре это еще подсветит немножко. Но эти теоремы неконструктивные,\n",
            "[41:43.400 --> 41:51.320]  то есть они говорят о существовании нейросети, но они не указывают, как такой нейросеть построить.\n",
            "[41:51.320 --> 42:01.000]  Поэтому на практике возможны два варианта, ну даже три. Первый вариант, это вы можете\n",
            "[42:01.000 --> 42:06.320]  сочинить велосипед, но как правило это плохо будет работать. И вот действительно первый вариант,\n",
            "[42:06.320 --> 42:15.880]  который можно воспользоваться, это best practices, то есть для задач классификации или для любых других\n",
            "[42:15.880 --> 42:20.720]  задач, которые мы будем рассматривать в курсе, вы идете, смотрите свежие статьи, смотрите,\n",
            "[42:20.720 --> 42:26.800]  что люди сделали для вас и начинаете использовать эту архитектуру как базовую и ее может быть\n",
            "[42:26.800 --> 42:35.120]  чуть-чуть откручиваете. Другой вариант, это поиск архитектур, такая уже мета оптимизация\n",
            "[42:35.120 --> 42:48.640]  начинается, Neural Architecture, то есть мы задаем какие-то базовые блоки, это могут быть нейроны,\n",
            "[42:48.640 --> 42:54.000]  но как правило это более крупные блоки, какие-то наборы нейронов. И эти блоки мы можем как-то\n",
            "[42:54.000 --> 43:01.640]  соединять между собой и у нас есть какой-то алгоритм оптимизации или алгоритм поиска таких\n",
            "[43:01.640 --> 43:07.400]  архитектур, мы их как-то генерируем, можем каким-то генетическим алгоритмом, много разных вариантов\n",
            "[43:07.400 --> 43:12.840]  есть. И дальше, когда мы генерируем новые гипотезы архитектур, мы должны их проверить, то есть мы\n",
            "[43:12.840 --> 43:21.440]  обучаем архитектуру на какой-то задачи и смотрим какой получается результат. Вот этот поиск\n",
            "[43:21.440 --> 43:28.800]  архитектур, достаточно популярная тема, но очень затратная, то есть чтобы подобным заниматься,\n",
            "[43:28.800 --> 43:34.320]  вам как правило нужен хороший кластер, поэтому ваш вариант это номер один best practices.\n",
            "[43:34.320 --> 43:44.640]  Следующий важный этап, важный момент, откуда у нас берется градиент для обучения весов.\n",
            "[43:44.640 --> 43:52.040]  Вот предположим, что у нас есть какая-то простая нейросети, которая состоит из каких-то блоков,\n",
            "[43:52.040 --> 43:58.440]  расположенных линейно. На вход она принимает изображение, дальше идет через какие-то слои,\n",
            "[43:58.440 --> 44:10.880]  C1 и так далее, F8, считает какие-то активации, вообще говоря это тензоры. Дальше эти результаты\n",
            "[44:10.880 --> 44:19.080]  сети подаются в функцию потерь, которая задана аналитически, мы ее вычислили и нам нужно посчитать\n",
            "[44:19.080 --> 44:32.440]  градиенты D,E,D,V,1,8, то есть вот все, что здесь нарисовано, для того чтобы с помощью как раз\n",
            "[44:32.440 --> 44:39.480]  градиентного спуска это все обновить. Тут начинается обратное распространение ошибки,\n",
            "[44:39.480 --> 44:51.080]  работает оно следующим образом. У нас есть D,E,O,D,A8,A8 это активации, которые были здесь получены,\n",
            "[44:51.080 --> 45:00.360]  у нас это производное, мы ее можем посчитать аналитически. Дальше мы можем домножить ее на\n",
            "[45:00.360 --> 45:13.440]  частную производную D,A8, на D,V8 и таким образом мы получим первый градиент по весам. И вот эта\n",
            "[45:13.440 --> 45:22.320]  часть D,E по какой-то активации, она называется ошибкой и мы ее начинаем обратно распространять,\n",
            "[45:22.320 --> 45:30.320]  то есть вот у нас есть D,E,A8, D,E по D,A8 мы посчитали вот эту частную производную. Дальше нам\n",
            "[45:30.320 --> 45:39.600]  нужно ошибку пробросить вот сюда, то есть мы D,E по D,A8 домножаем на частную производную D,A8\n",
            "[45:39.600 --> 45:51.840]  по D,A7 и таким образом мы получили ошибку по активации уже седьмого слоя. И так мы идем\n",
            "[45:51.840 --> 46:01.320]  итеративно, пока не вернемся в начало. При этом вот эта операция домножения на якобиан,\n",
            "[46:01.320 --> 46:07.800]  она если ее напрямую кодить, она будет достаточно дорогой, потому что тут будут большие тензоры\n",
            "[46:07.800 --> 46:15.480]  A8 и A7, соответственно у нас там размерность одного тензора домножается на размерность другого,\n",
            "[46:15.480 --> 46:20.400]  в явном виде у нас это может не поместиться в память компьютера, в подавляющем большинстве\n",
            "[46:20.400 --> 46:26.800]  современных нейросетей. Поэтому точно так же как у нас есть некоторая операция forward,\n",
            "[46:26.800 --> 46:32.280]  которая у нас задает как ведет себя слой, это может быть планосвязные слой и другие\n",
            "[46:32.280 --> 46:37.480]  свои, которые мы дальше посмотрим. Так как у нас и должна быть операция backward для каждого\n",
            "[46:37.480 --> 46:49.800]  слоя, которая как раз осуществляет вот эту и вот эту операцию в явном виде, ну в каком-то\n",
            "[46:49.800 --> 46:56.840]  специальном виде. При этом это домножение происходит, но мы не считаем якобиан в явном\n",
            "[46:56.840 --> 47:01.840]  виде, иначе у нас это все не поместится в памяти компьютера. Соответственно вот эти все процедуры\n",
            "[47:01.840 --> 47:07.960]  forward backward вам в домашнем задании надо будет реализовать для того, чтобы как раз точно\n",
            "[47:07.960 --> 47:16.840]  понимать как это работает внутри любого фреймворка. Теперь мы можем перейти к простенькой нейросети,\n",
            "[47:16.840 --> 47:23.160]  это такая интересная нейросеть, которая использовалась до метода Виола Джонса, про которую я вам рассказывал\n",
            "[47:23.160 --> 47:32.240]  на первой лекции. Роли фейс детектор, у нас есть какая-то картинка, мы делаем из нее пирамиду\n",
            "[47:32.240 --> 47:37.520]  разрешений и дальше бегаем окошком, то есть вырезаем кусочки изображения и пытаемся понять,\n",
            "[47:37.520 --> 47:44.400]  есть ли в данном окошке лицо или нет. Нормализуем яркость и подаем в очень простенькую нейросетку,\n",
            "[47:44.400 --> 47:53.240]  на фото она принимает картинку 20 на 20, делит это изображение разными способами, на 4 части,\n",
            "[47:53.240 --> 48:06.320]  на 16 частей, тут еще на полосочке и к каждой области применяются свои нейроны. И вот дальше это все\n",
            "[48:06.320 --> 48:12.800]  объединяется для бинарной классификации. Такая простенькая нейросеть, она достаточно хорошо\n",
            "[48:12.800 --> 48:21.320]  работала до Виола Джонса, но при этом была медленная. Соответственно это вот такое видение\n",
            "[48:21.320 --> 48:27.560]  историческое, какие-то базовые вещи по нейросетям, есть ли вопросы. Дальше мы идем к сверточным\n",
            "[48:27.560 --> 48:33.680]  нейросетям, тоже небольшое историческое видение. А вот скажите, пожалуйста, вы хотите сказать,\n",
            "[48:33.680 --> 48:39.400]  что получается любую нейросеть можно представить буквально как вот набор функций активации\n",
            "[48:39.400 --> 48:45.600]  нейронов, буквально тензор, функция, тензор, функция, функция и все, то есть это звучит слишком\n",
            "[48:45.600 --> 48:51.080]  просто, как будто для крутых нейросетей. Ну да, примерно так, просто функции могут быть разные,\n",
            "[48:51.080 --> 49:00.320]  их там можно накручивать, придумать разные функции. Так, вот тут хороший вопрос, есть ли где-то удобные\n",
            "[49:00.320 --> 49:08.720]  банки знаний по best practices. Во-первых, этот курс это банк знаний, таких более-менее современных,\n",
            "[49:08.720 --> 49:15.480]  ну там я буду стараться читать, сейчас у нас историческое видение, но мы будем разбирать самые\n",
            "[49:15.480 --> 49:22.080]  современные статьи, там то, что обычно словом статьи взорта называется. Вот, а дальше уже у нас будет\n",
            "[49:22.080 --> 49:29.200]  даже отдельная лекция, где я буду рассказывать, как эти знания поддерживать. Вот, для, если вы в\n",
            "[49:29.200 --> 49:35.120]  какой-то узкой области будете находиться, ну занимаетесь классификацией или детекцией, тогда это не так\n",
            "[49:35.120 --> 49:41.000]  сложно, а вот если вы хотите поддерживать знания, вот как мне приходится поддерживать знания для данного\n",
            "[49:41.000 --> 49:48.920]  курса по широкому набору областей, то это вот может быть тяжело, потому что по компьютерному зрению\n",
            "[49:48.920 --> 49:57.760]  каждую неделю на архиве выходит ну порядка 500 или 1000 статей, соответственно их надо уметь\n",
            "[49:57.760 --> 50:02.160]  фильтровать, их надо уметь читать, про это я буду отдельно рассказывать. Как вот тогда\n",
            "[50:02.160 --> 50:07.040]  упрощаются вычисления производных, чтобы они не высчитывались полностью, это зависит от конкретного\n",
            "[50:07.040 --> 50:16.840]  слоя и я думаю на семинаре и в домашке вы с этим подробно разберетесь. Что такое архив, тоже\n",
            "[50:16.840 --> 50:28.000]  правильный вопрос, архив это, давайте я напишу в чатик, архив.org, это сайт, на котором выкладываются\n",
            "[50:28.000 --> 50:36.680]  свежие статьи научные, соответственно я предполагаю, что наш курс это такой пререквизит, после того,\n",
            "[50:36.680 --> 50:41.640]  какой курс освоите вы сможете без проблем читать любые научные статьи современные и как бы\n",
            "[50:41.640 --> 50:46.360]  получать знания из первоисточника. Как это устроено, мы еще поговорим про научные статьи,\n",
            "[50:46.360 --> 50:56.240]  про конференции. Есть ли еще вопросы? Если вопросов нет, давайте пойдем дальше. Сверточные сети,\n",
            "[50:56.320 --> 51:08.040]  откуда они взялись? Взялись они из экспериментов биологов-физиологов, конкретно можно назвать имена\n",
            "[51:08.040 --> 51:16.840]  Хьюбели и Визеля, они проводили эксперименты с визуальной корой головного мозга, здесь они\n",
            "[51:16.840 --> 51:23.080]  сфотографированы примерно в том возрасте, когда они проводили свои известные исследования,\n",
            "[51:23.520 --> 51:29.840]  за эти исследования они получили нобелевскую премию, они проводили эксперименты на млекопитающих,\n",
            "[51:29.840 --> 51:38.720]  на макаках и вот данный эксперимент на кошке, соответственно у нас есть кошка, она под анестезией,\n",
            "[51:38.720 --> 51:45.640]  голова у нее зафиксирована, можете на youtube найти видосик, куда-то взято там Хьюбели и Визеля\n",
            "[51:45.640 --> 51:53.320]  эксперимент и сразу его увидеть, но я показывать не буду, на эту кошку немножко жалко смотреть,\n",
            "[51:53.320 --> 52:05.400]  потому что у нее голова закреплена и под анестезией она находится. Дальше кошке показывали телевизор,\n",
            "[52:05.400 --> 52:13.320]  электроэлто телевизор, на котором показывались базовые стимулы, я про эти стимулы немножко\n",
            "[52:13.320 --> 52:20.640]  рассказывал в лекции про JPEG, здесь стимулы были чуть-чуть другие, не синусоиды, здесь были вот\n",
            "[52:20.640 --> 52:29.000]  либо такие палочки, либо там еще они на границе тоже смотрели, например если мы говорим про вот\n",
            "[52:29.000 --> 52:36.440]  эти палочки светящиеся, они проводили следующие эксперименты, они подключали к одной части\n",
            "[52:36.440 --> 52:45.560]  головного мозга, к клеткам, которые называются простыми и начинали считывать электрический сигнал\n",
            "[52:45.560 --> 52:57.040]  вот из этой области, на телевизоре брали вот этот светящийся трубку, ее сдвигали и соответственно\n",
            "[52:57.040 --> 53:05.160]  здесь они крестиками помечали место, в котором был сильный электрический сигнал, дальше где вот\n",
            "[53:05.160 --> 53:12.120]  здесь нолики, это они сдвигали эту трубку сюда и наблюдали что электрического сигнала нету,\n",
            "[53:12.120 --> 53:21.320]  таким образом они нашли простые клетки, они показаны здесь, то есть это клетки, которые\n",
            "[53:21.320 --> 53:29.400]  регулируют, а нет это уже сложные клетки, они нашли простые клетки, которые регулируют на\n",
            "[53:29.400 --> 53:37.440]  конкретную ориентацию, то есть какой-то угол тета и какое-то местоположение, то есть когда они\n",
            "[53:37.440 --> 53:44.440]  сдвигали, то уже реакция прекращалась, дальше они смотрели на другую часть мозга и там они нашли\n",
            "[53:44.440 --> 53:53.520]  сложные клетки, это клетки, которые уже реагируют на вот этот стимул под определенным углом вне\n",
            "[53:53.520 --> 54:01.040]  зависимости от того, где этот стимул находится в зрительном поле, при этом когда они этот стимул\n",
            "[54:01.040 --> 54:11.680]  поворачивали там на 60 градусов или на 15, то видно что отклик был славным, дальше если пытаться эти\n",
            "[54:11.680 --> 54:22.160]  простые и сложные клетки формализовать, то есть записать как-то математически, то можно прийти\n",
            "[54:22.160 --> 54:30.320]  к сверткам и вот посмотрим на простенький пример, как можно моделировать текстуру, то есть у нас\n",
            "[54:30.320 --> 54:39.880]  здесь простенькая текстура, здесь уже не эти палочки, а здесь у нас уголки и крестики, мы можем эти\n",
            "[54:39.880 --> 54:48.360]  уголки и крестики сворачивать сверткой с фильтром, который реагирует на границы определенной\n",
            "[54:48.360 --> 54:56.360]  ориентации, дальше усиливаем отклики и видим, что где-то активация сильнее, где-то активация слабее,\n",
            "[54:56.360 --> 55:05.800]  если мы наберем таких фильтров большое количество, то есть какие-то разные ориентации, то по сути у нас\n",
            "[55:05.800 --> 55:12.600]  получится банк фильтров и сворачивая картинку с каждым этим фильтром, мы для каждого пикселя\n",
            "[55:12.600 --> 55:21.520]  будем получать какой-то вектор-признак, который описывает данный пиксель границами разных направлений.\n",
            "[55:21.520 --> 55:30.560]  Банк фильтров мы математически можем записать, например, с помощью функции габора, это известная\n",
            "[55:30.560 --> 55:37.760]  функция, которая сначала использовалась в физике для изучения лазеров, потом было независимо\n",
            "[55:37.760 --> 55:47.880]  переоткрыто в компьютерных науках, формализуется она как экспонента, домноженная на синусоиду, при этом\n",
            "[55:47.880 --> 55:58.120]  есть параметры, которые мы можем регулировать, это размах гауссианы, ее вытянутость, направление,\n",
            "[55:58.120 --> 56:05.480]  длину волны и сдвиг для синусоида, то есть мы получаем вот такие фильтры, которые нам позволят\n",
            "[56:05.480 --> 56:16.320]  смотреть на текстуры. Теперь перейдем к простым и сложным клеткам. Простые клетки мы будем\n",
            "[56:16.320 --> 56:31.360]  моделировать с помощью сверток с каким-то определенным ядром, то есть у нас будет,\n",
            "[56:31.360 --> 56:38.000]  по сути, взвешенная сумма пикселей, вот точно так же, как мы смотрели на второй лекции. Дальше,\n",
            "[56:38.000 --> 56:45.440]  для того чтобы у нас получится, при операции свертки у нас получится много откликов для каждой\n",
            "[56:45.440 --> 56:55.840]  окрестности локальной и для того чтобы нам добиться инвариантности к сдвигу, то есть получить какую-то\n",
            "[56:55.840 --> 57:03.560]  функцию, которая будет смотреть на всю окрестность, мы можем брать вот эти выходы из сверток и\n",
            "[57:03.560 --> 57:11.200]  применять операцию максимума. Где бы у нас не находился подклик, мы все равно его увидим.\n",
            "[57:11.200 --> 57:21.920]  Теперь посмотрим, как можно свертку и операцию максимума реализовать в нейросетях. Свертку по\n",
            "[57:21.920 --> 57:31.480]  всему изображению, то есть линейную фильтрацию, мы можем смоделировать, используя слой нейронов,\n",
            "[57:31.480 --> 57:38.360]  у которых будут одинаковые пошаренные веса. То есть как будет работать один нейрон. Нейрон будет\n",
            "[57:38.360 --> 57:44.000]  смотреть не на все изображение входное, а только на его небольшую окрестность, в данном случае 3х3.\n",
            "[57:44.000 --> 57:54.880]  И дальше нейрон применяет какие-то веса и дает выход. Нейрон у нас при этом может смотреть не\n",
            "[57:54.880 --> 58:01.280]  на одноканальные изображения, а на многоканальные изображения, предположим, что у нас входного\n",
            "[58:01.280 --> 58:13.760]  изображения DIN входных каналов. Тогда у нейрона одного будет DIN, умноженное на размер ядра K на K,\n",
            "[58:13.760 --> 58:27.040]  плюс один еще параметр это будет bias. Дальше, если у нас в данном слое будет не одна свертка,\n",
            "[58:27.040 --> 58:32.960]  которая моделируется с набором нейронов, а несколько сверток, тогда у нас на входе будет\n",
            "[58:32.960 --> 58:40.320]  какое-то DIN количество каналов, на выходе у нас будет DOUT каналов. То есть каждая свертка будет\n",
            "[58:40.320 --> 58:46.320]  многоканальной, при этом для каждого канала мы будем облучать свое ядро. Тогда у нас будет уже\n",
            "[58:46.320 --> 58:59.160]  DOUT сверток, у каждой свертки DIN параметров на K и плюс еще у нас по одному bias на каждую\n",
            "[58:59.160 --> 59:10.280]  свертку, то есть DOUT параметров. То есть сверточный слой это набор сверток по одному и тому же входу,\n",
            "[59:10.280 --> 59:24.280]  при этом у этих сверток DOUT обучаемых ядер, у которых DIN на K на K параметров. Теперь поговорим про\n",
            "[59:24.280 --> 59:31.040]  операцию максима. Для того, чтобы нам быть инвариантными к позиции отклика, мы будем использовать\n",
            "[59:31.040 --> 59:40.120]  нейроны, у которых нет параметров, они просто применяют функцию максима. Такая будет функция\n",
            "[59:40.120 --> 59:52.360]  активации. Соответственно, эти нейроны у нас не обучаемые, но мы их можем засовывать как элемент\n",
            "[59:52.360 --> 01:00:03.760]  нейросети. Самый популярный вид операции максима это max pooling. Разбиваем изображение на квадратики\n",
            "[01:00:03.760 --> 01:00:11.880]  размером 2х2, пусть у нас тут, например, значение 1, 2, 3, 4 и дальше из каждого такого квадратика мы\n",
            "[01:00:11.880 --> 01:00:18.440]  получаем по одному значению, в левом нижнем углу мы получаем значение 4. При этом, когда мы\n",
            "[01:00:18.440 --> 01:00:29.360]  делаем backprop, нам нужно посчитать производную. Мы будем задавать производную только в том элементе,\n",
            "[01:00:29.360 --> 01:00:36.560]  откуда у нас пришел максимум. Здесь у нас будут нули, а здесь будет единичка. Это будет операция\n",
            "[01:00:36.560 --> 01:00:44.080]  максимума со сдвигом 2. Мы идем как с сверткой, плавающим окошком, со сдвигом 2, а не 1, как обычно\n",
            "[01:00:44.080 --> 01:00:51.120]  в свертках и вместо линейной фильтрации применяем операцию максимума. Если это все подытожить,\n",
            "[01:00:51.120 --> 01:00:59.400]  то можно получить такую известную работу историческую, называется неокакнетрон. Это сетка,\n",
            "[01:00:59.400 --> 01:01:07.560]  которая состоит из чередующихся простых и сложных клеток. То есть у нас свертка, операция максима,\n",
            "[01:01:07.560 --> 01:01:13.480]  дальше свертка, операция максима, свертка и в какой-то момент операция максима уже смотрит\n",
            "[01:01:13.480 --> 01:01:20.960]  на все изображение. Таких сверток у нас несколько и в конце мы получаем какой-то вектор-признак,\n",
            "[01:01:21.960 --> 01:01:30.680]  по которому уже делаем классификацию. При этом неокакнетрон не умели эффективно обучать,\n",
            "[01:01:30.680 --> 01:01:40.880]  потому что еще не было переоткрыт метод обратного распространения ошибки. В 86 году Хинтон переоткрыл\n",
            "[01:01:40.880 --> 01:01:46.960]  метод обратного распространения ошибки. Если мы сложим неокакнетрон с обратным распространением\n",
            "[01:01:46.960 --> 01:01:57.920]  ошибки, то получим работу линей. Это первая сверточная нейросеть. Современные нейросети не так\n",
            "[01:01:57.920 --> 01:02:05.640]  сильно от нее отличаются. Линей использовался для классификации рукописных цифр и рукописных\n",
            "[01:02:05.640 --> 01:02:12.320]  букв. В данном случае на примере букв тут показывается, правда нарисована буква, а классификация\n",
            "[01:02:12.320 --> 01:02:24.240]  на 10 классов, то есть это на самом деле классификация цифр. У нас на вход изображение размером 32 на 32,\n",
            "[01:02:24.240 --> 01:02:31.480]  и у нас тут происходит то же самое, как и в неокакнетроне. То есть у нас сначала свертки,\n",
            "[01:02:31.480 --> 01:02:38.720]  дальше операция макс пуллинг, свертка макс пуллинг. Потом мы получили какие-то выходы,\n",
            "[01:02:38.720 --> 01:02:50.160]  мы эти выходы, какие-то карты, мы их вытягиваем в вектора и уже применяем несколько полносвязных\n",
            "[01:02:50.160 --> 01:02:59.440]  слоев. В результате получаем 10 выходов, здесь уже softmax и классификация. При этом очень рекомендую\n",
            "[01:02:59.440 --> 01:03:08.160]  для понимания попробовать посчитать количество параметров во всех этих слоях. В свертках его вообще\n",
            "[01:03:08.160 --> 01:03:14.520]  легко считать, потому что свертки не зависят от того, какого размера тензор приходит им на вход,\n",
            "[01:03:14.520 --> 01:03:21.040]  они зависят от количества входных каналов, от количества выходных каналов и от ядра. У\n",
            "[01:03:21.040 --> 01:03:30.680]  максимума нету параметров, а вот у персептронов, то есть у полносвязных слоев, там уже интереснее,\n",
            "[01:03:30.680 --> 01:03:36.560]  потому что нужно правильно посчитать размер входного тензора и в зависимости от входного тензора\n",
            "[01:03:36.560 --> 01:03:43.080]  у нас там будет меняться количество параметров. Соответственно очень рекомендую посчитать и вот это\n",
            "[01:03:43.080 --> 01:03:50.160]  простое упражнение, оно позволяет лучше разобраться, как устроены свертки и другие\n",
            "[01:03:50.160 --> 01:04:00.600]  процессы в инверстиях. Если посмотреть, какие признаки обучаются вот здесь, например, на первом\n",
            "[01:04:00.600 --> 01:04:09.920]  слое, на первом сверточном слое, и тут мы будем визуализировать ядра, там в данном случае ядра\n",
            "[01:04:09.920 --> 01:04:20.040]  размером 5 на 5, но тут показаны из алекснета ядра размером 11 на 11, и видно, что несмотря на то,\n",
            "[01:04:20.040 --> 01:04:25.440]  что мы не использовали никакого явного аналитического прайора, то есть какие-нибудь\n",
            "[01:04:25.440 --> 01:04:32.360]  функции гобора не сдавали подобные вещи, видно, что ядра, которые обучаются в результате, похожи\n",
            "[01:04:32.360 --> 01:04:40.520]  на фильтр гобора или они считают границы или они считают какие-то высокочастотные компоненты, то\n",
            "[01:04:40.520 --> 01:04:48.240]  есть видно, что в результате обучения нейросети мы получили что-то достаточно близкое к тому,\n",
            "[01:04:48.240 --> 01:04:55.040]  как мы представляем, как работает человеческое зрение. При этом какие-то слои могут друг друга\n",
            "[01:04:55.040 --> 01:05:03.200]  дублировать, то есть здесь, например, вот эти две свертки, они дублируют, то есть нету гарантии,\n",
            "[01:05:03.200 --> 01:05:10.280]  что это не избыточное представление. Важное понятие в нейронных сетях – это рецептивное поле,\n",
            "[01:05:10.280 --> 01:05:17.520]  это область, на которую может посмотреть конкретный нейрон. Ещё глубже мы идём в нейросеть,\n",
            "[01:05:17.520 --> 01:05:24.960]  тем на большую часть изображений гипотетический нейрон могут посмотреть. Это зависит от архитектуры\n",
            "[01:05:24.960 --> 01:05:33.120]  нейросети. Есть такое другое понятие, называется эффективное рецептивное поле, то есть это та\n",
            "[01:05:33.120 --> 01:05:39.000]  область изображения, на которую нейрон фактически смотрит с конкретными весами на конкретном\n",
            "[01:05:39.000 --> 01:05:46.480]  изображении. Архитектурно, то он может смотреть условно область 200 на 200, а когда мы его обучили,\n",
            "[01:05:46.480 --> 01:05:51.760]  может так оказаться, что он смотрит на область размером 100 на 100, и это нужно имперически выяснять.\n",
            "[01:05:51.760 --> 01:06:00.640]  Соответственно, это вот то, что касается сверточных нейронных сетей. Есть ли какие-то вопросы?\n",
            "[01:06:00.640 --> 01:06:11.080]  Да, у меня есть вопросик. Можем с вами вернуться на 34-й слайд, пожалуйста. Вот, и мне не совсем\n",
            "[01:06:11.080 --> 01:06:19.480]  понятно, исходя из какой последовательности рассуждений разработчик нейронной сети приходит к тому,\n",
            "[01:06:19.480 --> 01:06:26.360]  что у него на определенных этапах ядра вот такого конкретного размера, такое количество каналов,\n",
            "[01:06:26.360 --> 01:06:35.600]  это как-то грейдсерчем подбирается или каким образом? Хороший вопрос. Но если у вас нейросеть\n",
            "[01:06:35.600 --> 01:06:41.360]  достаточно маленькая, каким-то грейдсерчем, наверное, можно подобрать. Но вообще это чисто\n",
            "[01:06:41.360 --> 01:06:47.680]  империка. То есть вот конкретно как Линет подбирался, честно говоря, я не знаю. Вот какие-то правила,\n",
            "[01:06:47.680 --> 01:07:01.480]  как зависит. Там есть некоторая империка по количеству сверток последовательных. Условно,\n",
            "[01:07:01.480 --> 01:07:09.840]  если у нас тут было бы 128, дальше было бы 256 сверток. Их становится все больше. Мы про это\n",
            "[01:07:09.840 --> 01:07:16.040]  еще поговорим. Какая-то империка есть. Дальше мы, когда будем еще на следующей лекции разбираться\n",
            "[01:07:16.040 --> 01:07:25.240]  в архитектурах, вы увидите. Чуть больше будет понятно. Но это как бы best practices. Каких-то\n",
            "[01:07:25.240 --> 01:07:32.640]  жестких правил, каких-то наказанных правил тут нет. А вот может быть это очень философский вопрос,\n",
            "[01:07:32.640 --> 01:07:38.440]  но тем не менее. Разработчик Линета, когда он ее представил, он доказывал коллегам,\n",
            "[01:07:38.440 --> 01:07:46.000]  что именно такое сочетание дает наилучший результат. Я имею в виду, что если он на первом\n",
            "[01:07:46.000 --> 01:07:52.040]  сверточном этапе добавит еще один канал, будет ли это лучше или хуже? Ему такой вопрос\n",
            "[01:07:52.040 --> 01:08:02.440]  конечно можно добавлять и можно только еще раз обучить и провалидироваться. Пытаться\n",
            "[01:08:02.440 --> 01:08:09.200]  менять количество сверток, количество нейронов в полносвятных слоях и в какой-то момент просто\n",
            "[01:08:09.200 --> 01:08:16.080]  уткнемся либо в переобучение, либо в то, что у нас качество будет расти. Максимум все,\n",
            "[01:08:16.080 --> 01:08:26.080]  что мы можем показать. Я примерно понял. Спасибо большое. Есть ли еще вопросы? Теперь давайте\n",
            "[01:08:26.080 --> 01:08:33.560]  поговорим про AlexNet. Как я говорил, это этапный результат. Это нейросеть, которая в 2012 году\n",
            "[01:08:33.560 --> 01:08:44.200]  показала на соревновании ImageNet очень хорошие результаты. В целом можно говорить о том,\n",
            "[01:08:44.200 --> 01:08:53.080]  что с 1998 года до 2012 года в распознавании изображений в компьютерном зрении был такой классический\n",
            "[01:08:53.080 --> 01:09:02.720]  этап, когда данных было не очень много и мощностей тоже было не очень много. То есть одно дело,\n",
            "[01:09:02.720 --> 01:09:08.360]  когда вы обучаете Linet и его Nutspool обучить совершенно не проблема. Другое дело,\n",
            "[01:09:08.360 --> 01:09:12.840]  когда вы хотите работать с картинками большего разрешения, когда у вас нейросеть становится\n",
            "[01:09:12.840 --> 01:09:21.600]  больше на цепу, это достаточно плохо ложится. Соответственно, в промежуток 15 лет развивались\n",
            "[01:09:21.600 --> 01:09:28.800]  алгоритмы классические. Это всевозможные признаки, евристические или какие-то иерархии признаков,\n",
            "[01:09:28.800 --> 01:09:34.440]  которые ресечеры как-то сами придумывали. У них было меньшее число настраиваемых параметров\n",
            "[01:09:34.440 --> 01:09:41.560]  и их обучали какими-то классическими методами машинного обучения типа SV или Boosting. Но мы это\n",
            "[01:09:41.560 --> 01:09:48.920]  рассматривать не будем, а посмотрим этот качественный скачок, который произошел. Давайте сначала посмотрим\n",
            "[01:09:48.920 --> 01:09:59.480]  на архитектуру Alexnet. На фото Alexnet принимает изображение 224х224. Дальше у нас идут сначала\n",
            "[01:09:59.480 --> 01:10:09.920]  свертки и пуллинг. Свертки достаточно большие 11х11, потом 5х5, 3х3 и так далее. Затем в какой-то\n",
            "[01:10:09.920 --> 01:10:16.920]  момент мы накопили достаточно информации обо всем изображении. Помните, что я говорил про\n",
            "[01:10:16.920 --> 01:10:24.800]  Reset to Field. Мы вытягиваем эти признаки в вектора и начинаем применять тяжелые планосвязные слои,\n",
            "[01:10:24.800 --> 01:10:33.000]  два планосвязных слоя 4.0.96 и планосвязные слои с классификацией на тысячу классов. Если расписывать\n",
            "[01:10:33.000 --> 01:10:39.200]  параметры Alexnet, то наибольшее количество параметров находится в хвосте. В целом,\n",
            "[01:10:39.200 --> 01:10:46.240]  опять же, я рекомендую расписать, как такая домашка без проверки, количество параметров Alexnet и\n",
            "[01:10:46.240 --> 01:10:52.120]  попробовать себя потом проверить, в какой-нибудь торче запустить. Там есть подсчет параметров,\n",
            "[01:10:52.120 --> 01:10:59.840]  и вот удостоверится, что вы правильно понимаете, как устроены слои. Соответственно, видно, что здесь\n",
            "[01:10:59.840 --> 01:11:06.200]  как-то сначала увеличивается количество сверток, потом чуть меньше становится, но это как бы ивристика,\n",
            "[01:11:06.200 --> 01:11:13.760]  как я говорил, и дальше два тяжелых слоя. По факту архитектурно поменялись только какие-то\n",
            "[01:11:13.760 --> 01:11:24.040]  гиперпараметры. Что еще интересного произошло? И тут следующая история. Произошли две вещи,\n",
            "[01:11:24.040 --> 01:11:34.680]  которые сделали возможным. Обучение Alexnet, первое, стало достаточное количество изображений,\n",
            "[01:11:34.680 --> 01:11:39.960]  то есть вот эта работа по скрапингу интернета, которую провели в ImageNet,\n",
            "[01:11:39.960 --> 01:11:47.040]  она как раз позволила получить достаточное количество картинок, чтобы вот это огромное\n",
            "[01:11:47.040 --> 01:11:52.680]  количество параметров, а в Alexnet 60 миллионов параметров, то есть если у вас 60 миллионов\n",
            "[01:11:52.680 --> 01:11:57.840]  параметров, то у вас количество изображений должно исчисляться миллионами для обучающих.\n",
            "[01:11:57.840 --> 01:12:06.840]  Это первая история, но при этом изображений все равно в Vanilla ImageNet было недостаточно,\n",
            "[01:12:06.840 --> 01:12:13.160]  поэтому в Alexnet делалось достаточно много аугументаций, про это вы будете говорить еще на\n",
            "[01:12:13.160 --> 01:12:20.720]  семинаре. Аугументация – это по сути мы как-то пытаемся шевелить наши картинки, можно вспомнить\n",
            "[01:12:20.720 --> 01:12:26.320]  любые преобразования, которые мы изучали до этого, это могут быть какие-то повороты,\n",
            "[01:12:26.320 --> 01:12:33.320]  масштабирования, кропы, гауссовские размытия, можно джипэк-артефакты добавлять как-то,\n",
            "[01:12:33.320 --> 01:12:40.640]  шатать немножко свет, контраст и так далее. Все, что мы обсуждали, как раз здесь применимо.\n",
            "[01:12:40.640 --> 01:12:51.360]  Дальше из ключевых предпосылок такой в ренессансе нейросети – это GPU. Как мы\n",
            "[01:12:51.360 --> 01:12:59.880]  обсудили в нейросети много нейронов, которые делают простые операции. И GPU, который изначально\n",
            "[01:12:59.880 --> 01:13:06.480]  развивался для компьютерных игрушек, как раз состоит из маленьких вычислителей, достаточно\n",
            "[01:13:06.480 --> 01:13:15.320]  обрезанных, но которых много и которые параллельно могут выполнять простые операции эффективно. Мы\n",
            "[01:13:15.320 --> 01:13:20.280]  можем раскладывать на эти простые вычислители какие-то базовые операции вроде матричного\n",
            "[01:13:20.280 --> 01:13:26.440]  умножения, и на GPU это будет работать намного быстрее, чем на CPU. В тот момент, когда у нас\n",
            "[01:13:26.440 --> 01:13:31.520]  появилось достаточное количество изображений и игровые видеокарты стали достаточно мощными,\n",
            "[01:13:31.520 --> 01:13:38.840]  и появилась возможность программировать эти GPU, то есть то, что обычно называется слон CUDA\n",
            "[01:13:38.840 --> 01:13:48.720]  для NVIDIA карт, в этот момент как раз появилась возможность обучать нейросети. AlexNet по имени\n",
            "[01:13:48.720 --> 01:13:54.640]  Алекса Криджевского обучался на самописном фреймворке, который Алекс сам написал, называется\n",
            "[01:13:54.640 --> 01:14:05.280]  CUDA ConvNet. И раньше он был на Google Code выложен, сейчас Google Code уже закрыт, но я думаю, что вы можете\n",
            "[01:14:05.280 --> 01:14:11.600]  поискать и где-то в каких-то архивах найти и посмотреть, какой он это реализовал. То есть там написано\n",
            "[01:14:11.600 --> 01:14:18.920]  это все на плюсах на CUDA, и там дальше питоновские биндинги, то есть такой прообраз фреймворка.\n",
            "[01:14:18.920 --> 01:14:26.480]  Помимо этого есть еще две важные идеи, но достаточно простые, это Reo-активация, то есть вот\n",
            "[01:14:26.480 --> 01:14:34.120]  эта активация простенькая, почти линейная вместо сигмоиды, у которой не затухал градиент, и dropout.\n",
            "[01:14:34.120 --> 01:14:42.760]  Про dropout вы поговорите на семинаре и будете делать в домашке, но по сути это случайное\n",
            "[01:14:42.760 --> 01:14:49.600]  занудление каких-то нейронах, особенно хорошо это работает для толстых полносвязных слоев,\n",
            "[01:14:49.600 --> 01:14:57.520]  и вот когда этих слоев было много в нейросетях dropout активно использовался, сейчас он уже\n",
            "[01:14:57.520 --> 01:15:05.240]  используется реже. При этом обучалась нейросеть на 2 кпу, на каждой кпу по 3 гигабайта оперативной\n",
            "[01:15:05.240 --> 01:15:13.680]  памяти, обычной оперативной памяти было 5 гигабайт, жесткий диск 27 гигабайт. Это у Алекса был\n",
            "[01:15:13.680 --> 01:15:26.680]  десктоп, на котором он обучал неделю сетку. При этом есть такая байка, что Алекс не очень любил\n",
            "[01:15:26.680 --> 01:15:34.040]  писать технические отчеты и статьи, и он договорился с Хинтоном, что каждую неделю, когда он будет\n",
            "[01:15:34.040 --> 01:15:41.160]  улучшать результат нейросети в соревнования, он будет получать поблажку, что ему не нужно будет\n",
            "[01:15:41.160 --> 01:15:48.680]  писать технический отчет на эту тему, то что превратилось потом в статью не особую, и по-моему\n",
            "[01:15:48.680 --> 01:15:55.320]  там продержался он 7 недель. Есть люди среди ресетчеров, которые любят что-то поделать\n",
            "[01:15:55.320 --> 01:16:00.520]  руками, но статью не писать, никак это формально не описывается, то же самое касается студентов с декламы.\n",
            "[01:16:00.520 --> 01:16:08.080]  Ну и Дамашка я сказал, что можно посчитать количество параметров в нейросети. Итого, мы\n",
            "[01:16:08.080 --> 01:16:19.960]  сделали такое базовое погружение в нейросети, посмотрели про задачу классификации, как получаются\n",
            "[01:16:19.960 --> 01:16:27.560]  данные. Я проговорил некоторые современные нюансы касаемо лицензирования и privacy, в больших\n",
            "[01:16:27.560 --> 01:16:33.880]  серьезных корпорациях такие вещи, вы можете с ними спокойно столкнуться. Дальше мы поговорили про модель\n",
            "[01:16:33.880 --> 01:16:40.400]  нейрона, персоптрона, и поговорили про биологические принципы за сверточными нейронными сетями,\n",
            "[01:16:40.400 --> 01:16:48.400]  и перешли вот к этапному результату AlexNet, с которого мы можем отсчитывать современную\n",
            "[01:16:48.400 --> 01:16:56.560]  эру как компьютерного зрения, так сейчас уже и фаундейш модели. В целом, даже если вам это показалось\n",
            "[01:16:56.560 --> 01:17:06.280]  немножко легко и даже скучновато, это окей, потому что Дамашки у нас достаточно дотошные,\n",
            "[01:17:06.280 --> 01:17:15.120]  вам там нужно будет форварды, бэкворды эффективно реализовывать ручками, и в целом вы можете там\n",
            "[01:17:15.120 --> 01:17:21.000]  показать как раз ваше знание. Если есть какие-то вопросы, я готов ответить.\n",
            "[01:17:26.720 --> 01:17:33.600]  На моих часах сейчас 19.20, давайте сделаем 10 минутный перерыв, я тут еще пока поводу,\n",
            "[01:17:33.600 --> 01:17:38.160]  а в 19.30 Андрей уже начнет семинару, у него там много материала.\n",
            "[01:17:45.120 --> 01:18:01.600]  Окей, ну если вопросов нет, тогда всем спасибо за внимание, я с вами прощаюсь.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Transcript file created: /content/drive/My Drive/Colab Notebooks/Whisper Youtube/IEz-lvAqC_A.txt**"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown # **Run the model** 🚀\n",
        "\n",
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** ⚙️\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown ---\n",
        "language = \"Russian\" #@param ['Auto detection', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba']\n",
        "#@markdown > Language spoken in the audio, use `Auto detection` to let Whisper detect the language.\n",
        "#@markdown ---\n",
        "verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n",
        "#@markdown > Whether to print out the progress and debug messages.\n",
        "#@markdown ---\n",
        "output_format = 'txt' #@param ['txt', 'vtt', 'srt', 'tsv', 'json', 'all']\n",
        "#@markdown > Type of file to generate to record the transcription.\n",
        "#@markdown ---\n",
        "task = 'transcribe' #@param ['transcribe', 'translate']\n",
        "#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown <br/>\n",
        "\n",
        "#@markdown ### **Optional: Fine tunning**\n",
        "#@markdown ---\n",
        "temperature = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to use for sampling.\n",
        "#@markdown ---\n",
        "temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n",
        "#@markdown ---\n",
        "best_of = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of candidates when sampling with non-zero temperature.\n",
        "#@markdown ---\n",
        "beam_size = 8 #@param {type:\"integer\"}\n",
        "#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n",
        "#@markdown ---\n",
        "patience = 1.0 #@param {type:\"number\"}\n",
        "#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n",
        "#@markdown ---\n",
        "length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n",
        "#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n",
        "#@markdown ---\n",
        "suppress_tokens = \"-1\" #@param {type:\"string\"}\n",
        "#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n",
        "#@markdown ---\n",
        "initial_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Optional text to provide as a prompt for the first window.\n",
        "#@markdown ---\n",
        "condition_on_previous_text = True #@param {type:\"boolean\"}\n",
        "#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n",
        "#@markdown ---\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "#@markdown > whether to perform inference in fp16.\n",
        "#@markdown ---\n",
        "compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n",
        "#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "logprob_threshold = -1.0 #@param {type:\"number\"}\n",
        "#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n",
        "#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n",
        "#@markdown ---\n",
        "\n",
        "verbose_lut = {\n",
        "    'Live transcription': True,\n",
        "    'Progress bar': False,\n",
        "    'None': None\n",
        "}\n",
        "\n",
        "args = dict(\n",
        "    language = (None if language == \"Auto detection\" else language),\n",
        "    verbose = verbose_lut[verbose],\n",
        "    task = task,\n",
        "    temperature = temperature,\n",
        "    temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "    best_of = best_of,\n",
        "    beam_size = beam_size,\n",
        "    patience=patience,\n",
        "    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "    suppress_tokens=suppress_tokens,\n",
        "    initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "    condition_on_previous_text=condition_on_previous_text,\n",
        "    fp16=fp16,\n",
        "    compression_ratio_threshold=compression_ratio_threshold,\n",
        "    logprob_threshold=logprob_threshold,\n",
        "    no_speech_threshold=no_speech_threshold\n",
        ")\n",
        "\n",
        "temperature = args.pop(\"temperature\")\n",
        "temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "if temperature_increment_on_fallback is not None:\n",
        "    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "else:\n",
        "    temperature = [temperature]\n",
        "\n",
        "if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n",
        "    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "    args[\"language\"] = \"en\"\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    display(Markdown(f\"### {video_path_local}\"))\n",
        "\n",
        "    video_transcription = whisper.transcribe(\n",
        "        whisper_model,\n",
        "        str(video_path_local),\n",
        "        temperature=temperature,\n",
        "        **args,\n",
        "    )\n",
        "\n",
        "    # Save output\n",
        "    whisper.utils.get_writer(\n",
        "        output_format=output_format,\n",
        "        output_dir=video_path_local.parent\n",
        "    )(\n",
        "        video_transcription,\n",
        "        str(video_path_local.stem),\n",
        "        options=dict(\n",
        "            highlight_words=False,\n",
        "            max_line_count=None,\n",
        "            max_line_width=None,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    def exportTranscriptFile(ext: str):\n",
        "        local_path = video_path_local.parent / video_path_local.with_suffix(ext).name\n",
        "        export_path = drive_whisper_path / video_path_local.with_suffix(ext).name\n",
        "        shutil.copy(\n",
        "            local_path,\n",
        "            export_path\n",
        "        )\n",
        "        display(Markdown(f\"**Transcript file created: {export_path}**\"))\n",
        "\n",
        "    if output_format==\"all\":\n",
        "        for ext in ('.txt', '.vtt', '.srt', '.tsv', '.json'):\n",
        "            exportTranscriptFile(ext)\n",
        "    else:\n",
        "        exportTranscriptFile(\".\" + output_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ad6n1m4deAHp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}